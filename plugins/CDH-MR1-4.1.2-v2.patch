diff -rupN ./src/mapred/mapred-default.xml ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/mapred-default.xml
--- ./src/mapred/mapred-default.xml	2012-11-02 02:56:46.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/mapred-default.xml	2013-09-15 07:43:01.391926000 +0200
@@ -219,6 +219,24 @@
 </property>
 
 <property>
+  <name>mapreduce.job.reduce.shuffle.consumer.plugin.class</name>
+  <value>org.apache.hadoop.mapred.ReduceTask$ReduceCopier</value>
+  <description>Name of the class whose instance will be used
+   to send shuffle requests by reducetasks of this job.
+   The class must be an instance of org.apache.hadoop.mapred.ShuffleConsumerPlugin.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.shuffle.provider.plugin.classes</name>
+  <value>org.apache.hadoop.mapred.TaskTracker$DefaultShuffleProvider</value>
+  <description>A comma-separated list of classes that should be loaded as ShuffleProviderPlugin(s).
+   A ShuffleProviderPlugin can serve shuffle requests from reducetasks.
+   Each class in the list must be an instance of org.apache.hadoop.mapred.ShuffleProviderPlugin.
+  </description>
+</property>
+
+<property>
   <name>mapred.tasktracker.taskmemorymanager.monitoring-interval</name>
   <value>5000</value>
   <description>The interval, in milliseconds, for which the tasktracker waits
diff -rupN ./src/mapred/mapred-default.xml.orig ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/mapred-default.xml.orig
--- ./src/mapred/mapred-default.xml.orig	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/mapred-default.xml.orig	2012-11-02 02:56:46.000000000 +0200
@@ -0,0 +1,1310 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+
+<!-- Do not modify this file directly.  Instead, copy entries that you -->
+<!-- wish to modify from this file into mapred-site.xml and change them -->
+<!-- there.  If mapred-site.xml does not already exist, create it.      -->
+
+<configuration>
+
+<property>
+  <name>hadoop.job.history.location</name>
+  <value></value>
+  <description> If job tracker is static the history files are stored 
+  in this single well known place. If No value is set here, by default,
+  it is in the local file system at ${hadoop.log.dir}/history.
+  </description>
+</property>
+
+<property>
+  <name>hadoop.job.history.user.location</name>
+  <value></value>
+  <description> User can specify a location to store the history files of 
+  a particular job. If nothing is specified, the logs are stored in 
+  output directory. The files are stored in "_logs/history/" in the directory.
+  User can stop logging by giving the value "none". 
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.tracker.history.completed.location</name>
+  <value></value>
+  <description> The completed job history files are stored at this single well 
+  known location. If nothing is specified, the files are stored at 
+  ${hadoop.job.history.location}/done.
+  </description>
+</property>
+
+<!-- i/o properties -->
+
+<property>
+  <name>io.sort.factor</name>
+  <value>10</value>
+  <description>The number of streams to merge at once while sorting
+  files.  This determines the number of open file handles.</description>
+</property>
+
+<property>
+  <name>io.sort.mb</name>
+  <value>100</value>
+  <description>The total amount of buffer memory to use while sorting 
+  files, in megabytes.  By default, gives each merge stream 1MB, which
+  should minimize seeks.</description>
+</property>
+
+<property>
+  <name>io.sort.record.percent</name>
+  <value>0.05</value>
+  <description>The percentage of io.sort.mb dedicated to tracking record
+  boundaries. Let this value be r, io.sort.mb be x. The maximum number
+  of records collected before the collection thread must block is equal
+  to (r * x) / 4</description>
+</property>
+
+<property>
+  <name>io.sort.spill.percent</name>
+  <value>0.80</value>
+  <description>The soft limit in either the buffer or record collection
+  buffers. Once reached, a thread will begin to spill the contents to disk
+  in the background. Note that this does not imply any chunking of data to
+  the spill. A value less than 0.5 is not recommended.</description>
+</property>
+
+<property>
+  <name>io.map.index.skip</name>
+  <value>0</value>
+  <description>Number of index entries to skip between each entry.
+  Zero by default. Setting this to values larger than zero can
+  facilitate opening large map files using less memory.</description>
+</property>
+
+<property>
+  <name>mapred.job.tracker</name>
+  <value>local</value>
+  <description>The host and port that the MapReduce job tracker runs
+  at.  If "local", then jobs are run in-process as a single map
+  and reduce task.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.tracker.http.address</name>
+  <value>0.0.0.0:50030</value>
+  <description>
+    The job tracker http server address and port the server will listen on.
+    If the port is 0 then the server will start on a free port.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.tracker.handler.count</name>
+  <value>10</value>
+  <description>
+    The number of server threads for the JobTracker. This should be roughly
+    4% of the number of tasktracker nodes.
+  </description>
+</property>
+
+<property>
+  <name>mapred.task.tracker.report.address</name>
+  <value>127.0.0.1:0</value>
+  <description>The interface and port that task tracker server listens on. 
+  Since it is only connected to by the tasks, it uses the local interface.
+  EXPERT ONLY. Should only be changed if your host does not have the loopback 
+  interface.</description>
+</property>
+
+<property>
+  <name>mapred.local.dir</name>
+  <value>${hadoop.tmp.dir}/mapred/local</value>
+  <description>The local directory where MapReduce stores intermediate
+  data files.  May be a comma-separated list of
+  directories on different devices in order to spread disk i/o.
+  Directories that do not exist are ignored.
+  </description>
+</property>
+
+<property>
+  <name>mapred.system.dir</name>
+  <value>${hadoop.tmp.dir}/mapred/system</value>
+  <description>The directory where MapReduce stores control files.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.jobtracker.staging.root.dir</name>
+  <value>${hadoop.tmp.dir}/mapred/staging</value>
+  <description>The root of the staging area for users' job files
+  In practice, this should be the directory where users' home 
+  directories are located (usually /user)
+  </description>
+</property>
+
+<property>
+  <name>mapred.temp.dir</name>
+  <value>${hadoop.tmp.dir}/mapred/temp</value>
+  <description>A shared directory for temporary files.
+  </description>
+</property>
+
+<property>
+  <name>mapred.local.dir.minspacestart</name>
+  <value>0</value>
+  <description>If the space in mapred.local.dir drops under this, 
+  do not ask for more tasks.
+  Value in bytes.
+  </description>
+</property>
+
+<property>
+  <name>mapred.local.dir.minspacekill</name>
+  <value>0</value>
+  <description>If the space in mapred.local.dir drops under this, 
+    do not ask more tasks until all the current ones have finished and 
+    cleaned up. Also, to save the rest of the tasks we have running, 
+    kill one of them, to clean up some space. Start with the reduce tasks,
+    then go with the ones that have finished the least.
+    Value in bytes.
+  </description>
+</property>
+
+<!-- TaskTracker DistributedCache configuration -->
+<property>
+  <name>local.cache.size</name>
+  <value>10737418240</value>
+  <description>The number of bytes to allocate in each local TaskTracker
+  directory for holding Distributed Cache data.</description>
+</property>
+
+<property>
+  <name>mapreduce.tasktracker.cache.local.numberdirectories</name>
+  <value>10000</value>
+  <description>
+  The maximum number of subdirectories that should be created in any particular
+  distributed cache store. After this many directories have been created,
+  cache items will be expunged regardless of whether the total size threshold
+  has been exceeded.
+  </description>
+</property>
+<!-- End of TaskTracker DistributedCache configuration -->
+
+
+<property>
+  <name>mapred.tasktracker.expiry.interval</name>
+  <value>600000</value>
+  <description>Expert: The time-interval, in miliseconds, after which
+  a tasktracker is declared 'lost' if it doesn't send heartbeats.
+  </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.instrumentation</name>
+  <value>org.apache.hadoop.mapred.TaskTrackerMetricsInst</value>
+  <description>Expert: The instrumentation class to associate with each TaskTracker.
+  </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.resourcecalculatorplugin</name>
+  <value></value>
+  <description>
+   Name of the class whose instance will be used to query resource information
+   on the tasktracker.
+   
+   The class must be an instance of 
+   org.apache.hadoop.util.ResourceCalculatorPlugin. If the value is null, the
+   tasktracker attempts to use a class appropriate to the platform. 
+   Currently, the only platform supported is Linux.
+  </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.taskmemorymanager.monitoring-interval</name>
+  <value>5000</value>
+  <description>The interval, in milliseconds, for which the tasktracker waits
+   between two cycles of monitoring its tasks' memory usage. Used only if
+   tasks' memory management is enabled via mapred.tasktracker.tasks.maxmemory.
+   </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.tasks.sleeptime-before-sigkill</name>
+  <value>5000</value>
+  <description>The time, in milliseconds, the tasktracker waits for sending a
+  SIGKILL to a process, after it has been sent a SIGTERM.</description>
+</property>
+
+<property>
+  <name>mapred.map.tasks</name>
+  <value>2</value>
+  <description>The default number of map tasks per job.
+  Ignored when mapred.job.tracker is "local".  
+  </description>
+</property>
+
+<property>
+  <name>mapred.reduce.tasks</name>
+  <value>1</value>
+  <description>The default number of reduce tasks per job. Typically set to 99%
+  of the cluster's reduce capacity, so that if a node fails the reduces can 
+  still be executed in a single wave.
+  Ignored when mapred.job.tracker is "local".
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.tasktracker.outofband.heartbeat</name>
+  <value>false</value>
+  <description>Expert: Set this to true to let the tasktracker send an 
+  out-of-band heartbeat on task-completion for better latency.
+  </description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.restart.recover</name>
+  <value>false</value>
+  <description>"true" to enable (job) recovery upon restart,
+               "false" to start afresh
+  </description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.job.history.block.size</name>
+  <value>3145728</value>
+  <description>The block size of the job history file. Since the job recovery
+               uses job history, its important to dump job history to disk as 
+               soon as possible. Note that this is an expert level parameter.
+               The default value is set to 3 MB.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.jobtracker.split.metainfo.maxsize</name>
+  <value>10000000</value>
+  <description>The maximum permissible size of the split metainfo file.
+  The JobTracker won't attempt to read split metainfo files bigger than
+  the configured value.
+  No limits if set to -1.
+  </description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.taskScheduler</name>
+  <value>org.apache.hadoop.mapred.JobQueueTaskScheduler</value>
+  <description>The class responsible for scheduling the tasks.</description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.taskScheduler.maxRunningTasksPerJob</name>
+  <value></value>
+  <description>The maximum number of running tasks for a job before
+  it gets preempted. No limits if undefined.
+  </description>
+</property>
+
+<property>
+  <name>mapred.map.max.attempts</name>
+  <value>4</value>
+  <description>Expert: The maximum number of attempts per map task.
+  In other words, framework will try to execute a map task these many number
+  of times before giving up on it.
+  </description>
+</property>
+
+<property>
+  <name>mapred.reduce.max.attempts</name>
+  <value>4</value>
+  <description>Expert: The maximum number of attempts per reduce task.
+  In other words, framework will try to execute a reduce task these many number
+  of times before giving up on it.
+  </description>
+</property>
+
+<property>
+  <name>mapred.reduce.parallel.copies</name>
+  <value>5</value>
+  <description>The default number of parallel transfers run by reduce
+  during the copy(shuffle) phase.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.reduce.shuffle.maxfetchfailures</name>
+  <value>10</value>
+  <description>The maximum number of times a reducer tries to
+  fetch a map output before it reports it.
+</description></property>
+
+<property>
+  <name>mapreduce.reduce.shuffle.connect.timeout</name>
+  <value>180000</value>
+  <description>Expert: The maximum amount of time (in milli seconds) a reduce
+  task spends in trying to connect to a tasktracker for getting map output.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.reduce.shuffle.read.timeout</name>
+  <value>180000</value>
+  <description>Expert: The maximum amount of time (in milli seconds) a reduce
+  task waits for map output data to be available for reading after obtaining
+  connection.
+  </description>
+</property>
+
+<property>
+  <name>mapred.task.timeout</name>
+  <value>600000</value>
+  <description>The number of milliseconds before a task will be
+  terminated if it neither reads an input, writes an output, nor
+  updates its status string.
+  </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.map.tasks.maximum</name>
+  <value>2</value>
+  <description>The maximum number of map tasks that will be run
+  simultaneously by a task tracker.
+  </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.reduce.tasks.maximum</name>
+  <value>2</value>
+  <description>The maximum number of reduce tasks that will be run
+  simultaneously by a task tracker.
+  </description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.completeuserjobs.maximum</name>
+  <value>100</value>
+  <description>The maximum number of complete jobs per user to keep around 
+  before delegating them to the job history.</description>
+</property>
+
+<property>
+  <name>mapreduce.reduce.input.limit</name>
+  <value>-1</value>
+  <description>The limit on the input size of the reduce. If the estimated
+  input size of the reduce is greater than this value, job is failed. A
+  value of -1 means that there is no limit set. </description>
+</property>
+
+<property>
+  <name>mapred.job.tracker.retiredjobs.cache.size</name>
+  <value>1000</value>
+  <description>The number of retired job status to keep in the cache.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.tracker.jobhistory.lru.cache.size</name>
+  <value>5</value>
+  <description>The number of job history files loaded in memory. The jobs are 
+  loaded when they are first accessed. The cache is cleared based on LRU.
+  </description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.instrumentation</name>
+  <value>org.apache.hadoop.mapred.JobTrackerMetricsInst</value>
+  <description>Expert: The instrumentation class to associate with each JobTracker.
+  </description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.plugins</name>
+  <value></value>
+  <description>Comma-separated list of jobtracker plug-ins to be activated.
+  </description>
+</property>
+
+<property>
+  <name>mapred.child.java.opts</name>
+  <value>-Xmx200m</value>
+  <description>Java opts for the task tracker child processes.  
+  The following symbol, if present, will be interpolated: @taskid@ is replaced 
+  by current TaskID. Any other occurrences of '@' will go unchanged.
+  For example, to enable verbose gc logging to a file named for the taskid in
+  /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
+        -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc
+  
+  The configuration variable mapred.child.ulimit can be used to control the
+  maximum virtual memory of the child processes. 
+  </description>
+</property>
+
+<property>
+  <name>mapred.child.env</name>
+  <value></value>
+  <description>User added environment variables for the task tracker child 
+  processes. Example :
+  1) A=foo  This will set the env variable A to foo
+  2) B=$B:c This is inherit tasktracker's B env variable.  
+  </description>
+</property>
+
+<property>
+  <name>mapred.child.ulimit</name>
+  <value></value>
+  <description>The maximum virtual memory, in KB, of a process launched by the 
+  Map-Reduce framework. This can be used to control both the Mapper/Reducer 
+  tasks and applications using Hadoop Pipes, Hadoop Streaming etc. 
+  By default it is left unspecified to let cluster admins control it via 
+  limits.conf and other such relevant mechanisms.
+  
+  Note: mapred.child.ulimit must be greater than or equal to the -Xmx passed to
+  JavaVM, else the VM might not start. 
+  </description>
+</property>
+
+<property>
+  <name>mapred.cluster.map.memory.mb</name>
+  <value>-1</value>
+  <description>The size, in terms of virtual memory, of a single map slot 
+  in the Map-Reduce framework, used by the scheduler. 
+  A job can ask for multiple slots for a single map task via 
+  mapred.job.map.memory.mb, upto the limit specified by 
+  mapred.cluster.max.map.memory.mb, if the scheduler supports the feature. 
+  The value of -1 indicates that this feature is turned off.
+  </description>
+</property>
+
+<property>
+  <name>mapred.cluster.reduce.memory.mb</name>
+  <value>-1</value>
+  <description>The size, in terms of virtual memory, of a single reduce slot 
+  in the Map-Reduce framework, used by the scheduler. 
+  A job can ask for multiple slots for a single reduce task via 
+  mapred.job.reduce.memory.mb, upto the limit specified by 
+  mapred.cluster.max.reduce.memory.mb, if the scheduler supports the feature. 
+  The value of -1 indicates that this feature is turned off.
+  </description>
+</property>
+
+<property>
+  <name>mapred.cluster.max.map.memory.mb</name>
+  <value>-1</value>
+  <description>The maximum size, in terms of virtual memory, of a single map 
+  task launched by the Map-Reduce framework, used by the scheduler. 
+  A job can ask for multiple slots for a single map task via 
+  mapred.job.map.memory.mb, upto the limit specified by 
+  mapred.cluster.max.map.memory.mb, if the scheduler supports the feature. 
+  The value of -1 indicates that this feature is turned off.
+  </description>
+</property>
+
+<property>
+  <name>mapred.cluster.max.reduce.memory.mb</name>
+  <value>-1</value>
+  <description>The maximum size, in terms of virtual memory, of a single reduce 
+  task launched by the Map-Reduce framework, used by the scheduler. 
+  A job can ask for multiple slots for a single reduce task via 
+  mapred.job.reduce.memory.mb, upto the limit specified by 
+  mapred.cluster.max.reduce.memory.mb, if the scheduler supports the feature. 
+  The value of -1 indicates that this feature is turned off.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.map.memory.mb</name>
+  <value>-1</value>
+  <description>The size, in terms of virtual memory, of a single map task 
+  for the job.
+  A job can ask for multiple slots for a single map task, rounded up to the 
+  next multiple of mapred.cluster.map.memory.mb and upto the limit 
+  specified by mapred.cluster.max.map.memory.mb, if the scheduler supports 
+  the feature. 
+  The value of -1 indicates that this feature is turned off iff 
+  mapred.cluster.map.memory.mb is also turned off (-1).
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.reduce.memory.mb</name>
+  <value>-1</value>
+  <description>The size, in terms of virtual memory, of a single reduce task 
+  for the job.
+  A job can ask for multiple slots for a single map task, rounded up to the 
+  next multiple of mapred.cluster.reduce.memory.mb and upto the limit 
+  specified by mapred.cluster.max.reduce.memory.mb, if the scheduler supports 
+  the feature. 
+  The value of -1 indicates that this feature is turned off iff
+  mapred.cluster.reduce.memory.mb is also turned off (-1).  
+  </description>
+</property>
+
+<property>
+  <name>mapred.child.tmp</name>
+  <value>./tmp</value>
+  <description> To set the value of tmp directory for map and reduce tasks.
+  If the value is an absolute path, it is directly assigned. Otherwise, it is
+  prepended with task's working directory. The java tasks are executed with
+  option -Djava.io.tmpdir='the absolute path of the tmp dir'. Pipes and
+  streaming are set with environment variable,
+   TMPDIR='the absolute path of the tmp dir'
+  </description>
+</property>
+
+<property>
+  <name>mapred.map.child.log.level</name>
+  <value>INFO</value>
+  <description>The logging level for the map task. The allowed levels are:
+  OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL.
+  </description>
+</property>
+
+<property>
+  <name>mapred.reduce.child.log.level</name>
+  <value>INFO</value>
+  <description>The logging level for the reduce task. The allowed levels are:
+  OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL.
+  </description>
+</property>
+
+<property>
+  <name>mapred.inmem.merge.threshold</name>
+  <value>1000</value>
+  <description>The threshold, in terms of the number of files 
+  for the in-memory merge process. When we accumulate threshold number of files
+  we initiate the in-memory merge and spill to disk. A value of 0 or less than
+  0 indicates we want to DON'T have any threshold and instead depend only on
+  the ramfs's memory consumption to trigger the merge.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.shuffle.merge.percent</name>
+  <value>0.66</value>
+  <description>The usage threshold at which an in-memory merge will be
+  initiated, expressed as a percentage of the total memory allocated to
+  storing in-memory map outputs, as defined by
+  mapred.job.shuffle.input.buffer.percent.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.shuffle.input.buffer.percent</name>
+  <value>0.70</value>
+  <description>The percentage of memory to be allocated from the maximum heap
+  size to storing map outputs during the shuffle.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.reduce.input.buffer.percent</name>
+  <value>0.0</value>
+  <description>The percentage of memory- relative to the maximum heap size- to
+  retain map outputs during the reduce. When the shuffle is concluded, any
+  remaining map outputs in memory must consume less than this threshold before
+  the reduce can begin.
+  </description>
+</property>
+
+<property>
+  <name>mapred.map.tasks.speculative.execution</name>
+  <value>true</value>
+  <description>If true, then multiple instances of some map tasks 
+               may be executed in parallel.</description>
+</property>
+
+<property>
+  <name>mapred.reduce.tasks.speculative.execution</name>
+  <value>true</value>
+  <description>If true, then multiple instances of some reduce tasks 
+               may be executed in parallel.</description>
+</property>
+
+<property>
+  <name>mapred.job.reuse.jvm.num.tasks</name>
+  <value>1</value>
+  <description>How many tasks to run per jvm. If set to -1, there is
+  no limit. 
+  </description>
+</property>
+
+<property>
+  <name>mapred.min.split.size</name>
+  <value>0</value>
+  <description>The minimum size chunk that map input should be split
+  into.  Note that some file formats may have minimum split sizes that
+  take priority over this setting.</description>
+</property>
+
+<property>
+  <name>mapred.jobtracker.maxtasks.per.job</name>
+  <value>-1</value>
+  <description>The maximum number of tasks for a single job.
+  A value of -1 indicates that there is no maximum.  </description>
+</property>
+
+<property>
+  <name>mapred.submit.replication</name>
+  <value>10</value>
+  <description>The replication level for submitted job files.  This
+  should be around the square root of the number of nodes.
+  </description>
+</property>
+
+
+<property>
+  <name>mapred.tasktracker.dns.interface</name>
+  <value>default</value>
+  <description>The name of the Network Interface from which a task
+  tracker should report its IP address.
+  </description>
+ </property>
+ 
+<property>
+  <name>mapred.tasktracker.dns.nameserver</name>
+  <value>default</value>
+  <description>The host name or IP address of the name server (DNS)
+  which a TaskTracker should use to determine the host name used by
+  the JobTracker for communication and display purposes.
+  </description>
+ </property>
+ 
+<property>
+  <name>tasktracker.http.threads</name>
+  <value>40</value>
+  <description>The number of worker threads that for the http server. This is
+               used for map output fetching
+  </description>
+</property>
+
+<property>
+  <name>mapred.task.tracker.http.address</name>
+  <value>0.0.0.0:50060</value>
+  <description>
+    The task tracker http server address and port.
+    If the port is 0 then the server will start on a free port.
+  </description>
+</property>
+
+<property>
+  <name>keep.failed.task.files</name>
+  <value>false</value>
+  <description>Should the files for failed tasks be kept. This should only be 
+               used on jobs that are failing, because the storage is never
+               reclaimed. It also prevents the map outputs from being erased
+               from the reduce directory as they are consumed.</description>
+</property>
+
+
+<!-- 
+  <property>
+  <name>keep.task.files.pattern</name>
+  <value>.*_m_123456_0</value>
+  <description>Keep all files from tasks whose task names match the given
+               regular expression. Defaults to none.</description>
+  </property>
+-->
+
+<property>
+  <name>mapred.output.compress</name>
+  <value>false</value>
+  <description>Should the job outputs be compressed?
+  </description>
+</property>
+
+<property>
+  <name>mapred.output.compression.type</name>
+  <value>RECORD</value>
+  <description>If the job outputs are to compressed as SequenceFiles, how should
+               they be compressed? Should be one of NONE, RECORD or BLOCK.
+  </description>
+</property>
+
+<property>
+  <name>mapred.output.compression.codec</name>
+  <value>org.apache.hadoop.io.compress.DefaultCodec</value>
+  <description>If the job outputs are compressed, how should they be compressed?
+  </description>
+</property>
+
+<property>
+  <name>mapred.compress.map.output</name>
+  <value>false</value>
+  <description>Should the outputs of the maps be compressed before being
+               sent across the network. Uses SequenceFile compression.
+  </description>
+</property>
+
+<property>
+  <name>mapred.map.output.compression.codec</name>
+  <value>org.apache.hadoop.io.compress.DefaultCodec</value>
+  <description>If the map outputs are compressed, how should they be 
+               compressed?
+  </description>
+</property>
+
+<property>
+  <name>map.sort.class</name>
+  <value>org.apache.hadoop.util.QuickSort</value>
+  <description>The default sort class for sorting keys.
+  </description>
+</property>
+
+<property>
+  <name>mapred.userlog.limit.kb</name>
+  <value>0</value>
+  <description>The maximum size of user-logs of each task in KB. 0 disables the cap.
+  </description>
+</property>
+
+<property>
+  <name>mapred.userlog.retain.hours</name>
+  <value>24</value>
+  <description>The maximum time, in hours, for which the user-logs are to be 
+               retained after the job completion.
+  </description>
+</property>
+
+<property>
+  <name>mapred.user.jobconf.limit</name>
+  <value>5242880</value>
+  <description>The maximum allowed size of the user jobconf. The 
+  default is set to 5 MB</description>
+</property>
+
+<property>
+  <name>mapred.hosts</name>
+  <value></value>
+  <description>Names a file that contains the list of nodes that may
+  connect to the jobtracker.  If the value is empty, all hosts are
+  permitted.</description>
+</property>
+
+<property>
+  <name>mapred.hosts.exclude</name>
+  <value></value>
+  <description>Names a file that contains the list of hosts that
+  should be excluded by the jobtracker.  If the value is empty, no
+  hosts are excluded.</description>
+</property>
+
+<property>
+  <name>mapred.heartbeats.in.second</name>
+  <value>100</value>
+  <description>Expert: Approximate number of heart-beats that could arrive 
+               at JobTracker in a second. Assuming each RPC can be processed 
+               in 10msec, the default value is made 100 RPCs in a second.
+  </description>
+</property> 
+
+<property>
+  <name>mapred.max.tracker.blacklists</name>
+  <value>4</value>
+  <description>The number of blacklists for a taskTracker by various jobs
+               after which the task tracker could be blacklisted across
+               all jobs. The tracker will be given a tasks later
+               (after a day). The tracker will become a healthy
+               tracker after a restart.
+  </description>
+</property> 
+
+<property>
+  <name>mapred.max.tracker.failures</name>
+  <value>4</value>
+  <description>The number of task-failures on a tasktracker of a given job 
+               after which new tasks of that job aren't assigned to it.
+  </description>
+</property>
+
+<property>
+  <name>jobclient.output.filter</name>
+  <value>FAILED</value>
+  <description>The filter for controlling the output of the task's userlogs sent
+               to the console of the JobClient. 
+               The permissible options are: NONE, KILLED, FAILED, SUCCEEDED and 
+               ALL.
+  </description>
+</property>
+
+  <property>
+    <name>jobclient.completion.poll.interval</name>
+    <value>5000</value>
+    <description>The interval (in milliseconds) between which the JobClient
+    polls the JobTracker for updates about job status. You may want to set this
+    to a lower value to make tests run faster on a single node system. Adjusting
+    this value in production may lead to unwanted client-server traffic.
+    </description>
+  </property>
+
+  <property>
+    <name>jobclient.progress.monitor.poll.interval</name>
+    <value>1000</value>
+    <description>The interval (in milliseconds) between which the JobClient
+    reports status to the console and checks for job completion. You may want to set this
+    to a lower value to make tests run faster on a single node system. Adjusting
+    this value in production may lead to unwanted client-server traffic.
+    </description>
+  </property>
+
+  <property>
+    <name>mapred.job.tracker.persist.jobstatus.active</name>
+    <value>false</value>
+    <description>Indicates if persistency of job status information is
+      active or not.
+    </description>
+  </property>
+
+  <property>
+  <name>mapred.job.tracker.persist.jobstatus.hours</name>
+  <value>0</value>
+  <description>The number of hours job status information is persisted in DFS.
+    The job status information will be available after it drops of the memory
+    queue and between jobtracker restarts. With a zero value the job status
+    information is not persisted at all in DFS.
+  </description>
+</property>
+
+  <property>
+    <name>mapred.job.tracker.persist.jobstatus.dir</name>
+    <value>/jobtracker/jobsInfo</value>
+    <description>The directory where the job status information is persisted
+      in a file system to be available after it drops of the memory queue and
+      between jobtracker restarts.
+    </description>
+  </property>
+
+  <property>
+    <name>mapreduce.job.complete.cancel.delegation.tokens</name>
+    <value>true</value>
+    <description> if false - do not unregister/cancel delegation tokens
+    from renewal, because same tokens may be used by spawned jobs
+    </description>
+  </property>
+
+  <property>
+    <name>mapred.task.profile</name>
+    <value>false</value>
+    <description>To set whether the system should collect profiler
+     information for some of the tasks in this job? The information is stored
+     in the user log directory. The value is "true" if task profiling
+     is enabled.</description>
+  </property>
+
+  <property>
+    <name>mapred.task.profile.maps</name>
+    <value>0-2</value>
+    <description> To set the ranges of map tasks to profile.
+    mapred.task.profile has to be set to true for the value to be accounted.
+    </description>
+  </property>
+
+  <property>
+    <name>mapred.task.profile.reduces</name>
+    <value>0-2</value>
+    <description> To set the ranges of reduce tasks to profile.
+    mapred.task.profile has to be set to true for the value to be accounted.
+    </description>
+  </property>
+
+  <property>
+    <name>mapred.line.input.format.linespermap</name>
+    <value>1</value>
+    <description> Number of lines per split in NLineInputFormat.
+    </description>
+  </property>
+  
+  <property>
+    <name>mapred.skip.attempts.to.start.skipping</name>
+    <value>2</value>
+    <description> The number of Task attempts AFTER which skip mode 
+    will be kicked off. When skip mode is kicked off, the 
+    tasks reports the range of records which it will process 
+    next, to the TaskTracker. So that on failures, TT knows which 
+    ones are possibly the bad records. On further executions, 
+    those are skipped.
+    </description>
+  </property>
+  
+  <property>
+    <name>mapred.skip.map.auto.incr.proc.count</name>
+    <value>true</value>
+    <description> The flag which if set to true, 
+    SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS is incremented 
+    by MapRunner after invoking the map function. This value must be set to 
+    false for applications which process the records asynchronously 
+    or buffer the input records. For example streaming. 
+    In such cases applications should increment this counter on their own.
+    </description>
+  </property>
+  
+  <property>
+    <name>mapred.skip.reduce.auto.incr.proc.count</name>
+    <value>true</value>
+    <description> The flag which if set to true, 
+    SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS is incremented 
+    by framework after invoking the reduce function. This value must be set to 
+    false for applications which process the records asynchronously 
+    or buffer the input records. For example streaming. 
+    In such cases applications should increment this counter on their own.
+    </description>
+  </property>
+  
+  <property>
+    <name>mapred.skip.out.dir</name>
+    <value></value>
+    <description> If no value is specified here, the skipped records are 
+    written to the output directory at _logs/skip.
+    User can stop writing skipped records by giving the value "none". 
+    </description>
+  </property>
+
+  <property>
+    <name>mapred.skip.map.max.skip.records</name>
+    <value>0</value>
+    <description> The number of acceptable skip records surrounding the bad 
+    record PER bad record in mapper. The number includes the bad record as well.
+    To turn the feature of detection/skipping of bad records off, set the 
+    value to 0.
+    The framework tries to narrow down the skipped range by retrying  
+    until this threshold is met OR all attempts get exhausted for this task. 
+    Set the value to Long.MAX_VALUE to indicate that framework need not try to 
+    narrow down. Whatever records(depends on application) get skipped are 
+    acceptable.
+    </description>
+  </property>
+  
+  <property>
+    <name>mapred.skip.reduce.max.skip.groups</name>
+    <value>0</value>
+    <description> The number of acceptable skip groups surrounding the bad 
+    group PER bad group in reducer. The number includes the bad group as well.
+    To turn the feature of detection/skipping of bad groups off, set the 
+    value to 0.
+    The framework tries to narrow down the skipped range by retrying  
+    until this threshold is met OR all attempts get exhausted for this task. 
+    Set the value to Long.MAX_VALUE to indicate that framework need not try to 
+    narrow down. Whatever groups(depends on application) get skipped are 
+    acceptable.
+    </description>
+  </property>
+
+  <property>
+    <name>mapreduce.ifile.readahead</name>
+    <value>true</value>
+    <description>Configuration key to enable/disable IFile readahead.
+    </description>
+  </property>
+
+  <property>
+    <name>mapreduce.ifile.readahead.bytes</name>
+    <value>4194304</value>
+    <description>Configuration key to set the IFile readahead length in bytes.
+    </description>
+  </property>
+
+<!-- Job Notification Configuration -->
+
+<!--
+<property>
+ <name>job.end.notification.url</name>
+ <value>http://localhost:8080/jobstatus.php?jobId=$jobId&amp;jobStatus=$jobStatus</value>
+ <description>Indicates url which will be called on completion of job to inform
+              end status of job.
+              User can give at most 2 variables with URI : $jobId and $jobStatus.
+              If they are present in URI, then they will be replaced by their
+              respective values.
+</description>
+</property>
+-->
+
+<property>
+  <name>job.end.retry.attempts</name>
+  <value>0</value>
+  <description>Indicates how many times hadoop should attempt to contact the
+               notification URL </description>
+</property>
+
+<property>
+  <name>job.end.retry.interval</name>
+   <value>30000</value>
+   <description>Indicates time in milliseconds between notification URL retry
+                calls</description>
+</property>
+  
+<!-- Proxy Configuration -->
+<property>
+  <name>hadoop.rpc.socket.factory.class.JobSubmissionProtocol</name>
+  <value></value>
+  <description> SocketFactory to use to connect to a Map/Reduce master
+    (JobTracker). If null or empty, then use hadoop.rpc.socket.class.default.
+  </description>
+</property>
+
+<property>
+  <name>mapred.task.cache.levels</name>
+  <value>2</value>
+  <description> This is the max level of the task cache. For example, if
+    the level is 2, the tasks cached are at the host level and at the rack
+    level.
+  </description>
+</property>
+
+<property>
+  <name>mapred.queue.names</name>
+  <value>default</value>
+  <description> Comma separated list of queues configured for this jobtracker.
+    Jobs are added to queues and schedulers can configure different 
+    scheduling properties for the various queues. To configure a property 
+    for a queue, the name of the queue must match the name specified in this 
+    value. Queue properties that are common to all schedulers are configured 
+    here with the naming convention, mapred.queue.$QUEUE-NAME.$PROPERTY-NAME,
+    for e.g. mapred.queue.default.submit-job-acl.
+    The number of queues configured in this parameter could depend on the
+    type of scheduler being used, as specified in 
+    mapred.jobtracker.taskScheduler. For example, the JobQueueTaskScheduler
+    supports only a single queue, which is the default configured here.
+    Before adding more queues, ensure that the scheduler you've configured
+    supports multiple queues.
+  </description>
+</property>
+
+<property>
+  <name>mapred.acls.enabled</name>
+  <value>false</value>
+  <description> Specifies whether ACLs should be checked
+    for authorization of users for doing various queue and job level operations.
+    ACLs are disabled by default. If enabled, access control checks are made by
+    JobTracker and TaskTracker when requests are made by users for queue
+    operations like submit job to a queue and kill a job in the queue and job
+    operations like viewing the job-details (See mapreduce.job.acl-view-job)
+    or for modifying the job (See mapreduce.job.acl-modify-job) using
+    Map/Reduce APIs, RPCs or via the console and web user interfaces.
+  </description>
+</property>
+
+<property>
+  <name>mapred.queue.default.state</name>
+  <value>RUNNING</value>
+  <description>
+   This values defines the state , default queue is in.
+   the values can be either "STOPPED" or "RUNNING"
+   This value can be changed at runtime.
+  </description>
+</property>
+
+<property>
+  <name>mapred.job.queue.name</name>
+  <value>default</value>
+  <description> Queue to which a job is submitted. This must match one of the
+    queues defined in mapred.queue.names for the system. Also, the ACL setup
+    for the queue must allow the current user to submit a job to the queue.
+    Before specifying a queue, ensure that the system is configured with 
+    the queue, and access is allowed for submitting jobs to the queue.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.job.acl-modify-job</name>
+  <value> </value>
+  <description> Job specific access-control list for 'modifying' the job. It
+    is only used if authorization is enabled in Map/Reduce by setting the
+    configuration property mapred.acls.enabled to true.
+    This specifies the list of users and/or groups who can do modification
+    operations on the job. For specifying a list of users and groups the
+    format to use is "user1,user2 group1,group". If set to '*', it allows all
+    users/groups to modify this job. If set to ' '(i.e. space), it allows
+    none. This configuration is used to guard all the modifications with respect
+    to this job and takes care of all the following operations:
+      o killing this job
+      o killing a task of this job, failing a task of this job
+      o setting the priority of this job
+    Each of these operations are also protected by the per-queue level ACL
+    "acl-administer-jobs" configured via mapred-queues.xml. So a caller should
+    have the authorization to satisfy either the queue-level ACL or the
+    job-level ACL.
+
+    Irrespective of this ACL configuration, job-owner, the user who started the
+    cluster, cluster administrators configured via
+    mapreduce.cluster.administrators and queue administrators of the queue to
+    which this job is submitted to configured via
+    mapred.queue.queue-name.acl-administer-jobs  in mapred-queue-acls.xml can
+    do all the modification operations on a job.
+
+    By default, nobody else besides job-owner, the user who started the cluster,
+    cluster administrators and queue administrators can perform modification
+    operations on a job.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.job.acl-view-job</name>
+  <value> </value>
+  <description> Job specific access-control list for 'viewing' the job. It is
+    only used if authorization is enabled in Map/Reduce by setting the
+    configuration property mapred.acls.enabled to true.
+    This specifies the list of users and/or groups who can view private details
+    about the job. For specifying a list of users and groups the
+    format to use is "user1,user2 group1,group". If set to '*', it allows all
+    users/groups to modify this job. If set to ' '(i.e. space), it allows
+    none. This configuration is used to guard some of the job-views and at
+    present only protects APIs that can return possibly sensitive information
+    of the job-owner like
+      o job-level counters
+      o task-level counters
+      o tasks' diagnostic information
+      o task-logs displayed on the TaskTracker web-UI and
+      o job.xml showed by the JobTracker's web-UI
+    Every other piece of information of jobs is still accessible by any other
+    user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc.
+    
+    Irrespective of this ACL configuration, job-owner, the user who started the
+    cluster, cluster administrators configured via
+    mapreduce.cluster.administrators and queue administrators of the queue to
+    which this job is submitted to configured via
+    mapred.queue.queue-name.acl-administer-jobs in mapred-queue-acls.xml can do
+    all the view operations on a job.
+    
+    By default, nobody else besides job-owner, the user who started the
+    cluster, cluster administrators and queue administrators can perform
+    view operations on a job.
+  </description>
+</property>
+
+<property>
+  <name>mapred.tasktracker.indexcache.mb</name>
+  <value>10</value>
+  <description> The maximum memory that a task tracker allows for the 
+    index cache that is used when serving map outputs to reducers.
+  </description>
+</property>
+
+<property>
+  <name>mapred.merge.recordsBeforeProgress</name>
+  <value>10000</value>
+  <description> The number of records to process during merge before
+   sending a progress notification to the TaskTracker.
+  </description>
+</property>
+
+<property>
+  <name>mapred.reduce.slowstart.completed.maps</name>
+  <value>0.05</value>
+  <description>Fraction of the number of maps in the job which should be 
+  complete before reduces are scheduled for the job. 
+  </description>
+</property>
+
+<property>
+  <name>mapred.task.tracker.task-controller</name>
+  <value>org.apache.hadoop.mapred.DefaultTaskController</value>
+  <description>TaskController which is used to launch and manage task execution 
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.tasktracker.group</name>
+  <value></value>
+  <description>Expert: Group to which TaskTracker belongs. If
+   LinuxTaskController is configured via mapreduce.tasktracker.taskcontroller,
+   the group owner of the task-controller binary should be same as this group.
+  </description>
+</property>
+
+ <property>
+  <name>mapred.disk.healthChecker.interval</name>
+  <value>60000</value>
+  <description>How often the TaskTracker checks the health of its
+  local directories. Configuring this to a value smaller than the
+  heartbeat interval is equivalent to setting this to heartbeat
+  interval value.
+  </description>
+</property>
+
+<!--  Node health script variables -->
+
+<property>
+  <name>mapred.healthChecker.script.path</name>
+  <value></value>
+  <description>Absolute path to the script which is
+  periodicallyrun by the node health monitoring service to determine if
+  the node is healthy or not. If the value of this key is empty or the
+  file does not exist in the location configured here, the node health
+  monitoring service is not started.</description>
+</property>
+
+<property>
+  <name>mapred.healthChecker.interval</name>
+  <value>60000</value>
+  <description>Frequency of the node health script to be run,
+  in milliseconds</description>
+</property>
+
+<property>
+  <name>mapred.healthChecker.script.timeout</name>
+  <value>600000</value>
+  <description>Time after node health script should be killed if 
+  unresponsive and considered that the script has failed.</description>
+</property>
+
+<property>
+  <name>mapred.healthChecker.script.args</name>
+  <value></value>
+  <description>List of arguments which are to be passed to 
+  node health script when it is being launched comma seperated.
+  </description>
+</property>
+
+<!--  end of node health script variables -->
+
+<property>
+  <name>hadoop.relaxed.worker.version.check</name>
+  <value>true</value>
+  <description>
+    This option changes the behavior of tasktrackers to only check for
+    a version match (eg "0.20.2-cdh4b2") but ignore the other build
+    fields (revision, user, and source checksum) when checking for
+    compatibility with jobtrackers. In previous releases tasktrackers
+    refused to connect to jobtrackers if their build version (version,
+    revision, user, and source checksum) did not match. This behavior
+    can be restored by disabling this option.
+  </description>
+</property>
+
+<property>
+  <name>hadoop.skip.worker.version.check</name>
+  <value>false</value>
+  <description>
+    By default datanodes refuse to connect to namenodes if their build
+    revision (svn revision) do not match, and tasktrackers refuse to
+    connect to jobtrackers if their build version (version, revision,
+    user, and source checksum) do not match. This option changes the
+    behavior of hadoop workers to skip doing a version check at all.
+    This option supersedes the 'hadoop.relaxed.worker.version.check'
+    option.
+  </description>
+</property>
+
+  <!-- Encrypted Shuffle Configuration -->
+
+<property>
+  <name>mapreduce.shuffle.ssl.enabled</name>
+  <value>${hadoop.ssl.enabled}</value>
+  <description>
+    Whether to use SSL for for the Shuffle HTTP endpoints.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.shuffle.ssl.address</name>
+  <value>0.0.0.0</value>
+  <description>
+    IP Address to bind the SSL Shuffle endpoint.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.shuffle.ssl.port</name>
+  <value>50443</value>
+  <description>
+    Port to bind the SSL Shuffle endpoint.
+  </description>
+</property>
+  
+</configuration>
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
--- ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java	2012-11-02 02:56:46.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ReduceTask.java	2013-09-15 07:43:01.402925000 +0200
@@ -77,6 +77,7 @@ import org.apache.hadoop.io.compress.Com
 import org.apache.hadoop.io.compress.Decompressor;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.mapred.IFile.*;
+import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapred.Merger.Segment;
 import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
 import org.apache.hadoop.mapred.TaskTracker.TaskInProgress;
@@ -94,7 +95,7 @@ import org.apache.hadoop.util.StringUtil
 import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
 
 /** A Reduce task. */
-class ReduceTask extends Task {
+public class ReduceTask extends Task {
 
   static {                                        // register a ctor
     WritableFactories.setFactory
@@ -106,7 +107,6 @@ class ReduceTask extends Task {
   
   private static final Log LOG = LogFactory.getLog(ReduceTask.class.getName());
   private int numMaps;
-  private ReduceCopier reduceCopier;
 
   private CompressionCodec codec;
 
@@ -394,16 +394,33 @@ class ReduceTask extends Task {
 
     // Initialize the codec
     codec = initCodec();
+    ShuffleConsumerPlugin shuffleConsumerPlugin = null;
 
     boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
     if (!isLocal) {
-      reduceCopier = new ReduceCopier(umbilical, job, reporter);
-      if (!reduceCopier.fetchOutputs()) {
-        if(reduceCopier.mergeThrowable instanceof FSError) {
-          throw (FSError)reduceCopier.mergeThrowable;
+      // loads ShuffleConsumerPlugin according to configuration file
+      // +++ NOTE: This code support load of 3rd party plugins at runtime +++
+      //
+      Class<? extends ShuffleConsumerPlugin> clazz =
+               job.getClass(JobContext.SHUFFLE_CONSUMER_PLUGIN_ATTR, ReduceCopier.class, ShuffleConsumerPlugin.class);
+
+      if (clazz != ReduceCopier.class) {
+        shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);
+      }
+      else {
+        shuffleConsumerPlugin = this.new ReduceCopier(); // default plugin is an inner class of ReduceTask
+      }
+      LOG.info(" Using ShuffleConsumerPlugin : " + shuffleConsumerPlugin);
+
+      ShuffleConsumerPlugin.Context context = new ShuffleConsumerPlugin.Context(ReduceTask.this, umbilical, conf, reporter);
+      shuffleConsumerPlugin.init(context);
+
+      if (!shuffleConsumerPlugin.fetchOutputs()) {
+        if(shuffleConsumerPlugin.getMergeThrowable() instanceof FSError) {
+          throw (FSError)shuffleConsumerPlugin.getMergeThrowable();
         }
         throw new IOException("Task: " + getTaskID() + 
-            " - The reduce copier failed", reduceCopier.mergeThrowable);
+            " - The ShuffleConsumerPlugin " + clazz.getSimpleName() + " failed", shuffleConsumerPlugin.getMergeThrowable());
       }
     }
     copyPhase.complete();                         // copy is already complete
@@ -417,7 +434,7 @@ class ReduceTask extends Task {
           !conf.getKeepFailedTaskFiles(), job.getInt("io.sort.factor", 100),
           new Path(getTaskID().toString()), job.getOutputKeyComparator(),
           reporter, spilledRecordsCounter, null)
-      : reduceCopier.createKVIterator(job, rfs, reporter);
+      : shuffleConsumerPlugin.createKVIterator(job, rfs, reporter);
         
     // free up the data structures
     mapOutputFilesOnDisk.clear();
@@ -436,6 +453,9 @@ class ReduceTask extends Task {
       runOldReducer(job, umbilical, reporter, rIter, comparator, 
                     keyClass, valueClass);
     }
+    if (shuffleConsumerPlugin != null) {
+      shuffleConsumerPlugin.close();
+    }
     done(umbilical, reporter);
 
     if (sslFactory != null) {
@@ -602,11 +622,11 @@ class ReduceTask extends Task {
     OTHER_ERROR
   };
 
-  class ReduceCopier<K, V> implements MRConstants {
+  class ReduceCopier<K, V> implements ShuffleConsumerPlugin, MRConstants {
 
     /** Reference to the umbilical object */
     private TaskUmbilicalProtocol umbilical;
-    private final TaskReporter reporter;
+    private TaskReporter reporter;
     
     /** Reference to the task object */
     
@@ -688,18 +708,18 @@ class ReduceTask extends Task {
     /**
      * When we accumulate maxInMemOutputs number of files in ram, we merge/spill
      */
-    private final int maxInMemOutputs;
+    private int maxInMemOutputs;
 
     /**
      * Usage threshold for in-memory output accumulation.
      */
-    private final float maxInMemCopyPer;
+    private float maxInMemCopyPer;
 
     /**
      * Maximum memory usage of map outputs to merge from memory into
      * the reduce, in bytes.
      */
-    private final long maxInMemReduce;
+    private long maxInMemReduce;
 
     /**
      * The threads for fetching the files.
@@ -749,7 +769,7 @@ class ReduceTask extends Task {
     /**
      * Maximum number of fetch failures before reducer aborts.
      */
-    private final int abortFailureLimit;
+    private int abortFailureLimit;
 
     /**
      * Initial penalty time in ms for a fetch failure.
@@ -1866,16 +1886,16 @@ class ReduceTask extends Task {
       URLClassLoader loader = new URLClassLoader(urls, parent);
       conf.setClassLoader(loader);
     }
-    
-    public ReduceCopier(TaskUmbilicalProtocol umbilical, JobConf conf,
-                        TaskReporter reporter
-                        )throws ClassNotFoundException, IOException {
+
+    @Override
+    public void init (ShuffleConsumerPlugin.Context context)throws ClassNotFoundException, IOException {
       
+      JobConf conf = context.getConf();
+      this.reporter = context.getReporter();
+      this.umbilical = context.getUmbilical();
+      this.reduceTask = context.getReduceTask();
       configureClasspath(conf);
-      this.reporter = reporter;
       this.shuffleClientMetrics = new ShuffleClientMetrics(conf);
-      this.umbilical = umbilical;      
-      this.reduceTask = ReduceTask.this;
 
       this.scheduledCopies = new ArrayList<MapOutputLocation>(100);
       this.copyResults = new ArrayList<CopyResult>(100);    
@@ -1935,12 +1955,22 @@ class ReduceTask extends Task {
       this.reportReadErrorImmediately = 
         conf.getBoolean("mapreduce.reduce.shuffle.notify.readerror", true);
     }
-    
+
+    @Override
+    public Throwable getMergeThrowable() {
+      return mergeThrowable;
+    }
+
+    @Override
+    public void close(){
+    }
+
     private boolean busyEnough(int numInFlight) {
       return numInFlight > maxInFlight;
     }
     
     
+    @Override
     public boolean fetchOutputs() throws IOException {
       int totalFailures = 0;
       int            numInFlight = 0, numCopied = 0;
@@ -2382,8 +2412,9 @@ class ReduceTask extends Task {
      * first merge pass. If not, then said outputs must be written to disk
      * first.
      */
+    @Override
     @SuppressWarnings("unchecked")
-    private RawKeyValueIterator createKVIterator(
+    public RawKeyValueIterator createKVIterator(
         JobConf job, FileSystem fs, Reporter reporter) throws IOException {
 
       // merge config params
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java.orig ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ReduceTask.java.orig
--- ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java.orig	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ReduceTask.java.orig	2012-11-02 02:56:46.000000000 +0200
@@ -0,0 +1,2930 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.lang.Math;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
+import java.net.URI;
+import java.net.URL;
+import java.net.HttpURLConnection;
+import java.net.URLClassLoader;
+import java.net.URLConnection;
+import java.security.GeneralSecurityException;
+import java.text.DecimalFormat;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+import java.util.SortedSet;
+import java.util.TreeSet;
+import java.util.concurrent.ConcurrentHashMap;
+
+import javax.crypto.SecretKey;
+import javax.net.ssl.HttpsURLConnection;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.ChecksumFileSystem;
+import org.apache.hadoop.fs.FSError;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalFileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableFactories;
+import org.apache.hadoop.io.WritableFactory;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.compress.CodecPool;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.Decompressor;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.mapred.IFile.*;
+import org.apache.hadoop.mapred.Merger.Segment;
+import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.mapred.TaskTracker.TaskInProgress;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.metrics.MetricsContext;
+import org.apache.hadoop.metrics.MetricsRecord;
+import org.apache.hadoop.metrics.MetricsUtil;
+import org.apache.hadoop.metrics.Updater;
+import org.apache.hadoop.security.ssl.SSLFactory;
+import org.apache.hadoop.util.Progress;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+
+import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
+
+/** A Reduce task. */
+class ReduceTask extends Task {
+
+  static {                                        // register a ctor
+    WritableFactories.setFactory
+      (ReduceTask.class,
+       new WritableFactory() {
+         public Writable newInstance() { return new ReduceTask(); }
+       });
+  }
+  
+  private static final Log LOG = LogFactory.getLog(ReduceTask.class.getName());
+  private int numMaps;
+  private ReduceCopier reduceCopier;
+
+  private CompressionCodec codec;
+
+
+  { 
+    getProgress().setStatus("reduce"); 
+    setPhase(TaskStatus.Phase.SHUFFLE);        // phase to start with 
+  }
+
+  private Progress copyPhase;
+  private Progress sortPhase;
+  private Progress reducePhase;
+  private Counters.Counter reduceShuffleBytes = 
+    getCounters().findCounter(Counter.REDUCE_SHUFFLE_BYTES);
+  private Counters.Counter reduceInputKeyCounter = 
+    getCounters().findCounter(Counter.REDUCE_INPUT_GROUPS);
+  private Counters.Counter reduceInputValueCounter = 
+    getCounters().findCounter(Counter.REDUCE_INPUT_RECORDS);
+  private Counters.Counter reduceOutputCounter = 
+    getCounters().findCounter(Counter.REDUCE_OUTPUT_RECORDS);
+  private Counters.Counter reduceCombineOutputCounter =
+    getCounters().findCounter(Counter.COMBINE_OUTPUT_RECORDS);
+
+  // A custom comparator for map output files. Here the ordering is determined
+  // by the file's size and path. In case of files with same size and different
+  // file paths, the first parameter is considered smaller than the second one.
+  // In case of files with same size and path are considered equal.
+  private Comparator<FileStatus> mapOutputFileComparator = 
+    new Comparator<FileStatus>() {
+      public int compare(FileStatus a, FileStatus b) {
+        if (a.getLen() < b.getLen())
+          return -1;
+        else if (a.getLen() == b.getLen())
+          if (a.getPath().toString().equals(b.getPath().toString()))
+            return 0;
+          else
+            return -1; 
+        else
+          return 1;
+      }
+  };
+  
+  // A sorted set for keeping a set of map output files on disk
+  private final SortedSet<FileStatus> mapOutputFilesOnDisk = 
+    new TreeSet<FileStatus>(mapOutputFileComparator);
+
+  private static boolean sslShuffle;
+  private static SSLFactory sslFactory;
+
+  public ReduceTask() {
+    super();
+  }
+
+  public ReduceTask(String jobFile, TaskAttemptID taskId,
+                    int partition, int numMaps, int numSlotsRequired) {
+    super(jobFile, taskId, partition, numSlotsRequired);
+    this.numMaps = numMaps;
+  }
+  
+  private CompressionCodec initCodec() {
+    // check if map-outputs are to be compressed
+    if (conf.getCompressMapOutput()) {
+      Class<? extends CompressionCodec> codecClass =
+        conf.getMapOutputCompressorClass(DefaultCodec.class);
+      return ReflectionUtils.newInstance(codecClass, conf);
+    } 
+
+    return null;
+  }
+
+  @Override
+  public TaskRunner createRunner(TaskTracker tracker, TaskInProgress tip,
+                                 TaskTracker.RunningJob rjob
+                                 ) throws IOException {
+    return new ReduceTaskRunner(tip, tracker, this.conf, rjob);
+  }
+
+  @Override
+  public boolean isMapTask() {
+    return false;
+  }
+
+  public int getNumMaps() { return numMaps; }
+  
+  /**
+   * Localize the given JobConf to be specific for this task.
+   */
+  @Override
+  public void localizeConfiguration(JobConf conf) throws IOException {
+    super.localizeConfiguration(conf);
+    conf.setNumMapTasks(numMaps);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+
+    out.writeInt(numMaps);                        // write the number of maps
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+
+    numMaps = in.readInt();
+  }
+  
+  // Get the input files for the reducer.
+  private Path[] getMapFiles(FileSystem fs, boolean isLocal) 
+  throws IOException {
+    List<Path> fileList = new ArrayList<Path>();
+    if (isLocal) {
+      // for local jobs
+      for(int i = 0; i < numMaps; ++i) {
+        fileList.add(mapOutputFile.getInputFile(i));
+      }
+    } else {
+      // for non local jobs
+      for (FileStatus filestatus : mapOutputFilesOnDisk) {
+        fileList.add(filestatus.getPath());
+      }
+    }
+    return fileList.toArray(new Path[0]);
+  }
+
+  private class ReduceValuesIterator<KEY,VALUE> 
+          extends ValuesIterator<KEY,VALUE> {
+    public ReduceValuesIterator (RawKeyValueIterator in,
+                                 RawComparator<KEY> comparator, 
+                                 Class<KEY> keyClass,
+                                 Class<VALUE> valClass,
+                                 Configuration conf, Progressable reporter)
+      throws IOException {
+      super(in, comparator, keyClass, valClass, conf, reporter);
+    }
+
+    @Override
+    public VALUE next() {
+      reduceInputValueCounter.increment(1);
+      return moveToNext();
+    }
+    
+    protected VALUE moveToNext() {
+      return super.next();
+    }
+    
+    public void informReduceProgress() {
+      reducePhase.set(super.in.getProgress().get()); // update progress
+      reporter.progress();
+    }
+  }
+
+  private class SkippingReduceValuesIterator<KEY,VALUE> 
+     extends ReduceValuesIterator<KEY,VALUE> {
+     private SkipRangeIterator skipIt;
+     private TaskUmbilicalProtocol umbilical;
+     private Counters.Counter skipGroupCounter;
+     private Counters.Counter skipRecCounter;
+     private long grpIndex = -1;
+     private Class<KEY> keyClass;
+     private Class<VALUE> valClass;
+     private SequenceFile.Writer skipWriter;
+     private boolean toWriteSkipRecs;
+     private boolean hasNext;
+     private TaskReporter reporter;
+     
+     public SkippingReduceValuesIterator(RawKeyValueIterator in,
+         RawComparator<KEY> comparator, Class<KEY> keyClass,
+         Class<VALUE> valClass, Configuration conf, TaskReporter reporter,
+         TaskUmbilicalProtocol umbilical) throws IOException {
+       super(in, comparator, keyClass, valClass, conf, reporter);
+       this.umbilical = umbilical;
+       this.skipGroupCounter = 
+         reporter.getCounter(Counter.REDUCE_SKIPPED_GROUPS);
+       this.skipRecCounter = 
+         reporter.getCounter(Counter.REDUCE_SKIPPED_RECORDS);
+       this.toWriteSkipRecs = toWriteSkipRecs() &&  
+         SkipBadRecords.getSkipOutputPath(conf)!=null;
+       this.keyClass = keyClass;
+       this.valClass = valClass;
+       this.reporter = reporter;
+       skipIt = getSkipRanges().skipRangeIterator();
+       mayBeSkip();
+     }
+     
+     void nextKey() throws IOException {
+       super.nextKey();
+       mayBeSkip();
+     }
+     
+     boolean more() { 
+       return super.more() && hasNext; 
+     }
+     
+     private void mayBeSkip() throws IOException {
+       hasNext = skipIt.hasNext();
+       if(!hasNext) {
+         LOG.warn("Further groups got skipped.");
+         return;
+       }
+       grpIndex++;
+       long nextGrpIndex = skipIt.next();
+       long skip = 0;
+       long skipRec = 0;
+       while(grpIndex<nextGrpIndex && super.more()) {
+         while (hasNext()) {
+           VALUE value = moveToNext();
+           if(toWriteSkipRecs) {
+             writeSkippedRec(getKey(), value);
+           }
+           skipRec++;
+         }
+         super.nextKey();
+         grpIndex++;
+         skip++;
+       }
+       
+       //close the skip writer once all the ranges are skipped
+       if(skip>0 && skipIt.skippedAllRanges() && skipWriter!=null) {
+         skipWriter.close();
+       }
+       skipGroupCounter.increment(skip);
+       skipRecCounter.increment(skipRec);
+       reportNextRecordRange(umbilical, grpIndex);
+     }
+     
+     @SuppressWarnings("unchecked")
+     private void writeSkippedRec(KEY key, VALUE value) throws IOException{
+       if(skipWriter==null) {
+         Path skipDir = SkipBadRecords.getSkipOutputPath(conf);
+         Path skipFile = new Path(skipDir, getTaskID().toString());
+         skipWriter = SequenceFile.createWriter(
+               skipFile.getFileSystem(conf), conf, skipFile,
+               keyClass, valClass, 
+               CompressionType.BLOCK, reporter);
+       }
+       skipWriter.append(key, value);
+     }
+  }
+
+  @Override
+  @SuppressWarnings("unchecked")
+  public void run(JobConf job, final TaskUmbilicalProtocol umbilical)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    this.umbilical = umbilical;
+    job.setBoolean("mapred.skip.on", isSkipping());
+
+    if (isMapOrReduce()) {
+      copyPhase = getProgress().addPhase("copy");
+      sortPhase  = getProgress().addPhase("sort");
+      reducePhase = getProgress().addPhase("reduce");
+    }
+    // start thread that will handle communication with parent
+    TaskReporter reporter = new TaskReporter(getProgress(), umbilical,
+        jvmContext);
+    reporter.startCommunicationThread();
+    boolean useNewApi = job.getUseNewReducer();
+    initialize(job, getJobID(), reporter, useNewApi);
+
+    // check if it is a cleanupJobTask
+    if (jobCleanup) {
+      runJobCleanupTask(umbilical, reporter);
+      return;
+    }
+    if (jobSetup) {
+      runJobSetupTask(umbilical, reporter);
+      return;
+    }
+    if (taskCleanup) {
+      runTaskCleanupTask(umbilical, reporter);
+      return;
+    }
+
+    sslShuffle = job.getBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
+                                JobTracker.SHUFFLE_SSL_ENABLED_DEFAULT);
+    if (sslShuffle && sslFactory == null) {
+      sslFactory = new SSLFactory(SSLFactory.Mode.CLIENT, job);
+      try {
+        sslFactory.init();
+      } catch (Exception ex) {
+        sslFactory.destroy();
+        throw new RuntimeException(ex);
+      }
+    }
+
+    // Initialize the codec
+    codec = initCodec();
+
+    boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
+    if (!isLocal) {
+      reduceCopier = new ReduceCopier(umbilical, job, reporter);
+      if (!reduceCopier.fetchOutputs()) {
+        if(reduceCopier.mergeThrowable instanceof FSError) {
+          throw (FSError)reduceCopier.mergeThrowable;
+        }
+        throw new IOException("Task: " + getTaskID() + 
+            " - The reduce copier failed", reduceCopier.mergeThrowable);
+      }
+    }
+    copyPhase.complete();                         // copy is already complete
+    setPhase(TaskStatus.Phase.SORT);
+    statusUpdate(umbilical);
+
+    final FileSystem rfs = FileSystem.getLocal(job).getRaw();
+    RawKeyValueIterator rIter = isLocal
+      ? Merger.merge(job, rfs, job.getMapOutputKeyClass(),
+          job.getMapOutputValueClass(), codec, getMapFiles(rfs, true),
+          !conf.getKeepFailedTaskFiles(), job.getInt("io.sort.factor", 100),
+          new Path(getTaskID().toString()), job.getOutputKeyComparator(),
+          reporter, spilledRecordsCounter, null)
+      : reduceCopier.createKVIterator(job, rfs, reporter);
+        
+    // free up the data structures
+    mapOutputFilesOnDisk.clear();
+    
+    sortPhase.complete();                         // sort is complete
+    setPhase(TaskStatus.Phase.REDUCE); 
+    statusUpdate(umbilical);
+    Class keyClass = job.getMapOutputKeyClass();
+    Class valueClass = job.getMapOutputValueClass();
+    RawComparator comparator = job.getOutputValueGroupingComparator();
+
+    if (useNewApi) {
+      runNewReducer(job, umbilical, reporter, rIter, comparator, 
+                    keyClass, valueClass);
+    } else {
+      runOldReducer(job, umbilical, reporter, rIter, comparator, 
+                    keyClass, valueClass);
+    }
+    done(umbilical, reporter);
+
+    if (sslFactory != null) {
+      sslFactory.destroy();
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+  void runOldReducer(JobConf job,
+                     TaskUmbilicalProtocol umbilical,
+                     final TaskReporter reporter,
+                     RawKeyValueIterator rIter,
+                     RawComparator<INKEY> comparator,
+                     Class<INKEY> keyClass,
+                     Class<INVALUE> valueClass) throws IOException {
+    Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = 
+      ReflectionUtils.newInstance(job.getReducerClass(), job);
+    // make output collector
+    String finalName = getOutputName(getPartition());
+
+    FileSystem fs = FileSystem.get(job);
+
+    final RecordWriter<OUTKEY,OUTVALUE> out = 
+      job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);  
+    
+    OutputCollector<OUTKEY,OUTVALUE> collector = 
+      new OutputCollector<OUTKEY,OUTVALUE>() {
+        public void collect(OUTKEY key, OUTVALUE value)
+          throws IOException {
+          out.write(key, value);
+          reduceOutputCounter.increment(1);
+          // indicate that progress update needs to be sent
+          reporter.progress();
+        }
+      };
+    
+    // apply reduce function
+    try {
+      //increment processed counter only if skipping feature is enabled
+      boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&
+        SkipBadRecords.getAutoIncrReducerProcCount(job);
+      
+      ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ? 
+          new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter, 
+              comparator, keyClass, valueClass, 
+              job, reporter, umbilical) :
+          new ReduceValuesIterator<INKEY,INVALUE>(rIter, 
+          job.getOutputValueGroupingComparator(), keyClass, valueClass, 
+          job, reporter);
+      values.informReduceProgress();
+      while (values.more()) {
+        reduceInputKeyCounter.increment(1);
+        reducer.reduce(values.getKey(), values, collector, reporter);
+        if(incrProcCount) {
+          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, 
+              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);
+        }
+        values.nextKey();
+        values.informReduceProgress();
+      }
+
+      //Clean up: repeated in catch block below
+      reducer.close();
+      out.close(reporter);
+      //End of clean up.
+    } catch (IOException ioe) {
+      try {
+        reducer.close();
+      } catch (IOException ignored) {}
+        
+      try {
+        out.close(reporter);
+      } catch (IOException ignored) {}
+      
+      throw ioe;
+    }
+  }
+
+  static class NewTrackingRecordWriter<K,V> 
+      extends org.apache.hadoop.mapreduce.RecordWriter<K,V> {
+    private final org.apache.hadoop.mapreduce.RecordWriter<K,V> real;
+    private final org.apache.hadoop.mapreduce.Counter outputRecordCounter;
+  
+    NewTrackingRecordWriter(org.apache.hadoop.mapreduce.RecordWriter<K,V> real,
+                            org.apache.hadoop.mapreduce.Counter recordCounter) {
+      this.real = real;
+      this.outputRecordCounter = recordCounter;
+    }
+
+    @Override
+    public void close(TaskAttemptContext context) throws IOException,
+    InterruptedException {
+      real.close(context);
+    }
+
+    @Override
+    public void write(K key, V value) throws IOException, InterruptedException {
+      real.write(key,value);
+      outputRecordCounter.increment(1);
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+  void runNewReducer(JobConf job,
+                     final TaskUmbilicalProtocol umbilical,
+                     final TaskReporter reporter,
+                     RawKeyValueIterator rIter,
+                     RawComparator<INKEY> comparator,
+                     Class<INKEY> keyClass,
+                     Class<INVALUE> valueClass
+                     ) throws IOException,InterruptedException, 
+                              ClassNotFoundException {
+    // wrap value iterator to report progress.
+    final RawKeyValueIterator rawIter = rIter;
+    rIter = new RawKeyValueIterator() {
+      public void close() throws IOException {
+        rawIter.close();
+      }
+      public DataInputBuffer getKey() throws IOException {
+        return rawIter.getKey();
+      }
+      public Progress getProgress() {
+        return rawIter.getProgress();
+      }
+      public DataInputBuffer getValue() throws IOException {
+        return rawIter.getValue();
+      }
+      public boolean next() throws IOException {
+        boolean ret = rawIter.next();
+        reducePhase.set(rawIter.getProgress().get());
+        reporter.progress();
+        return ret;
+      }
+    };
+    // make a task context so we can get the classes
+    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
+      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, getTaskID());
+    // make a reducer
+    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =
+      (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)
+        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);
+    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> output =
+      (org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE>)
+        outputFormat.getRecordWriter(taskContext);
+     org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW = 
+       new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(output, reduceOutputCounter);
+    job.setBoolean("mapred.skip.on", isSkipping());
+    org.apache.hadoop.mapreduce.Reducer.Context 
+         reducerContext = createReduceContext(reducer, job, getTaskID(),
+                                               rIter, reduceInputKeyCounter,
+                                               reduceInputValueCounter, 
+                                               trackedRW, committer,
+                                               reporter, comparator, keyClass,
+                                               valueClass);
+    reducer.run(reducerContext);
+    output.close(reducerContext);
+  }
+
+  private static enum CopyOutputErrorType {
+    NO_ERROR,
+    READ_ERROR,
+    OTHER_ERROR
+  };
+
+  class ReduceCopier<K, V> implements MRConstants {
+
+    /** Reference to the umbilical object */
+    private TaskUmbilicalProtocol umbilical;
+    private final TaskReporter reporter;
+    
+    /** Reference to the task object */
+    
+    /** Number of ms before timing out a copy */
+    private static final int STALLED_COPY_TIMEOUT = 3 * 60 * 1000;
+    
+    /** Max events to fetch in one go from the tasktracker */
+    private static final int MAX_EVENTS_TO_FETCH = 10000;
+
+    /**
+     * our reduce task instance
+     */
+    private ReduceTask reduceTask;
+    
+    /**
+     * the list of map outputs currently being copied
+     */
+    private List<MapOutputLocation> scheduledCopies;
+    
+    /**
+     *  the results of dispatched copy attempts
+     */
+    private List<CopyResult> copyResults;
+    
+    int numEventsFetched = 0;
+    private Object copyResultsOrNewEventsLock = new Object();
+
+    
+    /**
+     *  the number of outputs to copy in parallel
+     */
+    private int numCopiers;
+    
+    /**
+     *  a number that is set to the max #fetches we'd schedule and then
+     *  pause the schduling
+     */
+    private int maxInFlight;
+    
+    
+    /**
+     * busy hosts from which copies are being backed off
+     * Map of host -> next contact time
+     */
+    private Map<String, Long> penaltyBox;
+    
+    /**
+     * the set of unique hosts from which we are copying
+     */
+    private Set<String> uniqueHosts;
+    
+    /**
+     * A reference to the RamManager for writing the map outputs to.
+     */
+    
+    private ShuffleRamManager ramManager;
+    
+    /**
+     * A reference to the local file system for writing the map outputs to.
+     */
+    private FileSystem localFileSys;
+
+    private FileSystem rfs;
+    /**
+     * Number of files to merge at a time
+     */
+    private int ioSortFactor;
+    
+    /**
+     * A reference to the throwable object (if merge throws an exception)
+     */
+    private volatile Throwable mergeThrowable;
+    
+    /** 
+     * A flag to indicate when to exit localFS merge
+     */
+    private volatile boolean exitLocalFSMerge = false;
+    
+    /**
+     * When we accumulate maxInMemOutputs number of files in ram, we merge/spill
+     */
+    private final int maxInMemOutputs;
+
+    /**
+     * Usage threshold for in-memory output accumulation.
+     */
+    private final float maxInMemCopyPer;
+
+    /**
+     * Maximum memory usage of map outputs to merge from memory into
+     * the reduce, in bytes.
+     */
+    private final long maxInMemReduce;
+
+    /**
+     * The threads for fetching the files.
+     */
+    private List<MapOutputCopier> copiers = null;
+    
+    /**
+     * The object for metrics reporting.
+     */
+    private ShuffleClientMetrics shuffleClientMetrics = null;
+    
+    /**
+     * the minimum interval between tasktracker polls
+     */
+    private static final long MIN_POLL_INTERVAL = 1000;
+    
+    /**
+     * a list of map output locations for fetch retrials 
+     */
+    private List<MapOutputLocation> retryFetches =
+      new ArrayList<MapOutputLocation>();
+    
+    /** 
+     * The set of required map outputs
+     */
+    private Set <TaskID> copiedMapOutputs = 
+      Collections.synchronizedSet(new TreeSet<TaskID>());
+    
+    /** 
+     * The set of obsolete map taskids.
+     */
+    private Set <TaskAttemptID> obsoleteMapIds = 
+      Collections.synchronizedSet(new TreeSet<TaskAttemptID>());
+    
+    private Random random = null;
+
+    /**
+     * the max of all the map completion times
+     */
+    private int maxMapRuntime;
+    
+    /**
+     * Maximum number of fetch-retries per-map before reporting it.
+     */
+    private int maxFetchFailuresBeforeReporting;
+    
+    /**
+     * Maximum number of fetch failures before reducer aborts.
+     */
+    private final int abortFailureLimit;
+
+    /**
+     * Initial penalty time in ms for a fetch failure.
+     */
+    private static final long INITIAL_PENALTY = 10000;
+
+    /**
+     * Penalty growth rate for each fetch failure.
+     */
+    private static final float PENALTY_GROWTH_RATE = 1.3f;
+
+    /**
+     * Default limit for maximum number of fetch failures before reporting.
+     */
+    private final static int REPORT_FAILURE_LIMIT = 10;
+
+    /**
+     * Combiner runner, if a combiner is needed
+     */
+    private CombinerRunner combinerRunner;
+
+    /**
+     * Resettable collector used for combine.
+     */
+    private CombineOutputCollector combineCollector = null;
+
+    /**
+     * Maximum percent of failed fetch attempt before killing the reduce task.
+     */
+    private static final float MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT = 0.5f;
+
+    /**
+     * Minimum percent of progress required to keep the reduce alive.
+     */
+    private static final float MIN_REQUIRED_PROGRESS_PERCENT = 0.5f;
+
+    /**
+     * Maximum percent of shuffle execution time required to keep the reducer alive.
+     */
+    private static final float MAX_ALLOWED_STALL_TIME_PERCENT = 0.5f;
+    
+    /**
+     * Minimum number of map fetch retries.
+     */
+    private static final int MIN_FETCH_RETRIES_PER_MAP = 2;
+
+    /**
+     * The minimum percentage of maps yet to be copied, 
+     * which indicates end of shuffle
+     */
+    private static final float MIN_PENDING_MAPS_PERCENT = 0.25f;
+    /**
+     * Maximum no. of unique maps from which we failed to fetch map-outputs
+     * even after {@link #maxFetchRetriesPerMap} retries; after this the
+     * reduce task is failed.
+     */
+    private int maxFailedUniqueFetches = 5;
+
+    /**
+     * The maps from which we fail to fetch map-outputs 
+     * even after {@link #maxFetchRetriesPerMap} retries.
+     */
+    Set<TaskID> fetchFailedMaps = new TreeSet<TaskID>(); 
+    
+    /**
+     * A map of taskId -> no. of failed fetches
+     */
+    Map<TaskAttemptID, Integer> mapTaskToFailedFetchesMap = 
+      new HashMap<TaskAttemptID, Integer>();    
+
+    /**
+     * Initial backoff interval (milliseconds)
+     */
+    private static final int BACKOFF_INIT = 4000; 
+    
+    /**
+     * The interval for logging in the shuffle
+     */
+    private static final int MIN_LOG_TIME = 60000;
+
+    /** 
+     * List of in-memory map-outputs.
+     */
+    private final List<MapOutput> mapOutputsFilesInMemory =
+      Collections.synchronizedList(new LinkedList<MapOutput>());
+    
+    /**
+     * The map for (Hosts, List of MapIds from this Host) maintaining
+     * map output locations
+     */
+    private final Map<String, List<MapOutputLocation>> mapLocations = 
+      new ConcurrentHashMap<String, List<MapOutputLocation>>();
+    
+    /**
+     * This class contains the methods that should be used for metrics-reporting
+     * the specific metrics for shuffle. This class actually reports the
+     * metrics for the shuffle client (the ReduceTask), and hence the name
+     * ShuffleClientMetrics.
+     */
+    class ShuffleClientMetrics implements Updater {
+      private MetricsRecord shuffleMetrics = null;
+      private int numFailedFetches = 0;
+      private int numSuccessFetches = 0;
+      private long numBytes = 0;
+      private int numThreadsBusy = 0;
+      ShuffleClientMetrics(JobConf conf) {
+        MetricsContext metricsContext = MetricsUtil.getContext("mapred");
+        this.shuffleMetrics = 
+          MetricsUtil.createRecord(metricsContext, "shuffleInput");
+        this.shuffleMetrics.setTag("user", conf.getUser());
+        this.shuffleMetrics.setTag("jobName", conf.getJobName());
+        this.shuffleMetrics.setTag("jobId", ReduceTask.this.getJobID().toString());
+        this.shuffleMetrics.setTag("taskId", getTaskID().toString());
+        this.shuffleMetrics.setTag("sessionId", conf.getSessionId());
+        metricsContext.registerUpdater(this);
+      }
+      public synchronized void inputBytes(long numBytes) {
+        this.numBytes += numBytes;
+      }
+      public synchronized void failedFetch() {
+        ++numFailedFetches;
+      }
+      public synchronized void successFetch() {
+        ++numSuccessFetches;
+      }
+      public synchronized void threadBusy() {
+        ++numThreadsBusy;
+      }
+      public synchronized void threadFree() {
+        --numThreadsBusy;
+      }
+      public void doUpdates(MetricsContext unused) {
+        synchronized (this) {
+          shuffleMetrics.incrMetric("shuffle_input_bytes", numBytes);
+          shuffleMetrics.incrMetric("shuffle_failed_fetches", 
+                                    numFailedFetches);
+          shuffleMetrics.incrMetric("shuffle_success_fetches", 
+                                    numSuccessFetches);
+          if (numCopiers != 0) {
+            shuffleMetrics.setMetric("shuffle_fetchers_busy_percent",
+                100*((float)numThreadsBusy/numCopiers));
+          } else {
+            shuffleMetrics.setMetric("shuffle_fetchers_busy_percent", 0);
+          }
+          numBytes = 0;
+          numSuccessFetches = 0;
+          numFailedFetches = 0;
+        }
+        shuffleMetrics.update();
+      }
+    }
+
+    /** Represents the result of an attempt to copy a map output */
+    private class CopyResult {
+      
+      // the map output location against which a copy attempt was made
+      private final MapOutputLocation loc;
+      
+      // the size of the file copied, -1 if the transfer failed
+      private final long size;
+      
+      //a flag signifying whether a copy result is obsolete
+      private static final int OBSOLETE = -2;
+      
+      private CopyOutputErrorType error = CopyOutputErrorType.NO_ERROR;
+      CopyResult(MapOutputLocation loc, long size) {
+        this.loc = loc;
+        this.size = size;
+      }
+
+      CopyResult(MapOutputLocation loc, long size, CopyOutputErrorType error) {
+        this.loc = loc;
+        this.size = size;
+        this.error = error;
+      }
+
+      public boolean getSuccess() { return size >= 0; }
+      public boolean isObsolete() { 
+        return size == OBSOLETE;
+      }
+      public long getSize() { return size; }
+      public String getHost() { return loc.getHost(); }
+      public MapOutputLocation getLocation() { return loc; }
+      public CopyOutputErrorType getError() { return error; }
+    }
+    
+    private int nextMapOutputCopierId = 0;
+    private boolean reportReadErrorImmediately;
+    
+    /**
+     * Abstraction to track a map-output.
+     */
+    private class MapOutputLocation {
+      TaskAttemptID taskAttemptId;
+      TaskID taskId;
+      String ttHost;
+      URL taskOutput;
+      
+      public MapOutputLocation(TaskAttemptID taskAttemptId, 
+                               String ttHost, URL taskOutput) {
+        this.taskAttemptId = taskAttemptId;
+        this.taskId = this.taskAttemptId.getTaskID();
+        this.ttHost = ttHost;
+        this.taskOutput = taskOutput;
+      }
+      
+      public TaskAttemptID getTaskAttemptId() {
+        return taskAttemptId;
+      }
+      
+      public TaskID getTaskId() {
+        return taskId;
+      }
+      
+      public String getHost() {
+        return ttHost;
+      }
+      
+      public URL getOutputLocation() {
+        return taskOutput;
+      }
+    }
+    
+    /** Describes the output of a map; could either be on disk or in-memory. */
+    private class MapOutput {
+      final TaskID mapId;
+      final TaskAttemptID mapAttemptId;
+      
+      final Path file;
+      final Configuration conf;
+      
+      byte[] data;
+      final boolean inMemory;
+      long compressedSize;
+      
+      public MapOutput(TaskID mapId, TaskAttemptID mapAttemptId, 
+                       Configuration conf, Path file, long size) {
+        this.mapId = mapId;
+        this.mapAttemptId = mapAttemptId;
+        
+        this.conf = conf;
+        this.file = file;
+        this.compressedSize = size;
+        
+        this.data = null;
+        
+        this.inMemory = false;
+      }
+      
+      public MapOutput(TaskID mapId, TaskAttemptID mapAttemptId, byte[] data, int compressedLength) {
+        this.mapId = mapId;
+        this.mapAttemptId = mapAttemptId;
+        
+        this.file = null;
+        this.conf = null;
+        
+        this.data = data;
+        this.compressedSize = compressedLength;
+        
+        this.inMemory = true;
+      }
+      
+      public void discard() throws IOException {
+        if (inMemory) {
+          data = null;
+        } else {
+          FileSystem fs = file.getFileSystem(conf);
+          fs.delete(file, true);
+        }
+      }
+    }
+    
+    class ShuffleRamManager implements RamManager {
+      /* Maximum percentage of the in-memory limit that a single shuffle can 
+       * consume*/ 
+      private static final float MAX_SINGLE_SHUFFLE_SEGMENT_FRACTION = 0.25f;
+      
+      /* Maximum percentage of shuffle-threads which can be stalled 
+       * simultaneously after which a merge is triggered. */ 
+      private static final float MAX_STALLED_SHUFFLE_THREADS_FRACTION = 0.75f;
+      
+      private final long maxSize;
+      private final long maxSingleShuffleLimit;
+      
+      private long size = 0;
+      
+      private Object dataAvailable = new Object();
+      private long fullSize = 0;
+      private int numPendingRequests = 0;
+      private int numRequiredMapOutputs = 0;
+      private int numClosed = 0;
+      private boolean closed = false;
+      
+      public ShuffleRamManager(Configuration conf) throws IOException {
+        final float maxInMemCopyUse =
+          conf.getFloat("mapred.job.shuffle.input.buffer.percent", 0.70f);
+        if (maxInMemCopyUse > 1.0 || maxInMemCopyUse < 0.0) {
+          throw new IOException("mapred.job.shuffle.input.buffer.percent" +
+                                maxInMemCopyUse);
+        }
+        // Allow unit tests to fix Runtime memory
+        maxSize = (int)(conf.getInt("mapred.job.reduce.total.mem.bytes",
+            (int)Math.min(Runtime.getRuntime().maxMemory(), Integer.MAX_VALUE))
+          * maxInMemCopyUse);
+        maxSingleShuffleLimit = (long)(maxSize * MAX_SINGLE_SHUFFLE_SEGMENT_FRACTION);
+        LOG.info("ShuffleRamManager: MemoryLimit=" + maxSize + 
+                 ", MaxSingleShuffleLimit=" + maxSingleShuffleLimit);
+      }
+      
+      public synchronized boolean reserve(int requestedSize, InputStream in) 
+      throws InterruptedException {
+        // Wait till the request can be fulfilled...
+        while ((size + requestedSize) > maxSize) {
+          
+          // Close the input...
+          if (in != null) {
+            try {
+              in.close();
+            } catch (IOException ie) {
+              LOG.info("Failed to close connection with: " + ie);
+            } finally {
+              in = null;
+            }
+          } 
+
+          // Track pending requests
+          synchronized (dataAvailable) {
+            ++numPendingRequests;
+            dataAvailable.notify();
+          }
+
+          // Wait for memory to free up
+          wait();
+          
+          // Track pending requests
+          synchronized (dataAvailable) {
+            --numPendingRequests;
+          }
+        }
+        
+        size += requestedSize;
+        
+        return (in != null);
+      }
+      
+      public synchronized void unreserve(int requestedSize) {
+        size -= requestedSize;
+        
+        synchronized (dataAvailable) {
+          fullSize -= requestedSize;
+          --numClosed;
+        }
+        
+        // Notify the threads blocked on RamManager.reserve
+        notifyAll();
+      }
+      
+      public boolean waitForDataToMerge() throws InterruptedException {
+        boolean done = false;
+        synchronized (dataAvailable) {
+                 // Start in-memory merge if manager has been closed or...
+          while (!closed
+                 &&
+                 // In-memory threshold exceeded and at least two segments
+                 // have been fetched
+                 (getPercentUsed() < maxInMemCopyPer || numClosed < 2)
+                 &&
+                 // More than "mapred.inmem.merge.threshold" map outputs
+                 // have been fetched into memory
+                 (maxInMemOutputs <= 0 || numClosed < maxInMemOutputs)
+                 && 
+                 // More than MAX... threads are blocked on the RamManager
+                 // or the blocked threads are the last map outputs to be
+                 // fetched. If numRequiredMapOutputs is zero, either
+                 // setNumCopiedMapOutputs has not been called (no map ouputs
+                 // have been fetched, so there is nothing to merge) or the
+                 // last map outputs being transferred without
+                 // contention, so a merge would be premature.
+                 (numPendingRequests < 
+                      numCopiers*MAX_STALLED_SHUFFLE_THREADS_FRACTION && 
+                  (0 == numRequiredMapOutputs ||
+                   numPendingRequests < numRequiredMapOutputs))) {
+            dataAvailable.wait();
+          }
+          done = closed;
+        }
+        return done;
+      }
+      
+      public void closeInMemoryFile(int requestedSize) {
+        synchronized (dataAvailable) {
+          fullSize += requestedSize;
+          ++numClosed;
+          dataAvailable.notify();
+        }
+      }
+      
+      public void setNumCopiedMapOutputs(int numRequiredMapOutputs) {
+        synchronized (dataAvailable) {
+          this.numRequiredMapOutputs = numRequiredMapOutputs;
+          dataAvailable.notify();
+        }
+      }
+      
+      public void close() {
+        synchronized (dataAvailable) {
+          closed = true;
+          LOG.info("Closed ram manager");
+          dataAvailable.notify();
+        }
+      }
+      
+      private float getPercentUsed() {
+        return (float)fullSize/maxSize;
+      }
+
+      boolean canFitInMemory(long requestedSize) {
+        return (requestedSize < Integer.MAX_VALUE && 
+                requestedSize < maxSingleShuffleLimit);
+      }
+    }
+
+    /** Copies map outputs as they become available */
+    private class MapOutputCopier extends Thread {
+      // basic/unit connection timeout (in milliseconds)
+      private final static int UNIT_CONNECT_TIMEOUT = 30 * 1000;
+      // default read timeout (in milliseconds)
+      private final static int DEFAULT_READ_TIMEOUT = 3 * 60 * 1000;
+      private final int shuffleConnectionTimeout;
+      private final int shuffleReadTimeout;
+
+      private MapOutputLocation currentLocation = null;
+      private int id = nextMapOutputCopierId++;
+      private Reporter reporter;
+      private boolean readError = false;
+      
+      // Decompression of map-outputs
+      private CompressionCodec codec = null;
+      private Decompressor decompressor = null;
+      
+      private final SecretKey jobTokenSecret;
+      
+      public MapOutputCopier(JobConf job, Reporter reporter, SecretKey jobTokenSecret) {
+        setName("MapOutputCopier " + reduceTask.getTaskID() + "." + id);
+        LOG.debug(getName() + " created");
+        this.reporter = reporter;
+
+        this.jobTokenSecret = jobTokenSecret;
+ 
+        shuffleConnectionTimeout =
+          job.getInt("mapreduce.reduce.shuffle.connect.timeout", STALLED_COPY_TIMEOUT);
+        shuffleReadTimeout =
+          job.getInt("mapreduce.reduce.shuffle.read.timeout", DEFAULT_READ_TIMEOUT);
+        
+        if (job.getCompressMapOutput()) {
+          Class<? extends CompressionCodec> codecClass =
+            job.getMapOutputCompressorClass(DefaultCodec.class);
+          codec = ReflectionUtils.newInstance(codecClass, job);
+          decompressor = CodecPool.getDecompressor(codec);
+        }
+      }
+      
+      /**
+       * Fail the current file that we are fetching
+       * @return were we currently fetching?
+       */
+      public synchronized boolean fail() {
+        if (currentLocation != null) {
+          finish(-1, CopyOutputErrorType.OTHER_ERROR);
+          return true;
+        } else {
+          return false;
+        }
+      }
+      
+      /**
+       * Get the current map output location.
+       */
+      public synchronized MapOutputLocation getLocation() {
+        return currentLocation;
+      }
+      
+      private synchronized void start(MapOutputLocation loc) {
+        currentLocation = loc;
+      }
+      
+      private synchronized void finish(long size, CopyOutputErrorType error) {
+        if (currentLocation != null) {
+          LOG.debug(getName() + " finishing " + currentLocation + " =" + size);
+          synchronized (copyResultsOrNewEventsLock) {
+            copyResults.add(new CopyResult(currentLocation, size, error));
+            copyResultsOrNewEventsLock.notifyAll();
+          }
+          currentLocation = null;
+        }
+      }
+      
+      /** Loop forever and fetch map outputs as they become available.
+       * The thread exits when it is interrupted by {@link ReduceTaskRunner}
+       */
+      @Override
+      public void run() {
+        while (true) {        
+          try {
+            MapOutputLocation loc = null;
+            long size = -1;
+            
+            synchronized (scheduledCopies) {
+              while (scheduledCopies.isEmpty()) {
+                scheduledCopies.wait();
+              }
+              loc = scheduledCopies.remove(0);
+            }
+            CopyOutputErrorType error = CopyOutputErrorType.OTHER_ERROR;
+            readError = false;
+            try {
+              shuffleClientMetrics.threadBusy();
+              start(loc);
+              size = copyOutput(loc);
+              shuffleClientMetrics.successFetch();
+              error = CopyOutputErrorType.NO_ERROR;
+            } catch (IOException e) {
+              LOG.warn(reduceTask.getTaskID() + " copy failed: " +
+                       loc.getTaskAttemptId() + " from " + loc.getHost());
+              LOG.warn(StringUtils.stringifyException(e));
+              shuffleClientMetrics.failedFetch();
+              if (readError) {
+                error = CopyOutputErrorType.READ_ERROR;
+              }
+              // Reset 
+              size = -1;
+            } finally {
+              shuffleClientMetrics.threadFree();
+              finish(size, error);
+            }
+          } catch (InterruptedException e) { 
+            break; // ALL DONE
+          } catch (FSError e) {
+            LOG.error("Task: " + reduceTask.getTaskID() + " - FSError: " + 
+                      StringUtils.stringifyException(e));
+            try {
+              umbilical.fsError(reduceTask.getTaskID(), e.getMessage(), jvmContext);
+            } catch (IOException io) {
+              LOG.error("Could not notify TT of FSError: " + 
+                      StringUtils.stringifyException(io));
+            }
+          } catch (Throwable th) {
+            String msg = getTaskID() + " : Map output copy failure : " 
+                         + StringUtils.stringifyException(th);
+            reportFatalError(getTaskID(), th, msg);
+          }
+        }
+        
+        if (decompressor != null) {
+          CodecPool.returnDecompressor(decompressor);
+        }
+          
+      }
+      
+      /** Copies a a map output from a remote host, via HTTP. 
+       * @param currentLocation the map output location to be copied
+       * @return the path (fully qualified) of the copied file
+       * @throws IOException if there is an error copying the file
+       * @throws InterruptedException if the copier should give up
+       */
+      private long copyOutput(MapOutputLocation loc
+                              ) throws IOException, InterruptedException {
+        // check if we still need to copy the output from this location
+        if (copiedMapOutputs.contains(loc.getTaskId()) || 
+            obsoleteMapIds.contains(loc.getTaskAttemptId())) {
+          return CopyResult.OBSOLETE;
+        } 
+ 
+        // a temp filename. If this file gets created in ramfs, we're fine,
+        // else, we will check the localFS to find a suitable final location
+        // for this path
+        TaskAttemptID reduceId = reduceTask.getTaskID();
+        Path filename =
+            new Path(String.format(
+                MapOutputFile.REDUCE_INPUT_FILE_FORMAT_STRING,
+                TaskTracker.OUTPUT, loc.getTaskId().getId()));
+
+        // Copy the map output to a temp file whose name is unique to this attempt 
+        Path tmpMapOutput = new Path(filename+"-"+id);
+        
+        // Copy the map output
+        MapOutput mapOutput = getMapOutput(loc, tmpMapOutput,
+                                           reduceId.getTaskID().getId());
+        if (mapOutput == null) {
+          throw new IOException("Failed to fetch map-output for " + 
+                                loc.getTaskAttemptId() + " from " + 
+                                loc.getHost());
+        }
+        
+        // The size of the map-output
+        long bytes = mapOutput.compressedSize;
+        
+        // lock the ReduceTask while we do the rename
+        synchronized (ReduceTask.this) {
+          if (copiedMapOutputs.contains(loc.getTaskId())) {
+            mapOutput.discard();
+            return CopyResult.OBSOLETE;
+          }
+
+          // Special case: discard empty map-outputs
+          if (bytes == 0) {
+            try {
+              mapOutput.discard();
+            } catch (IOException ioe) {
+              LOG.info("Couldn't discard output of " + loc.getTaskId());
+            }
+            
+            // Note that we successfully copied the map-output
+            noteCopiedMapOutput(loc.getTaskId());
+            
+            return bytes;
+          }
+          
+          // Process map-output
+          if (mapOutput.inMemory) {
+            // Save it in the synchronized list of map-outputs
+            mapOutputsFilesInMemory.add(mapOutput);
+          } else {
+            // Rename the temporary file to the final file; 
+            // ensure it is on the same partition
+            tmpMapOutput = mapOutput.file;
+            filename = new Path(tmpMapOutput.getParent(), filename.getName());
+            if (!localFileSys.rename(tmpMapOutput, filename)) {
+              localFileSys.delete(tmpMapOutput, true);
+              bytes = -1;
+              throw new IOException("Failed to rename map output " + 
+                  tmpMapOutput + " to " + filename);
+            }
+
+            synchronized (mapOutputFilesOnDisk) {        
+              addToMapOutputFilesOnDisk(localFileSys.getFileStatus(filename));
+            }
+          }
+
+          // Note that we successfully copied the map-output
+          noteCopiedMapOutput(loc.getTaskId());
+        }
+        
+        return bytes;
+      }
+      
+      /**
+       * Save the map taskid whose output we just copied.
+       * This function assumes that it has been synchronized on ReduceTask.this.
+       * 
+       * @param taskId map taskid
+       */
+      private void noteCopiedMapOutput(TaskID taskId) {
+        copiedMapOutputs.add(taskId);
+        ramManager.setNumCopiedMapOutputs(numMaps - copiedMapOutputs.size());
+      }
+
+      protected HttpURLConnection openConnection(URL url) throws IOException {
+        HttpURLConnection conn = (HttpURLConnection)url.openConnection();
+        if (sslShuffle) {
+          HttpsURLConnection httpsConn = (HttpsURLConnection) conn;
+          try {
+            httpsConn.setSSLSocketFactory(sslFactory.createSSLSocketFactory());
+          } catch (GeneralSecurityException ex) {
+            throw new IOException(ex);
+          }
+          httpsConn.setHostnameVerifier(sslFactory.getHostnameVerifier());
+        }
+        return conn;
+      }
+
+      /**
+       * Get the map output into a local file (either in the inmemory fs or on the 
+       * local fs) from the remote server.
+       * We use the file system so that we generate checksum files on the data.
+       * @param mapOutputLoc map-output to be fetched
+       * @param filename the filename to write the data into
+       * @param connectionTimeout number of milliseconds for connection timeout
+       * @param readTimeout number of milliseconds for read timeout
+       * @return the path of the file that got created
+       * @throws IOException when something goes wrong
+       */
+      private MapOutput getMapOutput(MapOutputLocation mapOutputLoc, 
+                                     Path filename, int reduce)
+      throws IOException, InterruptedException {
+        // Connect
+        URL url = mapOutputLoc.getOutputLocation();
+        HttpURLConnection connection = openConnection(url);
+        
+        InputStream input = setupSecureConnection(mapOutputLoc, connection);
+ 
+        // Validate header from map output
+        TaskAttemptID mapId = null;
+        try {
+          mapId =
+            TaskAttemptID.forName(connection.getHeaderField(FROM_MAP_TASK));
+        } catch (IllegalArgumentException ia) {
+          LOG.warn("Invalid map id ", ia);
+          return null;
+        }
+        TaskAttemptID expectedMapId = mapOutputLoc.getTaskAttemptId();
+        if (!mapId.equals(expectedMapId)) {
+          LOG.warn("data from wrong map:" + mapId +
+              " arrived to reduce task " + reduce +
+              ", where as expected map output should be from " + expectedMapId);
+          return null;
+        }
+        
+        long decompressedLength = 
+          Long.parseLong(connection.getHeaderField(RAW_MAP_OUTPUT_LENGTH));  
+        long compressedLength = 
+          Long.parseLong(connection.getHeaderField(MAP_OUTPUT_LENGTH));
+
+        if (compressedLength < 0 || decompressedLength < 0) {
+          LOG.warn(getName() + " invalid lengths in map output header: id: " +
+              mapId + " compressed len: " + compressedLength +
+              ", decompressed len: " + decompressedLength);
+          return null;
+        }
+        int forReduce =
+          (int)Integer.parseInt(connection.getHeaderField(FOR_REDUCE_TASK));
+        
+        if (forReduce != reduce) {
+          LOG.warn("data for the wrong reduce: " + forReduce +
+              " with compressed len: " + compressedLength +
+              ", decompressed len: " + decompressedLength +
+              " arrived to reduce task " + reduce);
+          return null;
+        }
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("header: " + mapId + ", compressed len: " + compressedLength +
+              ", decompressed len: " + decompressedLength);
+        }
+
+        //We will put a file in memory if it meets certain criteria:
+        //1. The size of the (decompressed) file should be less than 25% of 
+        //    the total inmem fs
+        //2. There is space available in the inmem fs
+        
+        // Check if this map-output can be saved in-memory
+        boolean shuffleInMemory = ramManager.canFitInMemory(decompressedLength); 
+
+        // Shuffle
+        MapOutput mapOutput = null;
+        if (shuffleInMemory) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Shuffling " + decompressedLength + " bytes (" + 
+                compressedLength + " raw bytes) " + 
+                "into RAM from " + mapOutputLoc.getTaskAttemptId());
+          }
+
+          mapOutput = shuffleInMemory(mapOutputLoc, connection, input,
+                                      (int)decompressedLength,
+                                      (int)compressedLength);
+        } else {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Shuffling " + decompressedLength + " bytes (" + 
+                compressedLength + " raw bytes) " + 
+                "into Local-FS from " + mapOutputLoc.getTaskAttemptId());
+          }
+          
+          mapOutput = shuffleToDisk(mapOutputLoc, input, filename, 
+              compressedLength);
+        }
+            
+        return mapOutput;
+      }
+      
+      private InputStream setupSecureConnection(MapOutputLocation mapOutputLoc, 
+          URLConnection connection) throws IOException {
+
+        // generate hash of the url
+        String msgToEncode = 
+          SecureShuffleUtils.buildMsgFrom(connection.getURL());
+        String encHash = SecureShuffleUtils.hashFromString(msgToEncode, 
+            jobTokenSecret);
+
+        // put url hash into http header
+        connection.setRequestProperty(
+            SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);
+        
+        InputStream input = getInputStream(connection, shuffleConnectionTimeout,
+                                           shuffleReadTimeout); 
+
+        // get the replyHash which is HMac of the encHash we sent to the server
+        String replyHash = connection.getHeaderField(
+            SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);
+        if(replyHash==null) {
+          throw new IOException("security validation of TT Map output failed");
+        }
+        if (LOG.isDebugEnabled())
+          LOG.debug("url="+msgToEncode+";encHash="+encHash+";replyHash="
+              +replyHash);
+        // verify that replyHash is HMac of encHash
+        SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);
+        if (LOG.isDebugEnabled())
+          LOG.debug("for url="+msgToEncode+" sent hash and receievd reply");
+        return input;
+      }
+
+      /** 
+       * The connection establishment is attempted multiple times and is given up 
+       * only on the last failure. Instead of connecting with a timeout of 
+       * X, we try connecting with a timeout of x < X but multiple times. 
+       */
+      private InputStream getInputStream(URLConnection connection, 
+                                         int connectionTimeout, 
+                                         int readTimeout) 
+      throws IOException {
+        int unit = 0;
+        if (connectionTimeout < 0) {
+          throw new IOException("Invalid timeout "
+                                + "[timeout = " + connectionTimeout + " ms]");
+        } else if (connectionTimeout > 0) {
+          unit = (UNIT_CONNECT_TIMEOUT > connectionTimeout)
+                 ? connectionTimeout
+                 : UNIT_CONNECT_TIMEOUT;
+        }
+        // set the read timeout to the total timeout
+        connection.setReadTimeout(readTimeout);
+        // set the connect timeout to the unit-connect-timeout
+        connection.setConnectTimeout(unit);
+        while (true) {
+          try {
+            connection.connect();
+            break;
+          } catch (IOException ioe) {
+            // update the total remaining connect-timeout
+            connectionTimeout -= unit;
+
+            // throw an exception if we have waited for timeout amount of time
+            // note that the updated value if timeout is used here
+            if (connectionTimeout == 0) {
+              throw ioe;
+            }
+
+            // reset the connect timeout for the last try
+            if (connectionTimeout < unit) {
+              unit = connectionTimeout;
+              // reset the connect time out for the final connect
+              connection.setConnectTimeout(unit);
+            }
+          }
+        }
+        try {
+          return connection.getInputStream();
+        } catch (IOException ioe) {
+          readError = true;
+          throw ioe;
+        }
+      }
+
+      private MapOutput shuffleInMemory(MapOutputLocation mapOutputLoc,
+                                        URLConnection connection, 
+                                        InputStream input,
+                                        int mapOutputLength,
+                                        int compressedLength)
+      throws IOException, InterruptedException {
+        // Reserve ram for the map-output
+        boolean createdNow = ramManager.reserve(mapOutputLength, input);
+      
+        // Reconnect if we need to
+        if (!createdNow) {
+          // Reconnect
+          try {
+            connection = openConnection(mapOutputLoc.getOutputLocation());
+            input = setupSecureConnection(mapOutputLoc, connection);
+          } catch (IOException ioe) {
+            LOG.info("Failed reopen connection to fetch map-output from " + 
+                     mapOutputLoc.getHost());
+            
+            // Inform the ram-manager
+            ramManager.closeInMemoryFile(mapOutputLength);
+            ramManager.unreserve(mapOutputLength);
+            
+            throw ioe;
+          }
+        }
+
+        IFileInputStream checksumIn = 
+          new IFileInputStream(input,compressedLength, conf);
+
+        input = checksumIn;       
+      
+        // Are map-outputs compressed?
+        if (codec != null) {
+          decompressor.reset();
+          input = codec.createInputStream(input, decompressor);
+        }
+      
+        // Copy map-output into an in-memory buffer
+        byte[] shuffleData = new byte[mapOutputLength];
+        MapOutput mapOutput = 
+          new MapOutput(mapOutputLoc.getTaskId(), 
+                        mapOutputLoc.getTaskAttemptId(), shuffleData, compressedLength);
+        
+        int bytesRead = 0;
+        try {
+          int n = IOUtils.wrappedReadForCompressedData(input, shuffleData, 0,
+              shuffleData.length);
+          while (n > 0) {
+            bytesRead += n;
+            shuffleClientMetrics.inputBytes(n);
+
+            // indicate we're making progress
+            reporter.progress();
+            n = IOUtils.wrappedReadForCompressedData(input, shuffleData,
+                bytesRead, shuffleData.length - bytesRead);
+          }
+
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Read " + bytesRead + " bytes from map-output for " +
+                mapOutputLoc.getTaskAttemptId());
+          }
+
+          input.close();
+        } catch (IOException ioe) {
+          LOG.info("Failed to shuffle from " + mapOutputLoc.getTaskAttemptId(), 
+                   ioe);
+
+          // Inform the ram-manager
+          ramManager.closeInMemoryFile(mapOutputLength);
+          ramManager.unreserve(mapOutputLength);
+          
+          // Discard the map-output
+          try {
+            mapOutput.discard();
+          } catch (IOException ignored) {
+            LOG.info("Failed to discard map-output from " + 
+                     mapOutputLoc.getTaskAttemptId(), ignored);
+          }
+          mapOutput = null;
+          
+          // Close the streams
+          IOUtils.cleanup(LOG, input);
+
+          // Re-throw
+          readError = true;
+          throw ioe;
+        }
+
+        // Close the in-memory file
+        ramManager.closeInMemoryFile(mapOutputLength);
+
+        // Sanity check
+        if (bytesRead != mapOutputLength) {
+          // Inform the ram-manager
+          ramManager.unreserve(mapOutputLength);
+          
+          // Discard the map-output
+          try {
+            mapOutput.discard();
+          } catch (IOException ignored) {
+            // IGNORED because we are cleaning up
+            LOG.info("Failed to discard map-output from " + 
+                     mapOutputLoc.getTaskAttemptId(), ignored);
+          }
+          mapOutput = null;
+
+          throw new IOException("Incomplete map output received for " +
+                                mapOutputLoc.getTaskAttemptId() + " from " +
+                                mapOutputLoc.getOutputLocation() + " (" + 
+                                bytesRead + " instead of " + 
+                                mapOutputLength + ")"
+          );
+        }
+
+        // TODO: Remove this after a 'fix' for HADOOP-3647
+        if (LOG.isDebugEnabled()) {
+          if (mapOutputLength > 0) {
+            DataInputBuffer dib = new DataInputBuffer();
+            dib.reset(shuffleData, 0, shuffleData.length);
+            LOG.debug("Rec #1 from " + mapOutputLoc.getTaskAttemptId() + 
+                " -> (" + WritableUtils.readVInt(dib) + ", " + 
+                WritableUtils.readVInt(dib) + ") from " + 
+                mapOutputLoc.getHost());
+          }
+        }
+        
+        return mapOutput;
+      }
+      
+      private MapOutput shuffleToDisk(MapOutputLocation mapOutputLoc,
+                                      InputStream input,
+                                      Path filename,
+                                      long mapOutputLength) 
+      throws IOException {
+        // Find out a suitable location for the output on local-filesystem
+        Path localFilename = 
+          lDirAlloc.getLocalPathForWrite(filename.toUri().getPath(), 
+                                         mapOutputLength, conf);
+
+        MapOutput mapOutput = 
+          new MapOutput(mapOutputLoc.getTaskId(), mapOutputLoc.getTaskAttemptId(), 
+                        conf, localFileSys.makeQualified(localFilename), 
+                        mapOutputLength);
+
+
+        // Copy data to local-disk
+        OutputStream output = null;
+        long bytesRead = 0;
+        try {
+          output = rfs.create(localFilename);
+          
+          byte[] buf = new byte[64 * 1024];
+          int n = -1;
+          try {
+            n = input.read(buf, 0, buf.length);
+          } catch (IOException ioe) {
+            readError = true;
+            throw ioe;
+          }
+          while (n > 0) {
+            bytesRead += n;
+            shuffleClientMetrics.inputBytes(n);
+            output.write(buf, 0, n);
+
+            // indicate we're making progress
+            reporter.progress();
+            try {
+              n = input.read(buf, 0, buf.length);
+            } catch (IOException ioe) {
+              readError = true;
+              throw ioe;
+            }
+          }
+
+          LOG.info("Read " + bytesRead + " bytes from map-output for " +
+              mapOutputLoc.getTaskAttemptId());
+
+          output.close();
+          input.close();
+        } catch (IOException ioe) {
+          LOG.info("Failed to shuffle from " + mapOutputLoc.getTaskAttemptId(), 
+                   ioe);
+
+          // Discard the map-output
+          try {
+            mapOutput.discard();
+          } catch (IOException ignored) {
+            LOG.info("Failed to discard map-output from " + 
+                mapOutputLoc.getTaskAttemptId(), ignored);
+          }
+          mapOutput = null;
+
+          // Close the streams
+          IOUtils.cleanup(LOG, input, output);
+
+          // Re-throw
+          throw ioe;
+        }
+
+        // Sanity check
+        if (bytesRead != mapOutputLength) {
+          try {
+            mapOutput.discard();
+          } catch (Exception ioe) {
+            // IGNORED because we are cleaning up
+            LOG.info("Failed to discard map-output from " + 
+                mapOutputLoc.getTaskAttemptId(), ioe);
+          } catch (Throwable t) {
+            String msg = getTaskID() + " : Failed in shuffle to disk :" 
+                         + StringUtils.stringifyException(t);
+            reportFatalError(getTaskID(), t, msg);
+          }
+          mapOutput = null;
+
+          throw new IOException("Incomplete map output received for " +
+                                mapOutputLoc.getTaskAttemptId() + " from " +
+                                mapOutputLoc.getOutputLocation() + " (" + 
+                                bytesRead + " instead of " + 
+                                mapOutputLength + ")"
+          );
+        }
+
+        return mapOutput;
+
+      }
+      
+    } // MapOutputCopier
+    
+    private void configureClasspath(JobConf conf)
+      throws IOException {
+      
+      // get the task and the current classloader which will become the parent
+      Task task = ReduceTask.this;
+      ClassLoader parent = conf.getClassLoader();   
+      
+      // get the work directory which holds the elements we are dynamically
+      // adding to the classpath
+      File workDir = new File(task.getJobFile()).getParentFile();
+      ArrayList<URL> urllist = new ArrayList<URL>();
+      
+      // add the jars and directories to the classpath
+      String jar = conf.getJar();
+      if (jar != null) {      
+        File jobCacheDir = new File(new Path(jar).getParent().toString());
+
+        File[] libs = new File(jobCacheDir, "lib").listFiles();
+        if (libs != null) {
+          for (int i = 0; i < libs.length; i++) {
+            urllist.add(libs[i].toURL());
+          }
+        }
+        urllist.add(new File(jobCacheDir, "classes").toURL());
+        urllist.add(jobCacheDir.toURL());
+        
+      }
+      urllist.add(workDir.toURL());
+      
+      // create a new classloader with the old classloader as its parent
+      // then set that classloader as the one used by the current jobconf
+      URL[] urls = urllist.toArray(new URL[urllist.size()]);
+      URLClassLoader loader = new URLClassLoader(urls, parent);
+      conf.setClassLoader(loader);
+    }
+    
+    public ReduceCopier(TaskUmbilicalProtocol umbilical, JobConf conf,
+                        TaskReporter reporter
+                        )throws ClassNotFoundException, IOException {
+      
+      configureClasspath(conf);
+      this.reporter = reporter;
+      this.shuffleClientMetrics = new ShuffleClientMetrics(conf);
+      this.umbilical = umbilical;      
+      this.reduceTask = ReduceTask.this;
+
+      this.scheduledCopies = new ArrayList<MapOutputLocation>(100);
+      this.copyResults = new ArrayList<CopyResult>(100);    
+      this.numCopiers = conf.getInt("mapred.reduce.parallel.copies", 5);
+      this.maxInFlight = 4 * numCopiers;
+      Counters.Counter combineInputCounter = 
+        reporter.getCounter(Task.Counter.COMBINE_INPUT_RECORDS);
+      this.combinerRunner = CombinerRunner.create(conf, getTaskID(),
+                                                  combineInputCounter,
+                                                  reporter, null);
+      if (combinerRunner != null) {
+        combineCollector = 
+          new CombineOutputCollector(reduceCombineOutputCounter);
+      }
+      
+      this.ioSortFactor = conf.getInt("io.sort.factor", 10);
+
+      this.abortFailureLimit = Math.max(30, numMaps / 10);
+
+      this.maxFetchFailuresBeforeReporting = conf.getInt(
+          "mapreduce.reduce.shuffle.maxfetchfailures", REPORT_FAILURE_LIMIT);
+
+      this.maxFailedUniqueFetches = Math.min(numMaps, 
+                                             this.maxFailedUniqueFetches);
+      this.maxInMemOutputs = conf.getInt("mapred.inmem.merge.threshold", 1000);
+      this.maxInMemCopyPer =
+        conf.getFloat("mapred.job.shuffle.merge.percent", 0.66f);
+      final float maxRedPer =
+        conf.getFloat("mapred.job.reduce.input.buffer.percent", 0f);
+      if (maxRedPer > 1.0 || maxRedPer < 0.0) {
+        throw new IOException("mapred.job.reduce.input.buffer.percent" +
+                              maxRedPer);
+      }
+      this.maxInMemReduce = (int)Math.min(
+          Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);
+
+      // Setup the RamManager
+      ramManager = new ShuffleRamManager(conf);
+
+      localFileSys = FileSystem.getLocal(conf);
+
+      rfs = ((LocalFileSystem)localFileSys).getRaw();
+
+      // hosts -> next contact time
+      this.penaltyBox = new LinkedHashMap<String, Long>();
+      
+      // hostnames
+      this.uniqueHosts = new HashSet<String>();
+      
+      // Seed the random number generator with a reasonably globally unique seed
+      long randomSeed = System.nanoTime() + 
+                        (long)Math.pow(this.reduceTask.getPartition(),
+                                       (this.reduceTask.getPartition()%10)
+                                      );
+      this.random = new Random(randomSeed);
+      this.maxMapRuntime = 0;
+      this.reportReadErrorImmediately = 
+        conf.getBoolean("mapreduce.reduce.shuffle.notify.readerror", true);
+    }
+    
+    private boolean busyEnough(int numInFlight) {
+      return numInFlight > maxInFlight;
+    }
+    
+    
+    public boolean fetchOutputs() throws IOException {
+      int totalFailures = 0;
+      int            numInFlight = 0, numCopied = 0;
+      DecimalFormat  mbpsFormat = new DecimalFormat("0.00");
+      final Progress copyPhase = 
+        reduceTask.getProgress().phase();
+      LocalFSMerger localFSMergerThread = null;
+      InMemFSMergeThread inMemFSMergeThread = null;
+      GetMapEventsThread getMapEventsThread = null;
+      
+      for (int i = 0; i < numMaps; i++) {
+        copyPhase.addPhase();       // add sub-phase per file
+      }
+      
+      copiers = new ArrayList<MapOutputCopier>(numCopiers);
+      
+      // start all the copying threads
+      for (int i=0; i < numCopiers; i++) {
+        MapOutputCopier copier = new MapOutputCopier(conf, reporter, 
+            reduceTask.getJobTokenSecret());
+        copiers.add(copier);
+        copier.start();
+      }
+      
+      //start the on-disk-merge thread
+      localFSMergerThread = new LocalFSMerger((LocalFileSystem)localFileSys);
+      //start the in memory merger thread
+      inMemFSMergeThread = new InMemFSMergeThread();
+      localFSMergerThread.start();
+      inMemFSMergeThread.start();
+      
+      // start the map events thread
+      getMapEventsThread = new GetMapEventsThread();
+      getMapEventsThread.start();
+      
+      // start the clock for bandwidth measurement
+      long startTime = System.currentTimeMillis();
+      long currentTime = startTime;
+      long lastProgressTime = startTime;
+      long lastOutputTime = 0;
+      
+        // loop until we get all required outputs
+        while (copiedMapOutputs.size() < numMaps && mergeThrowable == null) {
+          int numEventsAtStartOfScheduling;
+          synchronized (copyResultsOrNewEventsLock) {
+            numEventsAtStartOfScheduling = numEventsFetched;
+          }
+          
+          currentTime = System.currentTimeMillis();
+          boolean logNow = false;
+          if (currentTime - lastOutputTime > MIN_LOG_TIME) {
+            lastOutputTime = currentTime;
+            logNow = true;
+          }
+          if (logNow) {
+            LOG.info(reduceTask.getTaskID() + " Need another " 
+                   + (numMaps - copiedMapOutputs.size()) + " map output(s) "
+                   + "where " + numInFlight + " is already in progress");
+          }
+
+          // Put the hash entries for the failed fetches.
+          Iterator<MapOutputLocation> locItr = retryFetches.iterator();
+
+          while (locItr.hasNext()) {
+            MapOutputLocation loc = locItr.next(); 
+            List<MapOutputLocation> locList = 
+              mapLocations.get(loc.getHost());
+            
+            // Check if the list exists. Map output location mapping is cleared 
+            // once the jobtracker restarts and is rebuilt from scratch.
+            // Note that map-output-location mapping will be recreated and hence
+            // we continue with the hope that we might find some locations
+            // from the rebuild map.
+            if (locList != null) {
+              // Add to the beginning of the list so that this map is 
+              //tried again before the others and we can hasten the 
+              //re-execution of this map should there be a problem
+              locList.add(0, loc);
+            }
+          }
+
+          if (retryFetches.size() > 0) {
+            LOG.info(reduceTask.getTaskID() + ": " +  
+                  "Got " + retryFetches.size() +
+                  " map-outputs from previous failures");
+          }
+          // clear the "failed" fetches hashmap
+          retryFetches.clear();
+
+          // now walk through the cache and schedule what we can
+          int numScheduled = 0;
+          int numDups = 0;
+          
+          synchronized (scheduledCopies) {
+  
+            // Randomize the map output locations to prevent 
+            // all reduce-tasks swamping the same tasktracker
+            List<String> hostList = new ArrayList<String>();
+            hostList.addAll(mapLocations.keySet()); 
+            
+            Collections.shuffle(hostList, this.random);
+              
+            Iterator<String> hostsItr = hostList.iterator();
+
+            while (hostsItr.hasNext()) {
+            
+              String host = hostsItr.next();
+
+              List<MapOutputLocation> knownOutputsByLoc = 
+                mapLocations.get(host);
+
+              // Check if the list exists. Map output location mapping is 
+              // cleared once the jobtracker restarts and is rebuilt from 
+              // scratch.
+              // Note that map-output-location mapping will be recreated and 
+              // hence we continue with the hope that we might find some 
+              // locations from the rebuild map and add then for fetching.
+              if (knownOutputsByLoc == null || knownOutputsByLoc.size() == 0) {
+                continue;
+              }
+              
+              //Identify duplicate hosts here
+              if (uniqueHosts.contains(host)) {
+                 numDups += knownOutputsByLoc.size(); 
+                 continue;
+              }
+
+              Long penaltyEnd = penaltyBox.get(host);
+              boolean penalized = false;
+            
+              if (penaltyEnd != null) {
+                if (currentTime < penaltyEnd.longValue()) {
+                  penalized = true;
+                } else {
+                  penaltyBox.remove(host);
+                }
+              }
+              
+              if (penalized)
+                continue;
+
+              synchronized (knownOutputsByLoc) {
+              
+                locItr = knownOutputsByLoc.iterator();
+            
+                while (locItr.hasNext()) {
+              
+                  MapOutputLocation loc = locItr.next();
+              
+                  // Do not schedule fetches from OBSOLETE maps
+                  if (obsoleteMapIds.contains(loc.getTaskAttemptId())) {
+                    locItr.remove();
+                    continue;
+                  }
+
+                  uniqueHosts.add(host);
+                  scheduledCopies.add(loc);
+                  locItr.remove();  // remove from knownOutputs
+                  numInFlight++; numScheduled++;
+
+                  break; //we have a map from this host
+                }
+              }
+            }
+            scheduledCopies.notifyAll();
+          }
+
+          if (numScheduled > 0 || logNow) {
+            LOG.info(reduceTask.getTaskID() + " Scheduled " + numScheduled +
+                   " outputs (" + penaltyBox.size() +
+                   " slow hosts and" + numDups + " dup hosts)");
+          }
+
+          if (penaltyBox.size() > 0 && logNow) {
+            LOG.info("Penalized(slow) Hosts: ");
+            for (String host : penaltyBox.keySet()) {
+              LOG.info(host + " Will be considered after: " + 
+                  ((penaltyBox.get(host) - currentTime)/1000) + " seconds.");
+            }
+          }
+
+          // if we have no copies in flight and we can't schedule anything
+          // new, just wait for a bit
+          try {
+            if (numInFlight == 0 && numScheduled == 0) {
+              // we should indicate progress as we don't want TT to think
+              // we're stuck and kill us
+              reporter.progress();
+              synchronized (copyResultsOrNewEventsLock) {
+                copyResultsOrNewEventsLock.wait(5000);
+              }
+            }
+          } catch (InterruptedException e) { } // IGNORE
+          
+          while (numInFlight > 0 && mergeThrowable == null) {
+            LOG.debug(reduceTask.getTaskID() + " numInFlight = " + 
+                      numInFlight);
+            //the call to getCopyResult will either 
+            //1) return immediately with a null or a valid CopyResult object,
+            //                 or
+            //2) if the numInFlight is above maxInFlight, return with a 
+            //   CopyResult object after getting a notification from a 
+            //   fetcher thread, 
+            //So, when getCopyResult returns null, we can be sure that
+            //we aren't busy enough and we should go and get more mapcompletion
+            //events from the tasktracker
+            CopyResult cr = getCopyResult(numInFlight, numEventsAtStartOfScheduling);
+
+            if (cr == null) {
+              break;
+            }
+            
+            if (cr.getSuccess()) {  // a successful copy
+              numCopied++;
+              lastProgressTime = System.currentTimeMillis();
+              reduceShuffleBytes.increment(cr.getSize());
+                
+              long secsSinceStart = 
+                (System.currentTimeMillis()-startTime)/1000+1;
+              float mbs = ((float)reduceShuffleBytes.getCounter())/(1024*1024);
+              float transferRate = mbs/secsSinceStart;
+                
+              copyPhase.startNextPhase();
+              copyPhase.setStatus("copy (" + numCopied + " of " + numMaps 
+                                  + " at " +
+                                  mbpsFormat.format(transferRate) +  " MB/s)");
+                
+              // Note successful fetch for this mapId to invalidate
+              // (possibly) old fetch-failures
+              fetchFailedMaps.remove(cr.getLocation().getTaskId());
+            } else if (cr.isObsolete()) {
+              //ignore
+              LOG.info(reduceTask.getTaskID() + 
+                       " Ignoring obsolete copy result for Map Task: " + 
+                       cr.getLocation().getTaskAttemptId() + " from host: " + 
+                       cr.getHost());
+            } else {
+              retryFetches.add(cr.getLocation());
+              
+              // note the failed-fetch
+              TaskAttemptID mapTaskId = cr.getLocation().getTaskAttemptId();
+              TaskID mapId = cr.getLocation().getTaskId();
+              
+              totalFailures++;
+              Integer noFailedFetches = 
+                mapTaskToFailedFetchesMap.get(mapTaskId);
+              noFailedFetches = 
+                (noFailedFetches == null) ? 1 : (noFailedFetches + 1);
+              mapTaskToFailedFetchesMap.put(mapTaskId, noFailedFetches);
+              LOG.info("Task " + getTaskID() + ": Failed fetch #" + 
+                       noFailedFetches + " from " + mapTaskId);
+
+              if (noFailedFetches >= abortFailureLimit) {
+                LOG.fatal(noFailedFetches + " failures downloading "
+                          + getTaskID() + ".");
+                umbilical.shuffleError(getTaskID(),
+                                 "Exceeded the abort failure limit;"
+                                 + " bailing-out.", jvmContext);
+              }
+              
+              checkAndInformJobTracker(noFailedFetches, mapTaskId,
+                  cr.getError().equals(CopyOutputErrorType.READ_ERROR));
+
+              // note unique failed-fetch maps
+              if (noFailedFetches == maxFetchFailuresBeforeReporting) {
+                fetchFailedMaps.add(mapId);
+                  
+                // did we have too many unique failed-fetch maps?
+                // and did we fail on too many fetch attempts?
+                // and did we progress enough
+                //     or did we wait for too long without any progress?
+               
+                // check if the reducer is healthy
+                boolean reducerHealthy = 
+                    (((float)totalFailures / (totalFailures + numCopied)) 
+                     < MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT);
+                
+                // check if the reducer has progressed enough
+                boolean reducerProgressedEnough = 
+                    (((float)numCopied / numMaps) 
+                     >= MIN_REQUIRED_PROGRESS_PERCENT);
+                
+                // check if the reducer is stalled for a long time
+                // duration for which the reducer is stalled
+                int stallDuration = 
+                    (int)(System.currentTimeMillis() - lastProgressTime);
+                // duration for which the reducer ran with progress
+                int shuffleProgressDuration = 
+                    (int)(lastProgressTime - startTime);
+                // min time the reducer should run without getting killed
+                int minShuffleRunDuration = 
+                    (shuffleProgressDuration > maxMapRuntime) 
+                    ? shuffleProgressDuration 
+                    : maxMapRuntime;
+                boolean reducerStalled = 
+                    (((float)stallDuration / minShuffleRunDuration) 
+                     >= MAX_ALLOWED_STALL_TIME_PERCENT);
+                
+                // kill if not healthy and has insufficient progress
+                if ((fetchFailedMaps.size() >= maxFailedUniqueFetches ||
+                     fetchFailedMaps.size() == (numMaps - copiedMapOutputs.size()))
+                    && !reducerHealthy 
+                    && (!reducerProgressedEnough || reducerStalled)) { 
+                  LOG.fatal("Shuffle failed with too many fetch failures " + 
+                            "and insufficient progress!" +
+                            "Killing task " + getTaskID() + ".");
+                  umbilical.shuffleError(getTaskID(), 
+                                         "Exceeded MAX_FAILED_UNIQUE_FETCHES;"
+                                         + " bailing-out.", jvmContext);
+                }
+
+              }
+                
+              currentTime = System.currentTimeMillis();
+              long currentBackOff = (long)(INITIAL_PENALTY *
+                  Math.pow(PENALTY_GROWTH_RATE, noFailedFetches));
+
+              penaltyBox.put(cr.getHost(), currentTime + currentBackOff);
+              LOG.warn(reduceTask.getTaskID() + " adding host " +
+                       cr.getHost() + " to penalty box, next contact in " +
+                       (currentBackOff/1000) + " seconds");
+            }
+            uniqueHosts.remove(cr.getHost());
+            numInFlight--;
+          }
+        }
+        
+        // all done, inform the copiers to exit
+        try {
+          getMapEventsThread.terminate();
+          LOG.info("getMapsEventsThread joined.");
+        } catch (InterruptedException ie) {
+          LOG.info("getMapsEventsThread threw an exception: " +
+              StringUtils.stringifyException(ie));
+        }
+
+        synchronized (copiers) {
+          synchronized (scheduledCopies) {
+            for (MapOutputCopier copier : copiers) {
+              copier.interrupt();
+            }
+            copiers.clear();
+          }
+        }
+        
+        // copiers are done, exit and notify the waiting merge threads
+        synchronized (mapOutputFilesOnDisk) {
+          exitLocalFSMerge = true;
+          mapOutputFilesOnDisk.notify();
+        }
+        
+        ramManager.close();
+        
+        //Do a merge of in-memory files (if there are any)
+        if (mergeThrowable == null) {
+          try {
+            // Wait for the on-disk merge to complete
+            localFSMergerThread.join();
+            LOG.info("Interleaved on-disk merge complete: " + 
+                     mapOutputFilesOnDisk.size() + " files left.");
+            
+            //wait for an ongoing merge (if it is in flight) to complete
+            inMemFSMergeThread.join();
+            LOG.info("In-memory merge complete: " + 
+                     mapOutputsFilesInMemory.size() + " files left.");
+            } catch (InterruptedException ie) {
+            LOG.warn(reduceTask.getTaskID() +
+                     " Final merge of the inmemory files threw an exception: " + 
+                     StringUtils.stringifyException(ie));
+            // check if the last merge generated an error
+            if (mergeThrowable != null) {
+              mergeThrowable = ie;
+            }
+            return false;
+          }
+        }
+        return mergeThrowable == null && copiedMapOutputs.size() == numMaps;
+    }
+    
+    // Notify the JobTracker
+    // after every read error, if 'reportReadErrorImmediately' is true or
+    // after every 'maxFetchFailuresBeforeReporting' failures
+    protected void checkAndInformJobTracker(
+        int failures, TaskAttemptID mapId, boolean readError) {
+      if ((reportReadErrorImmediately && readError)
+          || ((failures % maxFetchFailuresBeforeReporting) == 0)) {
+        synchronized (ReduceTask.this) {
+          taskStatus.addFetchFailedMap(mapId);
+          reporter.progress();
+          LOG.info("Failed to fetch map-output from " + mapId +
+                   " even after MAX_FETCH_RETRIES_PER_MAP retries... "
+                   + " or it is a read error, "
+                   + " reporting to the JobTracker");
+        }
+      }
+    }
+
+
+
+    private long createInMemorySegments(
+        List<Segment<K, V>> inMemorySegments, long leaveBytes)
+        throws IOException {
+      long totalSize = 0L;
+      synchronized (mapOutputsFilesInMemory) {
+        // fullSize could come from the RamManager, but files can be
+        // closed but not yet present in mapOutputsFilesInMemory
+        long fullSize = 0L;
+        for (MapOutput mo : mapOutputsFilesInMemory) {
+          fullSize += mo.data.length;
+        }
+        while(fullSize > leaveBytes) {
+          MapOutput mo = mapOutputsFilesInMemory.remove(0);
+          totalSize += mo.data.length;
+          fullSize -= mo.data.length;
+          Reader<K, V> reader = 
+            new InMemoryReader<K, V>(ramManager, mo.mapAttemptId,
+                                     mo.data, 0, mo.data.length);
+          Segment<K, V> segment = 
+            new Segment<K, V>(reader, true);
+          inMemorySegments.add(segment);
+        }
+      }
+      return totalSize;
+    }
+
+    /**
+     * Create a RawKeyValueIterator from copied map outputs. All copying
+     * threads have exited, so all of the map outputs are available either in
+     * memory or on disk. We also know that no merges are in progress, so
+     * synchronization is more lax, here.
+     *
+     * The iterator returned must satisfy the following constraints:
+     *   1. Fewer than io.sort.factor files may be sources
+     *   2. No more than maxInMemReduce bytes of map outputs may be resident
+     *      in memory when the reduce begins
+     *
+     * If we must perform an intermediate merge to satisfy (1), then we can
+     * keep the excluded outputs from (2) in memory and include them in the
+     * first merge pass. If not, then said outputs must be written to disk
+     * first.
+     */
+    @SuppressWarnings("unchecked")
+    private RawKeyValueIterator createKVIterator(
+        JobConf job, FileSystem fs, Reporter reporter) throws IOException {
+
+      // merge config params
+      Class<K> keyClass = (Class<K>)job.getMapOutputKeyClass();
+      Class<V> valueClass = (Class<V>)job.getMapOutputValueClass();
+      boolean keepInputs = job.getKeepFailedTaskFiles();
+      final Path tmpDir = new Path(getTaskID().toString());
+      final RawComparator<K> comparator =
+        (RawComparator<K>)job.getOutputKeyComparator();
+
+      // segments required to vacate memory
+      List<Segment<K,V>> memDiskSegments = new ArrayList<Segment<K,V>>();
+      long inMemToDiskBytes = 0;
+      if (mapOutputsFilesInMemory.size() > 0) {
+        TaskID mapId = mapOutputsFilesInMemory.get(0).mapId;
+        inMemToDiskBytes = createInMemorySegments(memDiskSegments,
+            maxInMemReduce);
+        final int numMemDiskSegments = memDiskSegments.size();
+        if (numMemDiskSegments > 0 &&
+              ioSortFactor > mapOutputFilesOnDisk.size()) {
+          // must spill to disk, but can't retain in-mem for intermediate merge
+          final Path outputPath =
+              mapOutputFile.getInputFileForWrite(mapId, inMemToDiskBytes);
+          final RawKeyValueIterator rIter = Merger.merge(job, fs,
+              keyClass, valueClass, memDiskSegments, numMemDiskSegments,
+              tmpDir, comparator, reporter, spilledRecordsCounter, null);
+          final Writer writer = new Writer(job, fs, outputPath,
+              keyClass, valueClass, codec, null);
+          try {
+            Merger.writeFile(rIter, writer, reporter, job);
+            addToMapOutputFilesOnDisk(fs.getFileStatus(outputPath));
+          } catch (Exception e) {
+            if (null != outputPath) {
+              fs.delete(outputPath, true);
+            }
+            throw new IOException("Final merge failed", e);
+          } finally {
+            if (null != writer) {
+              writer.close();
+            }
+          }
+          LOG.info("Merged " + numMemDiskSegments + " segments, " +
+                   inMemToDiskBytes + " bytes to disk to satisfy " +
+                   "reduce memory limit");
+          inMemToDiskBytes = 0;
+          memDiskSegments.clear();
+        } else if (inMemToDiskBytes != 0) {
+          LOG.info("Keeping " + numMemDiskSegments + " segments, " +
+                   inMemToDiskBytes + " bytes in memory for " +
+                   "intermediate, on-disk merge");
+        }
+      }
+
+      // segments on disk
+      List<Segment<K,V>> diskSegments = new ArrayList<Segment<K,V>>();
+      long onDiskBytes = inMemToDiskBytes;
+      Path[] onDisk = getMapFiles(fs, false);
+      for (Path file : onDisk) {
+        onDiskBytes += fs.getFileStatus(file).getLen();
+        diskSegments.add(new Segment<K, V>(job, fs, file, codec, keepInputs));
+      }
+      LOG.info("Merging " + onDisk.length + " files, " +
+               onDiskBytes + " bytes from disk");
+      Collections.sort(diskSegments, new Comparator<Segment<K,V>>() {
+        public int compare(Segment<K, V> o1, Segment<K, V> o2) {
+          if (o1.getLength() == o2.getLength()) {
+            return 0;
+          }
+          return o1.getLength() < o2.getLength() ? -1 : 1;
+        }
+      });
+
+      // build final list of segments from merged backed by disk + in-mem
+      List<Segment<K,V>> finalSegments = new ArrayList<Segment<K,V>>();
+      long inMemBytes = createInMemorySegments(finalSegments, 0);
+      LOG.info("Merging " + finalSegments.size() + " segments, " +
+               inMemBytes + " bytes from memory into reduce");
+      if (0 != onDiskBytes) {
+        final int numInMemSegments = memDiskSegments.size();
+        diskSegments.addAll(0, memDiskSegments);
+        memDiskSegments.clear();
+        RawKeyValueIterator diskMerge = Merger.merge(
+            job, fs, keyClass, valueClass, codec, diskSegments,
+            ioSortFactor, numInMemSegments, tmpDir, comparator,
+            reporter, false, spilledRecordsCounter, null);
+        diskSegments.clear();
+        if (0 == finalSegments.size()) {
+          return diskMerge;
+        }
+        finalSegments.add(new Segment<K,V>(
+              new RawKVIteratorReader(diskMerge, onDiskBytes), true));
+      }
+      return Merger.merge(job, fs, keyClass, valueClass,
+                   finalSegments, finalSegments.size(), tmpDir,
+                   comparator, reporter, spilledRecordsCounter, null);
+    }
+
+    class RawKVIteratorReader extends IFile.Reader<K,V> {
+
+      private final RawKeyValueIterator kvIter;
+
+      public RawKVIteratorReader(RawKeyValueIterator kvIter, long size)
+          throws IOException {
+        super(null, null, size, null, spilledRecordsCounter);
+        this.kvIter = kvIter;
+      }
+
+      public boolean next(DataInputBuffer key, DataInputBuffer value)
+          throws IOException {
+        if (kvIter.next()) {
+          final DataInputBuffer kb = kvIter.getKey();
+          final DataInputBuffer vb = kvIter.getValue();
+          final int kp = kb.getPosition();
+          final int klen = kb.getLength() - kp;
+          key.reset(kb.getData(), kp, klen);
+          final int vp = vb.getPosition();
+          final int vlen = vb.getLength() - vp;
+          value.reset(vb.getData(), vp, vlen);
+          bytesRead += klen + vlen;
+          return true;
+        }
+        return false;
+      }
+
+      public long getPosition() throws IOException {
+        return bytesRead;
+      }
+
+      public void close() throws IOException {
+        kvIter.close();
+      }
+    }
+
+    private CopyResult getCopyResult(int numInFlight, int numEventsAtStartOfScheduling) {
+      boolean waitedForNewEvents = false;
+      
+      synchronized (copyResultsOrNewEventsLock) {
+        while (copyResults.isEmpty()) {
+          try {
+            //The idea is that if we have scheduled enough, we can wait until
+            // we hear from one of the copiers, or until there are new
+            // map events ready to be scheduled
+            if (busyEnough(numInFlight)) {
+              // All of the fetcher threads are busy. So, no sense trying
+              // to schedule more until one finishes.
+              copyResultsOrNewEventsLock.wait();
+            } else if (numEventsFetched == numEventsAtStartOfScheduling &&
+                       !waitedForNewEvents) {
+              // no sense trying to schedule more, since there are no
+              // new events to even try to schedule.
+              // We could handle this with a normal wait() without a timeout,
+              // but since this code is being introduced in a stable branch,
+              // we want to be very conservative. A 2-second wait is enough
+              // to prevent the busy-loop experienced before.
+              waitedForNewEvents = true;
+              copyResultsOrNewEventsLock.wait(2000);
+            } else {
+              return null;
+            }
+          } catch (InterruptedException e) { }
+        }
+        return copyResults.remove(0);
+      }    
+    }
+    
+    private void addToMapOutputFilesOnDisk(FileStatus status) {
+      synchronized (mapOutputFilesOnDisk) {
+        mapOutputFilesOnDisk.add(status);
+        mapOutputFilesOnDisk.notify();
+      }
+    }
+    
+    
+    
+    /** Starts merging the local copy (on disk) of the map's output so that
+     * most of the reducer's input is sorted i.e overlapping shuffle
+     * and merge phases.
+     */
+    private class LocalFSMerger extends Thread {
+      private LocalFileSystem localFileSys;
+
+      public LocalFSMerger(LocalFileSystem fs) {
+        this.localFileSys = fs;
+        setName("Thread for merging on-disk files");
+        setDaemon(true);
+      }
+
+      @SuppressWarnings("unchecked")
+      public void run() {
+        try {
+          LOG.info(reduceTask.getTaskID() + " Thread started: " + getName());
+          while(!exitLocalFSMerge){
+            synchronized (mapOutputFilesOnDisk) {
+              while (!exitLocalFSMerge &&
+                  mapOutputFilesOnDisk.size() < (2 * ioSortFactor - 1)) {
+                LOG.info(reduceTask.getTaskID() + " Thread waiting: " + getName());
+                mapOutputFilesOnDisk.wait();
+              }
+            }
+            if(exitLocalFSMerge) {//to avoid running one extra time in the end
+              break;
+            }
+            List<Path> mapFiles = new ArrayList<Path>();
+            long approxOutputSize = 0;
+            int bytesPerSum = 
+              reduceTask.getConf().getInt("io.bytes.per.checksum", 512);
+            LOG.info(reduceTask.getTaskID() + "We have  " + 
+                mapOutputFilesOnDisk.size() + " map outputs on disk. " +
+                "Triggering merge of " + ioSortFactor + " files");
+            // 1. Prepare the list of files to be merged. This list is prepared
+            // using a list of map output files on disk. Currently we merge
+            // io.sort.factor files into 1.
+            synchronized (mapOutputFilesOnDisk) {
+              for (int i = 0; i < ioSortFactor; ++i) {
+                FileStatus filestatus = mapOutputFilesOnDisk.first();
+                mapOutputFilesOnDisk.remove(filestatus);
+                mapFiles.add(filestatus.getPath());
+                approxOutputSize += filestatus.getLen();
+              }
+            }
+            
+            // sanity check
+            if (mapFiles.size() == 0) {
+                return;
+            }
+            
+            // add the checksum length
+            approxOutputSize += ChecksumFileSystem
+                                .getChecksumLength(approxOutputSize,
+                                                   bytesPerSum);
+  
+            // 2. Start the on-disk merge process
+            Path outputPath = 
+              lDirAlloc.getLocalPathForWrite(mapFiles.get(0).toString(), 
+                                             approxOutputSize, conf)
+              .suffix(".merged");
+            Writer writer = 
+              new Writer(conf,rfs, outputPath, 
+                         conf.getMapOutputKeyClass(), 
+                         conf.getMapOutputValueClass(),
+                         codec, null);
+            RawKeyValueIterator iter  = null;
+            Path tmpDir = new Path(reduceTask.getTaskID().toString());
+            try {
+              iter = Merger.merge(conf, rfs,
+                                  conf.getMapOutputKeyClass(),
+                                  conf.getMapOutputValueClass(),
+                                  codec, mapFiles.toArray(new Path[mapFiles.size()]), 
+                                  true, ioSortFactor, tmpDir, 
+                                  conf.getOutputKeyComparator(), reporter,
+                                  spilledRecordsCounter, null);
+              
+              Merger.writeFile(iter, writer, reporter, conf);
+              writer.close();
+            } catch (Exception e) {
+              localFileSys.delete(outputPath, true);
+              throw new IOException (StringUtils.stringifyException(e));
+            }
+            
+            synchronized (mapOutputFilesOnDisk) {
+              addToMapOutputFilesOnDisk(localFileSys.getFileStatus(outputPath));
+            }
+            
+            LOG.info(reduceTask.getTaskID() +
+                     " Finished merging " + mapFiles.size() + 
+                     " map output files on disk of total-size " + 
+                     approxOutputSize + "." + 
+                     " Local output file is " + outputPath + " of size " +
+                     localFileSys.getFileStatus(outputPath).getLen());
+            }
+        } catch (Exception e) {
+          LOG.warn(reduceTask.getTaskID()
+                   + " Merging of the local FS files threw an exception: "
+                   + StringUtils.stringifyException(e));
+          if (mergeThrowable == null) {
+            mergeThrowable = e;
+          }
+        } catch (Throwable t) {
+          String msg = getTaskID() + " : Failed to merge on the local FS" 
+                       + StringUtils.stringifyException(t);
+          reportFatalError(getTaskID(), t, msg);
+        }
+      }
+    }
+
+    private class InMemFSMergeThread extends Thread {
+      
+      public InMemFSMergeThread() {
+        setName("Thread for merging in memory files");
+        setDaemon(true);
+      }
+      
+      public void run() {
+        LOG.info(reduceTask.getTaskID() + " Thread started: " + getName());
+        try {
+          boolean exit = false;
+          do {
+            exit = ramManager.waitForDataToMerge();
+            if (!exit) {
+              doInMemMerge();
+            }
+          } while (!exit);
+        } catch (Exception e) {
+          LOG.warn(reduceTask.getTaskID() +
+                   " Merge of the inmemory files threw an exception: "
+                   + StringUtils.stringifyException(e));
+          ReduceCopier.this.mergeThrowable = e;
+        } catch (Throwable t) {
+          String msg = getTaskID() + " : Failed to merge in memory" 
+                       + StringUtils.stringifyException(t);
+          reportFatalError(getTaskID(), t, msg);
+        }
+      }
+      
+      @SuppressWarnings("unchecked")
+      private void doInMemMerge() throws IOException{
+        if (mapOutputsFilesInMemory.size() == 0) {
+          return;
+        }
+        
+        //name this output file same as the name of the first file that is 
+        //there in the current list of inmem files (this is guaranteed to
+        //be absent on the disk currently. So we don't overwrite a prev. 
+        //created spill). Also we need to create the output file now since
+        //it is not guaranteed that this file will be present after merge
+        //is called (we delete empty files as soon as we see them
+        //in the merge method)
+
+        //figure out the mapId 
+        TaskID mapId = mapOutputsFilesInMemory.get(0).mapId;
+
+        List<Segment<K, V>> inMemorySegments = new ArrayList<Segment<K,V>>();
+        long mergeOutputSize = createInMemorySegments(inMemorySegments, 0);
+        int noInMemorySegments = inMemorySegments.size();
+
+        Path outputPath =
+            mapOutputFile.getInputFileForWrite(mapId, mergeOutputSize);
+
+        Writer writer = 
+          new Writer(conf, rfs, outputPath,
+                     conf.getMapOutputKeyClass(),
+                     conf.getMapOutputValueClass(),
+                     codec, null);
+
+        RawKeyValueIterator rIter = null;
+        try {
+          LOG.info("Initiating in-memory merge with " + noInMemorySegments + 
+                   " segments...");
+          
+          rIter = Merger.merge(conf, rfs,
+                               (Class<K>)conf.getMapOutputKeyClass(),
+                               (Class<V>)conf.getMapOutputValueClass(),
+                               inMemorySegments, inMemorySegments.size(),
+                               new Path(reduceTask.getTaskID().toString()),
+                               conf.getOutputKeyComparator(), reporter,
+                               spilledRecordsCounter, null);
+          
+          if (combinerRunner == null) {
+            Merger.writeFile(rIter, writer, reporter, conf);
+          } else {
+            combineCollector.setWriter(writer);
+            combinerRunner.combine(rIter, combineCollector);
+          }
+          writer.close();
+
+          LOG.info(reduceTask.getTaskID() + 
+              " Merge of the " + noInMemorySegments +
+              " files in-memory complete." +
+              " Local file is " + outputPath + " of size " + 
+              localFileSys.getFileStatus(outputPath).getLen());
+        } catch (Exception e) { 
+          //make sure that we delete the ondisk file that we created 
+          //earlier when we invoked cloneFileAttributes
+          localFileSys.delete(outputPath, true);
+          throw (IOException)new IOException
+                  ("Intermediate merge failed").initCause(e);
+        }
+
+        // Note the output of the merge
+        FileStatus status = localFileSys.getFileStatus(outputPath);
+        synchronized (mapOutputFilesOnDisk) {
+          addToMapOutputFilesOnDisk(status);
+        }
+      }
+    }
+
+    private class GetMapEventsThread extends Thread {
+      
+      /** 
+       * A flag to indicate when to exit getMapEvents thread 
+       */
+      private volatile boolean exitGetMapEvents = false;
+      private IntWritable fromEventId = new IntWritable(0);
+      private static final long SLEEP_TIME = 1000;
+      
+      public GetMapEventsThread() {
+        setName("Thread for polling Map Completion Events");
+        setDaemon(true);
+      }
+      
+      @Override
+      public void run() {
+      
+        LOG.info(reduceTask.getTaskID() + " Thread started: " + getName());
+        
+        do {
+          try {
+            int numNewMaps = getMapCompletionEvents();
+            if (numNewMaps > 0) {
+              synchronized (copyResultsOrNewEventsLock) {
+                numEventsFetched += numNewMaps;
+                copyResultsOrNewEventsLock.notifyAll();
+              }
+            }
+            if (LOG.isDebugEnabled()) {
+              if (numNewMaps > 0) {
+                LOG.debug(reduceTask.getTaskID() + ": " +  
+                    "Got " + numNewMaps + " new map-outputs"); 
+              }
+            }
+            synchronized(this) {
+              wait(SLEEP_TIME);
+            }
+          } 
+          catch (InterruptedException e) {
+            LOG.warn(reduceTask.getTaskID() +
+                " GetMapEventsThread returning after an " +
+                " interrupted exception");
+            return;
+          }
+          catch (Throwable t) {
+            String msg = reduceTask.getTaskID()
+                         + " GetMapEventsThread Ignoring exception : " 
+                         + StringUtils.stringifyException(t);
+            reportFatalError(getTaskID(), t, msg);
+          }
+        } while (!exitGetMapEvents);
+
+        LOG.info("GetMapEventsThread exiting");
+      
+      }
+      
+      public void terminate() throws InterruptedException {
+        exitGetMapEvents = true;
+        synchronized(this) {
+          notifyAll();
+        }
+        join();
+      }
+      
+      /** 
+       * Queries the {@link TaskTracker} for a set of map-completion events 
+       * from a given event ID.
+       * @throws IOException
+       */  
+      private int getMapCompletionEvents() throws IOException {
+        
+        int numNewMaps = 0;
+        
+        MapTaskCompletionEventsUpdate update = 
+          umbilical.getMapCompletionEvents(reduceTask.getJobID(), 
+                                           fromEventId.get(), 
+                                           MAX_EVENTS_TO_FETCH,
+                                           reduceTask.getTaskID(), jvmContext);
+        TaskCompletionEvent events[] = update.getMapTaskCompletionEvents();
+          
+        // Check if the reset is required.
+        // Since there is no ordering of the task completion events at the 
+        // reducer, the only option to sync with the new jobtracker is to reset 
+        // the events index
+        if (update.shouldReset()) {
+          fromEventId.set(0);
+          obsoleteMapIds.clear(); // clear the obsolete map
+          mapLocations.clear(); // clear the map locations mapping
+        }
+        
+        // Update the last seen event ID
+        fromEventId.set(fromEventId.get() + events.length);
+        
+        // Process the TaskCompletionEvents:
+        // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.
+        // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop 
+        //    fetching from those maps.
+        // 3. Remove TIPFAILED maps from neededOutputs since we don't need their
+        //    outputs at all.
+        for (TaskCompletionEvent event : events) {
+          switch (event.getTaskStatus()) {
+            case SUCCEEDED:
+            {
+              URI u = URI.create(event.getTaskTrackerHttp());
+              String host = u.getHost();
+              TaskAttemptID taskId = event.getTaskAttemptId();
+              URL mapOutputLocation = new URL(event.getTaskTrackerHttp() + 
+                                      "/mapOutput?job=" + taskId.getJobID() +
+                                      "&map=" + taskId + 
+                                      "&reduce=" + getPartition());
+              List<MapOutputLocation> loc = mapLocations.get(host);
+              if (loc == null) {
+                loc = Collections.synchronizedList
+                  (new LinkedList<MapOutputLocation>());
+                mapLocations.put(host, loc);
+               }
+              loc.add(new MapOutputLocation(taskId, host, mapOutputLocation));
+              numNewMaps ++;
+            }
+            break;
+            case FAILED:
+            case KILLED:
+            case OBSOLETE:
+            {
+              obsoleteMapIds.add(event.getTaskAttemptId());
+              LOG.info("Ignoring obsolete output of " + event.getTaskStatus() + 
+                       " map-task: '" + event.getTaskAttemptId() + "'");
+            }
+            break;
+            case TIPFAILED:
+            {
+              copiedMapOutputs.add(event.getTaskAttemptId().getTaskID());
+              LOG.info("Ignoring output of failed map TIP: '" +  
+                   event.getTaskAttemptId() + "'");
+            }
+            break;
+          }
+        }
+        return numNewMaps;
+      }
+    }
+  }
+
+  /**
+   * Return the exponent of the power of two closest to the given
+   * positive value, or zero if value leq 0.
+   * This follows the observation that the msb of a given value is
+   * also the closest power of two, unless the bit following it is
+   * set.
+   */
+  private static int getClosestPowerOf2(int value) {
+    if (value <= 0)
+      throw new IllegalArgumentException("Undefined for " + value);
+    final int hob = Integer.highestOneBit(value);
+    return Integer.numberOfTrailingZeros(hob) +
+      (((hob >>> 1) & value) == 0 ? 0 : 1);
+  }
+}
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
--- ./src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java	2013-09-15 07:43:01.405933000 +0200
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import java.io.IOException;
+import org.apache.hadoop.mapred.Task.TaskReporter;
+import org.apache.hadoop.fs.FileSystem;
+
+/**
+ * ShuffleConsumerPlugin for serving Reducers.  It may shuffle MOF files from
+ * either the built-in provider (MapOutputServlet) or from a 3rd party ShuffleProviderPlugin.
+ *
+ */
+@InterfaceAudience.LimitedPrivate("MapReduce")
+@InterfaceStability.Unstable
+public interface ShuffleConsumerPlugin {
+
+  /**
+   * initialize this instance after it was created by factory.
+   */
+  public void init(Context context) throws ClassNotFoundException, IOException;
+
+  /**
+   * fetch output of mappers from TaskTrackers
+   * @return true iff success.  In case of failure an appropriate Throwable may be available thru getMergeThrowable() member
+   */
+  public boolean fetchOutputs() throws IOException;
+
+  /**
+   * @ret reference to a Throwable object (if merge throws an exception)
+   */
+  public Throwable getMergeThrowable();
+
+  /**
+   * Create a RawKeyValueIterator from copied map outputs.
+   *
+   * The iterator returned must satisfy the following constraints:
+   *   1. Fewer than io.sort.factor files may be sources
+   *   2. No more than maxInMemReduce bytes of map outputs may be resident
+   *      in memory when the reduce begins
+   *
+   * If we must perform an intermediate merge to satisfy (1), then we can
+   * keep the excluded outputs from (2) in memory and include them in the
+   * first merge pass. If not, then said outputs must be written to disk
+   * first.
+   */
+  public RawKeyValueIterator createKVIterator(JobConf job, FileSystem fs, Reporter reporter) throws IOException;
+
+  /**
+   * close and clean any resource associated with this object.
+   */
+  public void close();
+
+  @InterfaceAudience.LimitedPrivate("MapReduce")
+  @InterfaceStability.Unstable
+  public static class Context {
+    private final ReduceTask reduceTask;
+    private final TaskUmbilicalProtocol umbilical;
+    private final JobConf conf;
+    private final TaskReporter reporter;
+
+    public Context(ReduceTask reduceTask, TaskUmbilicalProtocol umbilical, JobConf conf, TaskReporter reporter){
+      this.reduceTask = reduceTask;
+      this.umbilical = umbilical;
+      this.conf = conf;
+      this.reporter = reporter;
+    }
+
+    public ReduceTask getReduceTask() {
+      return reduceTask;
+    }
+    public JobConf getConf() {
+      return conf;
+    }
+    public TaskUmbilicalProtocol getUmbilical() {
+      return umbilical;
+    }
+    public TaskReporter getReporter() {
+      return reporter;
+    }
+  }
+}
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java
--- ./src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java	2013-09-15 07:43:01.407925000 +0200
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * This interface is implemented by objects that are able to answer shuffle requests which are
+ * sent from a matching Shuffle Consumer that lives in context of a ReduceTask object.
+ *
+ * ShuffleProviderPlugin object will be notified on the following events:
+ * initialize, destroy.
+ *
+ * NOTE: This interface is also used when loading 3rd party plugins at runtime
+ *
+ */
+@InterfaceAudience.LimitedPrivate("MapReduce")
+@InterfaceStability.Unstable
+public interface ShuffleProviderPlugin {
+  /**
+   * Do constructor work here.
+   * This method is invoked by the TaskTracker Constructor
+   */
+  public void initialize(TaskTracker taskTracker);
+
+  /**
+   * close and cleanup any resource, including threads and disk space.
+   * This method is invoked by TaskTracker.shutdown
+   */
+  public void destroy();
+}
diff -rupN ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
--- ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java	2012-11-02 02:56:46.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/TaskTracker.java	2013-09-15 07:43:01.415925000 +0200
@@ -163,6 +163,9 @@ public class TaskTracker implements MRCo
   private String shuffleScheme;
   private int shufflePort;
 
+  public static final String SHUFFLE_PROVIDER_PLUGIN_CLASSES = "mapreduce.shuffle.provider.plugin.classes";
+  final private ShuffleProviderPlugin shuffleProviderPlugin = new MultiShuffleProviderPlugin();
+
   static enum State {NORMAL, STALE, INTERRUPTED, DENIED}
 
   static final FsPermission LOCAL_DIR_PERMISSION =
@@ -282,6 +285,52 @@ public class TaskTracker implements MRCo
     }
   }
 
+  public static class DefaultShuffleProvider implements ShuffleProviderPlugin {
+    public void initialize(TaskTracker tt) {
+      tt.server.addInternalServlet("mapOutput", "/mapOutput", MapOutputServlet.class);
+    }
+
+    public void destroy() {
+    }
+  }
+
+  private static class MultiShuffleProviderPlugin implements ShuffleProviderPlugin {
+
+    private ShuffleProviderPlugin[] plugins;
+
+    public void initialize(TaskTracker tt) {
+      Configuration conf = tt.getJobConf();
+      Class<?>[] klasses = conf.getClasses(SHUFFLE_PROVIDER_PLUGIN_CLASSES, DefaultShuffleProvider.class);
+
+      plugins = new ShuffleProviderPlugin[klasses.length];
+      for (int i = 0; i < klasses.length; i++) {
+        try{
+          LOG.info(" Loading ShuffleProviderPlugin: " + klasses[i]);
+          plugins[i] =  (ShuffleProviderPlugin)ReflectionUtils.newInstance(klasses[i], conf);
+          plugins[i].initialize(tt);
+        }
+        catch(Throwable t) {
+          LOG.warn("Exception instantiating/initializing a ShuffleProviderPlugin: " + klasses[i], t);
+          plugins[i] =  null;
+        }
+      }
+    }
+
+    public void destroy() {
+      if (plugins != null) {
+          for (ShuffleProviderPlugin plugin : plugins) {
+            try {
+              if (plugin != null) {
+                plugin.destroy();
+              }
+            } catch (Throwable t) {
+              LOG.warn("Exception destroying a ShuffleProviderPlugin: " + plugin, t);
+            }
+          }
+        }
+      }
+    }
+
   private LocalStorage localStorage;
   private long lastCheckDirsTime;
   private int lastNumFailures;
@@ -735,7 +784,7 @@ public class TaskTracker implements MRCo
     + TaskTracker.LOCAL_SPLIT_FILE;
   }
 
-  static String getIntermediateOutputDir(String user, String jobid,
+  public static String getIntermediateOutputDir(String user, String jobid,
       String taskid) {
     return getLocalTaskDir(user, jobid, taskid) + Path.SEPARATOR
     + TaskTracker.OUTPUT;
@@ -1512,6 +1561,14 @@ public class TaskTracker implements MRCo
   public synchronized void shutdown() throws IOException, InterruptedException {
     shuttingDown = true;
     close();
+    if (this.shuffleProviderPlugin != null) {
+      try {
+        LOG.info("Shutting down shuffleProviderPlugin");
+        this.shuffleProviderPlugin.destroy();
+      } catch (Exception e) {
+        LOG.warn("Exception shutting down shuffleProviderPlugin", e);
+      }
+    }
     if (this.server != null) {
       try {
         LOG.info("Shutting down StatusHttpServer");
@@ -1723,7 +1780,7 @@ public class TaskTracker implements MRCo
     String exceptionMsgRegex = conf.get("mapreduce.reduce.shuffle.catch.exception.message.regex");
     server.setAttribute("exceptionStackRegex", exceptionStackRegex);
     server.setAttribute("exceptionMsgRegex", exceptionMsgRegex);
-    server.addInternalServlet("mapOutput", "/mapOutput", MapOutputServlet.class);
+    shuffleProviderPlugin.initialize(this);
     server.addServlet("taskLog", "/tasklog", TaskLogServlet.class);
 
     boolean shuffleSsl = conf.getBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
@@ -4000,9 +4057,22 @@ public class TaskTracker implements MRCo
   }
   
   /**
+   * Get the specific job conf for a running job.
+   */
+  public JobConf getJobConf(JobID jobId) throws IOException {
+    synchronized (runningJobs) {
+      RunningJob rjob = runningJobs.get(jobId);
+      if (rjob == null) {
+        throw new IOException("Unknown job " + jobId + "!!");
+      }
+      return rjob.getJobConf();
+    }
+  }
+
+  /**
    * Get the default job conf for this tracker.
    */
-  JobConf getJobConf() {
+  public JobConf getJobConf() {
     return fConf;
   }
     
@@ -4148,16 +4218,10 @@ public class TaskTracker implements MRCo
         FileSystem rfs = ((LocalFileSystem)
             context.getAttribute("local.file.system")).getRaw();
 
-      String userName = null;
-      String runAsUserName = null;
-      synchronized (tracker.runningJobs) {
-        RunningJob rjob = tracker.runningJobs.get(JobID.forName(jobId));
-        if (rjob == null) {
-          throw new IOException("Unknown job " + jobId + "!!");
-        }
-        userName = rjob.jobConf.getUser();
-        runAsUserName = tracker.getTaskController().getRunAsUser(rjob.jobConf);
-      }
+      JobConf jobConf = tracker.getJobConf(JobID.forName(jobId));
+      String userName = jobConf.getUser();
+      String runAsUserName = tracker.getTaskController().getRunAsUser(jobConf);
+
       // Index file
       String intermediateOutputDir = TaskTracker.getIntermediateOutputDir(userName, jobId, mapId);
       String indexKey = intermediateOutputDir + "/file.out.index";
diff -rupN ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java.orig ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/TaskTracker.java.orig
--- ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java.orig	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapred/TaskTracker.java.orig	2012-11-02 02:56:46.000000000 +0200
@@ -0,0 +1,4745 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.RandomAccessFile;
+import java.net.InetSocketAddress;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.security.GeneralSecurityException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.ListIterator;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.Vector;
+import java.util.Map.Entry;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.regex.Pattern;
+
+import javax.crypto.SecretKey;
+import javax.net.ssl.SSLServerSocketFactory;
+import javax.servlet.ServletContext;
+import javax.servlet.ServletException;
+import javax.servlet.http.HttpServlet;
+import javax.servlet.http.HttpServletRequest;
+import javax.servlet.http.HttpServletResponse;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.TaskDistributedCacheManager;
+import org.apache.hadoop.filecache.TrackerDistributedCacheManager;
+import org.apache.hadoop.mapreduce.server.tasktracker.*;
+import org.apache.hadoop.mapreduce.server.tasktracker.userlogs.*;
+import org.apache.hadoop.fs.DF;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.LocalDirAllocator;
+import org.apache.hadoop.fs.LocalFileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.http.HttpServer;
+import org.apache.hadoop.http.HttpConfig;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.nativeio.NativeIO;
+import org.apache.hadoop.io.ReadaheadPool;
+import org.apache.hadoop.io.ReadaheadPool.ReadaheadRequest;
+import org.apache.hadoop.io.SecureIOUtils;
+import org.apache.hadoop.ipc.ProtocolSignature;
+import org.apache.hadoop.ipc.RPC;
+import org.apache.hadoop.ipc.RemoteException;
+import org.apache.hadoop.ipc.Server;
+import org.apache.hadoop.mapred.CleanupQueue.PathDeletionContext;
+import org.apache.hadoop.mapred.TaskLog.LogFileDetail;
+import org.apache.hadoop.mapred.TaskLog.LogName;
+import org.apache.hadoop.mapred.TaskStatus.Phase;
+import org.apache.hadoop.mapred.TaskTrackerStatus.TaskTrackerHealthStatus;
+import org.apache.hadoop.mapred.pipes.Submitter;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
+import org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier;
+import org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager;
+import org.apache.hadoop.metrics.MetricsContext;
+import org.apache.hadoop.metrics.MetricsException;
+import org.apache.hadoop.metrics.MetricsRecord;
+import org.apache.hadoop.metrics.MetricsUtil;
+import org.apache.hadoop.metrics.Updater;
+import org.apache.hadoop.metrics.util.MBeanUtil;
+import org.apache.hadoop.net.DNS;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.authorize.PolicyProvider;
+import org.apache.hadoop.security.authorize.AccessControlList;
+import org.apache.hadoop.util.DiskChecker;
+import org.apache.hadoop.util.MemoryCalculatorPlugin;
+import org.apache.hadoop.util.ResourceCalculatorPlugin;
+import org.apache.hadoop.util.ProcfsBasedProcessTree;
+import org.apache.hadoop.security.ssl.SSLFactory;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.VersionInfo;
+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
+import org.apache.hadoop.util.Shell.ShellCommandExecutor;
+import org.apache.hadoop.util.MRAsyncDiskService;
+import org.apache.hadoop.mapreduce.security.TokenCache;
+import org.apache.hadoop.security.Credentials;
+import org.mortbay.jetty.security.SslSocketConnector;
+
+/*******************************************************
+ * TaskTracker is a process that starts and tracks MR Tasks
+ * in a networked environment.  It contacts the JobTracker
+ * for Task assignments and reporting results.
+ *
+ *******************************************************/
+public class TaskTracker implements MRConstants, TaskUmbilicalProtocol,
+    Runnable, TaskTrackerMXBean {
+  /**
+   * @deprecated
+   */
+  @Deprecated
+  static final String MAPRED_TASKTRACKER_VMEM_RESERVED_PROPERTY =
+    "mapred.tasktracker.vmem.reserved";
+  /**
+   * @deprecated  TODO(todd) this and above are removed in YDist
+   */
+  @Deprecated
+  static final String MAPRED_TASKTRACKER_PMEM_RESERVED_PROPERTY =
+     "mapred.tasktracker.pmem.reserved";
+  
+  static final String TT_RESERVED_PHYSICALMEMORY_MB =
+    "mapreduce.tasktracker.reserved.physicalmemory.mb";
+  
+  static final String TT_MEMORY_MANAGER_MONITORING_INTERVAL = 
+    "mapreduce.tasktracker.taskmemorymanager.monitoringinterval";
+
+  static final String CONF_VERSION_KEY = "mapreduce.tasktracker.conf.version";
+  static final String CONF_VERSION_DEFAULT = "default";
+
+  /** See src/core/core-default.xml */
+  static final String HADOOP_SKIP_VERSION_CHECK_KEY =
+      "hadoop.skip.worker.version.check";
+
+  static final long WAIT_FOR_DONE = 3 * 1000;
+  private int httpPort;
+
+  private SSLFactory sslFactory;
+  private String shuffleScheme;
+  private int shufflePort;
+
+  static enum State {NORMAL, STALE, INTERRUPTED, DENIED}
+
+  static final FsPermission LOCAL_DIR_PERMISSION =
+    FsPermission.createImmutable((short) 0755);
+
+  static{
+    Configuration.addDefaultResource("mapred-default.xml");
+    Configuration.addDefaultResource("mapred-site.xml");
+  }
+
+  public static final Log LOG =
+    LogFactory.getLog(TaskTracker.class);
+
+  public static final String MR_CLIENTTRACE_FORMAT =
+        "src: %s" +     // src IP
+        ", dest: %s" +  // dst IP
+        ", bytes: %s" + // byte count
+        ", op: %s" +    // operation
+        ", cliID: %s" +  // task id
+        ", duration: %s"; // duration
+  public static final Log ClientTraceLog =
+    LogFactory.getLog(TaskTracker.class.getName() + ".clienttrace");
+
+  //Job ACLs file is created by TaskController under userlogs/$jobid directory
+  //for each job at job localization time. This will be used by TaskLogServlet 
+  //for authorizing viewing of task logs of that job
+  static String jobACLsFile = "job-acls.xml";
+
+  volatile boolean running = true;
+
+  /**
+   * Manages TT local storage directories.
+   */
+  static class LocalStorage {
+    private List<String> localDirs;
+    private int numFailures;
+
+    /**
+     * TaskTracker internal only
+     */
+    public LocalStorage(String[] dirs) {
+      localDirs = new ArrayList<String>();
+      localDirs.addAll(Arrays.asList(dirs));
+    }
+
+    /**
+     * @return the number of valid local directories
+     */
+    synchronized int numDirs() {
+      return localDirs.size();
+    }
+
+    /**
+     * @return the current valid directories 
+     */
+    synchronized String[] getDirs() {
+      return localDirs.toArray(new String[localDirs.size()]);
+    }
+
+    /**
+     * @return the current valid directories
+     */
+    synchronized String getDirsString() {
+      return StringUtils.join(",", localDirs);
+    }
+
+    /**
+     * @return the number of directory failures
+     */
+     synchronized int numFailures() {
+       return numFailures;
+     }
+
+    /**
+     * Check the current set of local directories, updating the list
+     * of valid directories if necessary.
+     * @param checkAndFixPermissions should check the permissions of the
+     *        directory and try to fix them if incorrect. This is
+     *        expensive so should only be done at startup.
+     * @throws DiskErrorException if no directories are writable
+     */
+    synchronized void checkDirs(LocalFileSystem fs,
+                                boolean checkAndFixPermissions)
+        throws DiskErrorException {
+      ListIterator<String> it = localDirs.listIterator();
+      while (it.hasNext()) {
+        final String path = it.next();
+        try {
+          File dir = new File(path);
+          if (checkAndFixPermissions) {
+            DiskChecker.checkDir(fs, new Path(path), LOCAL_DIR_PERMISSION);
+            // This version of DiskChecker#checkDir - unlike the one
+            // below - doesn't use File to check if an actual read or
+            // write will fail (it just checks the permissions value)
+            // so we need to check that here.
+            if (!dir.canRead()) {
+              throw new DiskErrorException("Dir is not readable: " + path);
+            }
+            if (!dir.canWrite()) {
+              throw new DiskErrorException("Dir is not writable: " + path);
+            }
+          } else {
+            DiskChecker.checkDir(dir);
+          }
+        } catch (IOException ioe) {
+          LOG.warn("TaskTracker local dir " + path + " error " + 
+              ioe.getMessage() + ", removing from local dirs");
+          it.remove();
+          numFailures++;
+        }
+      }
+
+      if (localDirs.isEmpty()) {
+        throw new DiskErrorException(
+            "No mapred local directories are writable");
+      }
+    }
+  }
+
+  private LocalStorage localStorage;
+  private long lastCheckDirsTime;
+  private int lastNumFailures;
+  private LocalDirAllocator localDirAllocator;
+  String taskTrackerName;
+  String localHostname;
+  InetSocketAddress jobTrackAddr;
+    
+  InetSocketAddress taskReportAddress;
+
+  Server taskReportServer = null;
+  InterTrackerProtocol jobClient;
+  
+  private TrackerDistributedCacheManager distributedCacheManager;
+  static int FILE_CACHE_SIZE = 2000;
+    
+  // last heartbeat response recieved
+  short heartbeatResponseId = -1;
+  
+  static final String TASK_CLEANUP_SUFFIX = ".cleanup";
+
+  /*
+   * This is the last 'status' report sent by this tracker to the JobTracker.
+   * 
+   * If the rpc call succeeds, this 'status' is cleared-out by this tracker;
+   * indicating that a 'fresh' status report be generated; in the event the
+   * rpc calls fails for whatever reason, the previous status report is sent
+   * again.
+   */
+  TaskTrackerStatus status = null;
+  
+  // The system-directory on HDFS where job files are stored 
+  Path systemDirectory = null;
+  
+  // The filesystem where job files are stored
+  FileSystem systemFS = null;
+  private LocalFileSystem localFs = null;
+  private final TTHttpServer server;
+    
+  volatile boolean shuttingDown = false;
+    
+  Map<TaskAttemptID, TaskInProgress> tasks = new HashMap<TaskAttemptID, TaskInProgress>();
+  /**
+   * Map from taskId -> TaskInProgress.
+   */
+  Map<TaskAttemptID, TaskInProgress> runningTasks = null;
+  Map<JobID, RunningJob> runningJobs = new TreeMap<JobID, RunningJob>();
+  private final JobTokenSecretManager jobTokenSecretManager
+    = new JobTokenSecretManager();
+
+  JobTokenSecretManager getJobTokenSecretManager() {
+    return jobTokenSecretManager;
+  }
+
+  RunningJob getRunningJob(JobID jobId) {
+    return runningJobs.get(jobId);
+  }
+
+  volatile int mapTotal = 0;
+  volatile int reduceTotal = 0;
+  boolean justStarted = true;
+  boolean justInited = true;
+  // Mark reduce tasks that are shuffling to rollback their events index
+  Set<TaskAttemptID> shouldReset = new HashSet<TaskAttemptID>();
+    
+  //dir -> DF
+  Map<String, DF> localDirsDf = new HashMap<String, DF>();
+  long minSpaceStart = 0;
+  //must have this much space free to start new tasks
+  boolean acceptNewTasks = true;
+  long minSpaceKill = 0;
+  //if we run under this limit, kill one task
+  //and make sure we never receive any new jobs
+  //until all the old tasks have been cleaned up.
+  //this is if a machine is so full it's only good
+  //for serving map output to the other nodes
+
+  static Random r = new Random();
+  public static final String SUBDIR = "taskTracker";
+  static final String DISTCACHEDIR = "distcache";
+  static final String JOBCACHE = "jobcache";
+  static final String OUTPUT = "output";
+  static final String JARSDIR = "jars";
+  static final String LOCAL_SPLIT_FILE = "split.info";
+  static final String JOBFILE = "job.xml";
+  static final String TT_PRIVATE_DIR = "ttprivate";
+  public static final String TT_LOG_TMP_DIR = "tt_log_tmp";
+  static final String JVM_EXTRA_ENV_FILE = "jvm.extra.env";
+
+  static final String JOB_LOCAL_DIR = "job.local.dir";
+  static final String JOB_TOKEN_FILE="jobToken"; //localized file
+  static final String[] dirsToCleanup = new String[] { SUBDIR,
+      TT_PRIVATE_DIR, TT_LOG_TMP_DIR };
+
+  private JobConf fConf;
+  private JobConf originalConf;
+  private Localizer localizer;
+  private int maxMapSlots;
+  private int maxReduceSlots;
+  private int taskFailures;
+  final long mapRetainSize;
+  final long reduceRetainSize;
+
+  private ACLsManager aclsManager;
+  
+  // Performance-related config knob to send an out-of-band heartbeat
+  // on task completion
+  static final String TT_OUTOFBAND_HEARBEAT =
+    "mapreduce.tasktracker.outofband.heartbeat";
+  private volatile boolean oobHeartbeatOnTaskCompletion;
+  private boolean manageOsCacheInShuffle = false;
+  private int readaheadLength;
+  private ReadaheadPool readaheadPool = ReadaheadPool.getInstance();
+
+  // Track number of completed tasks to send an out-of-band heartbeat
+  private IntWritable finishedCount = new IntWritable(0);
+  
+  private MapEventsFetcherThread mapEventsFetcher;
+  final int workerThreads;
+  CleanupQueue directoryCleanupThread;
+  private volatile JvmManager jvmManager;
+  
+  private TaskMemoryManagerThread taskMemoryManager;
+  private boolean taskMemoryManagerEnabled = true;
+  private long totalVirtualMemoryOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+  private long totalPhysicalMemoryOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+  private long mapSlotMemorySizeOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+  private long reduceSlotSizeMemoryOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+  private long totalMemoryAllottedForTasks = JobConf.DISABLED_MEMORY_LIMIT;
+  private long reservedPhysicalMemoryOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+  private ResourceCalculatorPlugin resourceCalculatorPlugin = null;
+
+  private UserLogManager userLogManager;
+
+  static final String MAPRED_TASKTRACKER_MEMORY_CALCULATOR_PLUGIN_PROPERTY =
+      "mapred.tasktracker.memory_calculator_plugin";
+  public static final String TT_RESOURCE_CALCULATOR_PLUGIN = 
+      "mapreduce.tasktracker.resourcecalculatorplugin";
+
+  private MRAsyncDiskService asyncDiskService;
+  
+  /**
+   * the minimum interval between jobtracker polls
+   */
+  private volatile int heartbeatInterval = MRConstants.HEARTBEAT_INTERVAL_MIN_DEFAULT;
+  /**
+   * Number of maptask completion events locations to poll for at one time
+   */  
+  private int probe_sample_size = 500;
+
+  private IndexCache indexCache;
+
+  /**
+  * Handle to the specific instance of the {@link TaskController} class
+  */
+  private TaskController taskController;
+  
+  /**
+   * Handle to the specific instance of the {@link NodeHealthCheckerService}
+   */
+  private NodeHealthCheckerService healthChecker;
+
+  /**
+   * Thread which checks CPU usage of Jetty and shuts down the TT if it
+   * exceeds a configurable threshold.
+   */
+  private JettyBugMonitor jettyBugMonitor;
+
+  
+  /**
+   * Configuration property for disk health check interval in milli seconds.
+   * Currently, configuring this to a value smaller than the heartbeat interval
+   * is equivalent to setting this to heartbeat interval value.
+   */
+  static final String DISK_HEALTH_CHECK_INTERVAL_PROPERTY =
+      "mapred.disk.healthChecker.interval";
+  /**
+   * How often TaskTracker needs to check the health of its disks.
+   * Default value is {@link MRConstants#DEFAULT_DISK_HEALTH_CHECK_INTERVAL}
+   */
+  private long diskHealthCheckInterval;
+
+  /**
+   * Whether the TT performs a full or relaxed version check with the JT.
+   */
+  private boolean relaxedVersionCheck;
+
+  /**
+   * Whether the TT completely skips version check with the JT.
+   */
+  private boolean skipVersionCheck;
+
+  /*
+   * A list of commitTaskActions for whom commit response has been received 
+   */
+  private List<TaskAttemptID> commitResponses = 
+            Collections.synchronizedList(new ArrayList<TaskAttemptID>());
+
+  private ShuffleServerMetrics shuffleServerMetrics;
+  /** This class contains the methods that should be used for metrics-reporting
+   * the specific metrics for shuffle. The TaskTracker is actually a server for
+   * the shuffle and hence the name ShuffleServerMetrics.
+   */
+  class ShuffleServerMetrics implements Updater {
+    private MetricsRecord shuffleMetricsRecord = null;
+    private int serverHandlerBusy = 0;
+    private long outputBytes = 0;
+    private int failedOutputs = 0;
+    private int successOutputs = 0;
+    private int exceptionsCaught = 0;
+    ShuffleServerMetrics(JobConf conf) {
+      MetricsContext context = MetricsUtil.getContext("mapred");
+      shuffleMetricsRecord = 
+                           MetricsUtil.createRecord(context, "shuffleOutput");
+      this.shuffleMetricsRecord.setTag("sessionId", conf.getSessionId());
+      context.registerUpdater(this);
+    }
+    synchronized void serverHandlerBusy() {
+      ++serverHandlerBusy;
+    }
+    synchronized void serverHandlerFree() {
+      --serverHandlerBusy;
+    }
+    synchronized void outputBytes(long bytes) {
+      outputBytes += bytes;
+    }
+    synchronized void failedOutput() {
+      ++failedOutputs;
+    }
+    synchronized void successOutput() {
+      ++successOutputs;
+    }
+    synchronized void exceptionsCaught() {
+      ++exceptionsCaught;
+    }
+    public void doUpdates(MetricsContext unused) {
+      synchronized (this) {
+        if (workerThreads != 0) {
+          shuffleMetricsRecord.setMetric("shuffle_handler_busy_percent", 
+              100*((float)serverHandlerBusy/workerThreads));
+        } else {
+          shuffleMetricsRecord.setMetric("shuffle_handler_busy_percent", 0);
+        }
+        shuffleMetricsRecord.incrMetric("shuffle_output_bytes", 
+                                        outputBytes);
+        shuffleMetricsRecord.incrMetric("shuffle_failed_outputs", 
+                                        failedOutputs);
+        shuffleMetricsRecord.incrMetric("shuffle_success_outputs", 
+                                        successOutputs);
+        shuffleMetricsRecord.incrMetric("shuffle_exceptions_caught",
+            exceptionsCaught);
+        outputBytes = 0;
+        failedOutputs = 0;
+        successOutputs = 0;
+        exceptionsCaught = 0;
+      }
+      shuffleMetricsRecord.update();
+    }
+  }
+
+  
+  
+    
+  private TaskTrackerInstrumentation myInstrumentation = null;
+
+  public TaskTrackerInstrumentation getTaskTrackerInstrumentation() {
+    return myInstrumentation;
+  }
+  
+  /**
+   * A list of tips that should be cleaned up.
+   */
+  private BlockingQueue<TaskTrackerAction> tasksToCleanup = 
+    new LinkedBlockingQueue<TaskTrackerAction>();
+    
+  /**
+   * A daemon-thread that pulls tips off the list of things to cleanup.
+   */
+  private Thread taskCleanupThread = 
+    new Thread(new Runnable() {
+        public void run() {
+          while (true) {
+            try {
+              TaskTrackerAction action = tasksToCleanup.take();
+              checkJobStatusAndWait(action);
+              if (action instanceof KillJobAction) {
+                purgeJob((KillJobAction) action);
+              } else if (action instanceof KillTaskAction) {
+                processKillTaskAction((KillTaskAction) action);
+              } else {
+                LOG.error("Non-delete action given to cleanup thread: "
+                          + action);
+              }
+            } catch (Throwable except) {
+              LOG.warn(StringUtils.stringifyException(except));
+            }
+          }
+        }
+      }, "taskCleanup");
+
+  void processKillTaskAction(KillTaskAction killAction) throws IOException {
+    TaskInProgress tip;
+    synchronized (TaskTracker.this) {
+      tip = tasks.get(killAction.getTaskID());
+    }
+    LOG.info("Received KillTaskAction for task: " + killAction.getTaskID());
+    purgeTask(tip, false);
+  }
+  
+  private void checkJobStatusAndWait(TaskTrackerAction action) 
+  throws InterruptedException {
+    JobID jobId = null;
+    if (action instanceof KillJobAction) {
+      jobId = ((KillJobAction)action).getJobID();
+    } else if (action instanceof KillTaskAction) {
+      jobId = ((KillTaskAction)action).getTaskID().getJobID();
+    } else {
+      return;
+    }
+    RunningJob rjob = null;
+    synchronized (runningJobs) {
+      rjob = runningJobs.get(jobId);
+    }
+    if (rjob != null) {
+      synchronized (rjob) {
+        while (rjob.localizing) {
+          rjob.wait();
+        }
+      }
+    }
+  }
+
+  public TaskController getTaskController() {
+    return taskController;
+  }
+  
+  // Currently this is used only by tests
+  void setTaskController(TaskController t) {
+    taskController = t;
+  }
+  
+  private RunningJob addTaskToJob(JobID jobId, 
+                                  TaskInProgress tip) {
+    synchronized (runningJobs) {
+      RunningJob rJob = null;
+      if (!runningJobs.containsKey(jobId)) {
+        rJob = new RunningJob(jobId);
+        rJob.tasks = new HashSet<TaskInProgress>();
+        runningJobs.put(jobId, rJob);
+      } else {
+        rJob = runningJobs.get(jobId);
+      }
+      synchronized (rJob) {
+        rJob.tasks.add(tip);
+      }
+      return rJob;
+    }
+  }
+
+  private void removeTaskFromJob(JobID jobId, TaskInProgress tip) {
+    synchronized (runningJobs) {
+      RunningJob rjob = runningJobs.get(jobId);
+      if (rjob == null) {
+        LOG.warn("Unknown job " + jobId + " being deleted.");
+      } else {
+        synchronized (rjob) {
+          // Only remove the TIP if it is identical to the one that is finished
+          // Job recovery means that it is possible to have two task attempts
+          // with the same ID, which is used for TIP equals/hashcode.
+          for (TaskInProgress t : rjob.tasks) {
+            if (tip == t) {
+              rjob.tasks.remove(tip);
+              break;
+            }
+          }
+        }
+      }
+    }
+  }
+
+  UserLogManager getUserLogManager() {
+    return this.userLogManager;
+  }
+
+  void setUserLogManager(UserLogManager u) {
+    this.userLogManager = u;
+  }
+
+  public static String getUserDir(String user) {
+    return TaskTracker.SUBDIR + Path.SEPARATOR + user;
+  } 
+
+  Localizer getLocalizer() {
+    return localizer;
+  }
+
+  void setLocalizer(Localizer l) {
+    localizer = l;
+  }
+
+  public static String getPrivateDistributedCacheDir(String user) {
+    return getUserDir(user) + Path.SEPARATOR + TaskTracker.DISTCACHEDIR;
+  }
+  
+  public static String getPublicDistributedCacheDir() {
+    return TaskTracker.SUBDIR + Path.SEPARATOR + TaskTracker.DISTCACHEDIR;
+  }
+
+  public static String getJobCacheSubdir(String user) {
+    return getUserDir(user) + Path.SEPARATOR + TaskTracker.JOBCACHE;
+  }
+
+  public static String getLocalJobDir(String user, String jobid) {
+    return getJobCacheSubdir(user) + Path.SEPARATOR + jobid;
+  }
+
+  static String getLocalJobConfFile(String user, String jobid) {
+    return getLocalJobDir(user, jobid) + Path.SEPARATOR + TaskTracker.JOBFILE;
+  }
+  
+  static String getPrivateDirJobConfFile(String user, String jobid) {
+    return TT_PRIVATE_DIR + Path.SEPARATOR + getLocalJobConfFile(user, jobid);
+  }
+
+  static String getTaskConfFile(String user, String jobid, String taskid,
+      boolean isCleanupAttempt) {
+    return getLocalTaskDir(user, jobid, taskid, isCleanupAttempt)
+    + Path.SEPARATOR + TaskTracker.JOBFILE;
+  }
+  
+  static String getPrivateDirTaskScriptLocation(String user, String jobid, 
+     String taskid) {
+    return TT_PRIVATE_DIR + Path.SEPARATOR + 
+           getLocalTaskDir(user, jobid, taskid);
+  }
+
+  static String getJobJarsDir(String user, String jobid) {
+    return getLocalJobDir(user, jobid) + Path.SEPARATOR + TaskTracker.JARSDIR;
+  }
+
+  public static String getJobJarFile(String user, String jobid) {
+    return getJobJarsDir(user, jobid) + Path.SEPARATOR + "job.jar";
+  }
+  
+  static String getJobWorkDir(String user, String jobid) {
+    return getLocalJobDir(user, jobid) + Path.SEPARATOR + MRConstants.WORKDIR;
+  }
+
+  static String getLocalSplitFile(String user, String jobid, String taskid) {
+    return TaskTracker.getLocalTaskDir(user, jobid, taskid) + Path.SEPARATOR
+    + TaskTracker.LOCAL_SPLIT_FILE;
+  }
+
+  static String getIntermediateOutputDir(String user, String jobid,
+      String taskid) {
+    return getLocalTaskDir(user, jobid, taskid) + Path.SEPARATOR
+    + TaskTracker.OUTPUT;
+  }
+
+  public static String getLocalTaskDir(String user, String jobid, 
+      String taskid) {
+    return getLocalTaskDir(user, jobid, taskid, false);
+  }
+  
+  public static String getLocalTaskDir(String user, String jobid, String taskid,
+      boolean isCleanupAttempt) {
+    String taskDir = getLocalJobDir(user, jobid) + Path.SEPARATOR + taskid;
+    if (isCleanupAttempt) {
+      taskDir = taskDir + TASK_CLEANUP_SUFFIX;
+    }
+    return taskDir;
+  }
+  
+  static String getTaskWorkDir(String user, String jobid, String taskid,
+      boolean isCleanupAttempt) {
+    String dir = getLocalTaskDir(user, jobid, taskid, isCleanupAttempt);
+    return dir + Path.SEPARATOR + MRConstants.WORKDIR;
+  }
+
+  static String getLocalJobTokenFile(String user, String jobid) {
+    return getLocalJobDir(user, jobid) + Path.SEPARATOR + TaskTracker.JOB_TOKEN_FILE;
+  }
+  
+  static String getPrivateDirJobTokenFile(String user, String jobid) {
+    return TT_PRIVATE_DIR + Path.SEPARATOR + 
+           getLocalJobTokenFile(user, jobid); 
+  }
+  
+  static String getPrivateDirForJob(String user, String jobid) {
+    return TT_PRIVATE_DIR + Path.SEPARATOR + getLocalJobDir(user, jobid) ;
+  }
+
+  private FileSystem getFS(final Path filePath, JobID jobId,
+      final Configuration conf) throws IOException, InterruptedException {
+    RunningJob rJob = runningJobs.get(jobId);
+    FileSystem userFs = 
+      rJob.ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {
+        public FileSystem run() throws IOException {
+          return filePath.getFileSystem(conf);
+      }});
+    return userFs;
+  }
+  
+  String getPid(TaskAttemptID tid) {
+    TaskInProgress tip = tasks.get(tid);
+    if (tip != null) {
+      return jvmManager.getPid(tip.getTaskRunner());
+    }
+    return null;
+  }
+  
+  public long getProtocolVersion(String protocol, 
+                                 long clientVersion) throws IOException {
+    if (protocol.equals(TaskUmbilicalProtocol.class.getName())) {
+      return TaskUmbilicalProtocol.versionID;
+    } else {
+      throw new IOException("Unknown protocol for task tracker: " +
+                            protocol);
+    }
+  }
+
+  @Override // VersionedProtocol
+  public ProtocolSignature getProtocolSignature(String protocol,
+    long clientVersion, int clientMethodsHash) throws IOException {
+    return ProtocolSignature.getProtocolSignature(
+        this, protocol, clientVersion, clientMethodsHash);
+  }
+  
+  /**
+   * Delete all of the user directories.
+   * @param conf the TT configuration
+   * @throws IOException
+   */
+  private void deleteUserDirectories(Configuration conf) throws IOException {
+    for(String root: localStorage.getDirs()) {
+      try {
+        for (FileStatus status : localFs.listStatus(new Path(root, SUBDIR),
+            TaskLog.USERLOGS_PATH_FILTER)) {
+          String owner = status.getOwner();
+          String path = status.getPath().getName();
+          if (path.equals(owner)) {
+            taskController.deleteAsUser(owner, "");
+          }
+        }
+      } catch (FileNotFoundException e) {
+        // ignore
+      }
+    }
+  }
+
+  void initializeDirectories() throws IOException {
+    final String dirs = localStorage.getDirsString();
+    fConf.setStrings(JobConf.MAPRED_LOCAL_DIR_PROPERTY, dirs);
+    LOG.info("Good mapred local directories are: " + dirs);
+    taskController.setConf(fConf);
+    if (server != null) {
+      server.setAttribute("conf", fConf);
+    }
+
+    deleteUserDirectories(fConf);
+
+    asyncDiskService = new MRAsyncDiskService(fConf);
+    asyncDiskService.cleanupDirsInAllVolumes(dirsToCleanup);
+
+    final FsPermission ttdir = FsPermission.createImmutable((short) 0755);
+    for (String s : localStorage.getDirs()) {
+      localFs.mkdirs(new Path(s, SUBDIR), ttdir);
+    }
+    // NB: deleteLocalFiles uses the configured local dirs, but does not
+    // fail if a local directory has failed.
+    fConf.deleteLocalFiles(TT_PRIVATE_DIR);
+    final FsPermission priv = FsPermission.createImmutable((short) 0700);
+    for (String s : localStorage.getDirs()) {
+      localFs.mkdirs(new Path(s, TT_PRIVATE_DIR), priv);
+    }
+    fConf.deleteLocalFiles(TT_LOG_TMP_DIR);
+    final FsPermission pub = FsPermission.createImmutable((short) 0755);
+    for (String s : localStorage.getDirs()) {
+      localFs.mkdirs(new Path(s, TT_LOG_TMP_DIR), pub);
+    }
+    // Create userlogs directory under all good mapred-local-dirs
+    for (String s : localStorage.getDirs()) {
+      Path userLogsDir = new Path(s, TaskLog.USERLOGS_DIR_NAME);
+      if (!localFs.exists(userLogsDir)) {
+        if (!localFs.mkdirs(userLogsDir, pub)) {
+          LOG.warn("Unable to create task log directory: " + userLogsDir);
+        }
+      } else {
+        try {
+          localFs.setPermission(userLogsDir, new FsPermission((short)0755));
+        } catch (IOException ioe) {
+          throw new IOException(
+            "Unable to set permissions on task log directory. " +
+            userLogsDir + " should be owned by " +
+            "and accessible by user '" + System.getProperty("user.name") +
+            "'.", ioe);
+        }
+      }
+    }
+  }
+
+  private void checkSecurityRequirements() throws IOException {
+    if (!UserGroupInformation.isSecurityEnabled()) {
+      return;
+    }
+    if (!NativeIO.isAvailable()) {
+      throw new IOException("Secure IO is necessary to run a secure task tracker.");
+    }
+  }
+
+  public static final String TT_USER_NAME = "mapreduce.tasktracker.kerberos.principal";
+  public static final String TT_KEYTAB_FILE =
+    "mapreduce.tasktracker.keytab.file";  
+  /**
+   * Do the real constructor work here.  It's in a separate method
+   * so we can call it again and "recycle" the object after calling
+   * close().
+   */
+  synchronized void initialize() throws IOException, InterruptedException {
+    this.fConf = new JobConf(originalConf);
+
+    LOG.info("Starting tasktracker with owner as "
+        + getMROwner().getShortUserName());
+
+    if (fConf.get("slave.host.name") != null) {
+      this.localHostname = fConf.get("slave.host.name");
+    }
+    if (localHostname == null) {
+      this.localHostname =
+      DNS.getDefaultHost
+      (fConf.get("mapred.tasktracker.dns.interface","default"),
+       fConf.get("mapred.tasktracker.dns.nameserver","default"));
+    }
+ 
+    // Check local disk, start async disk service, and clean up all 
+    // local directories.
+    initializeDirectories();
+
+    // Check security requirements are met
+    checkSecurityRequirements();
+
+    // Clear out state tables
+    this.tasks.clear();
+    this.runningTasks = new LinkedHashMap<TaskAttemptID, TaskInProgress>();
+    this.runningJobs = new TreeMap<JobID, RunningJob>();
+    this.mapTotal = 0;
+    this.reduceTotal = 0;
+    this.acceptNewTasks = true;
+    this.status = null;
+
+    this.minSpaceStart = this.fConf.getLong("mapred.local.dir.minspacestart", 0L);
+    this.minSpaceKill = this.fConf.getLong("mapred.local.dir.minspacekill", 0L);
+    //tweak the probe sample size (make it a function of numCopiers)
+    probe_sample_size = this.fConf.getInt("mapred.tasktracker.events.batchsize", 500);
+    
+    try {
+      Class<? extends TaskTrackerInstrumentation> metricsInst = getInstrumentationClass(fConf);
+      java.lang.reflect.Constructor<? extends TaskTrackerInstrumentation> c =
+        metricsInst.getConstructor(new Class[] {TaskTracker.class} );
+      this.myInstrumentation = c.newInstance(this);
+    } catch(Exception e) {
+      //Reflection can throw lots of exceptions -- handle them all by 
+      //falling back on the default.
+      LOG.error(
+        "Failed to initialize taskTracker metrics. Falling back to default.",
+        e);
+      this.myInstrumentation = new TaskTrackerMetricsInst(this);
+    }
+    
+    // bind address
+    String address = 
+      NetUtils2.getServerAddress(fConf,
+                                "mapred.task.tracker.report.bindAddress", 
+                                "mapred.task.tracker.report.port", 
+                                "mapred.task.tracker.report.address");
+    InetSocketAddress socAddr = NetUtils.createSocketAddr(address);
+    String bindAddress = socAddr.getHostName();
+    int tmpPort = socAddr.getPort();
+    
+    this.jvmManager = new JvmManager(this);
+    
+    // RPC initialization
+    int max = maxMapSlots > maxReduceSlots ? 
+                       maxMapSlots : maxReduceSlots;
+    //set the num handlers to max*2 since canCommit may wait for the duration
+    //of a heartbeat RPC
+    this.taskReportServer = RPC.getServer(TaskUmbilicalProtocol.class,
+        this, bindAddress,
+        tmpPort, 2 * max, false, this.fConf, this.jobTokenSecretManager);
+
+    // Set service-level authorization security policy
+    if (this.fConf.getBoolean(
+          CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {
+      PolicyProvider policyProvider = 
+        (PolicyProvider)(ReflectionUtils.newInstance(
+            this.fConf.getClass(PolicyProvider.POLICY_PROVIDER_CONFIG, 
+                MapReducePolicyProvider.class, PolicyProvider.class), 
+            this.fConf));
+      this.taskReportServer.refreshServiceAcl(fConf, policyProvider);
+    }
+
+    this.taskReportServer.start();
+
+    // get the assigned address
+    this.taskReportAddress = taskReportServer.getListenerAddress();
+    this.fConf.set("mapred.task.tracker.report.address",
+        taskReportAddress.getHostName() + ":" + taskReportAddress.getPort());
+    LOG.info("TaskTracker up at: " + this.taskReportAddress);
+
+    this.taskTrackerName = "tracker_" + localHostname + ":" + taskReportAddress;
+    LOG.info("Starting tracker " + taskTrackerName);
+
+    // Initialize DistributedCache and
+    // clear out temporary files that might be lying around
+    this.distributedCacheManager = 
+        new TrackerDistributedCacheManager(this.fConf, taskController, asyncDiskService);
+    this.distributedCacheManager.purgeCache(); // TODO(todd) purge here?
+
+    this.jobClient = (InterTrackerProtocol) 
+    UserGroupInformation.getLoginUser().doAs(
+        new PrivilegedExceptionAction<Object>() {
+      public Object run() throws IOException {
+        return RPC.waitForProxy(InterTrackerProtocol.class,
+            InterTrackerProtocol.versionID,
+            jobTrackAddr, fConf);
+      }
+    });
+    this.justInited = true;
+    this.running = true;    
+    // start the thread that will fetch map task completion events
+    this.mapEventsFetcher = new MapEventsFetcherThread();
+    mapEventsFetcher.setDaemon(true);
+    mapEventsFetcher.setName(
+                             "Map-events fetcher for all reduce tasks " + "on " + 
+                             taskTrackerName);
+    mapEventsFetcher.start();
+
+    Class<? extends ResourceCalculatorPlugin> clazz =
+        fConf.getClass(TT_RESOURCE_CALCULATOR_PLUGIN,
+                       null, ResourceCalculatorPlugin.class);
+    resourceCalculatorPlugin = 
+      ResourceCalculatorPlugin.getResourceCalculatorPlugin(clazz, fConf);
+    LOG.info(" Using ResourceCalculatorPlugin : " + resourceCalculatorPlugin);
+    initializeMemoryManagement();
+
+    getUserLogManager().clearOldUserLogs(fConf);
+
+    setIndexCache(new IndexCache(this.fConf));
+
+    mapLauncher = new TaskLauncher(TaskType.MAP, maxMapSlots);
+    reduceLauncher = new TaskLauncher(TaskType.REDUCE, maxReduceSlots);
+    mapLauncher.start();
+    reduceLauncher.start();
+
+    // create a localizer instance
+    setLocalizer(new Localizer(localFs, localStorage.getDirs()));
+
+    //Start up node health checker service.
+    if (shouldStartHealthMonitor(this.fConf)) {
+      startHealthMonitor(this.fConf);
+    }
+    
+    // Start thread to monitor jetty bugs
+    startJettyBugMonitor();
+    
+    oobHeartbeatOnTaskCompletion = 
+      fConf.getBoolean(TT_OUTOFBAND_HEARBEAT, false);
+    
+    manageOsCacheInShuffle = fConf.getBoolean(
+        "mapred.tasktracker.shuffle.fadvise",
+        true);
+    readaheadLength = fConf.getInt(
+        "mapred.tasktracker.shuffle.readahead.bytes",
+        4 * 1024 * 1024);
+  }
+
+  private void startJettyBugMonitor() {
+    jettyBugMonitor = JettyBugMonitor.create(fConf);
+    if (jettyBugMonitor != null) {
+      jettyBugMonitor.start();
+    }
+  }
+
+  UserGroupInformation getMROwner() {
+    return aclsManager.getMROwner();
+  }
+
+  /**
+   * Are ACLs for authorization checks enabled on the TT ?
+   */
+  boolean areACLsEnabled() {
+    return fConf.getBoolean(JobConf.MR_ACLS_ENABLED, false);
+  }
+
+  public static Class<? extends TaskTrackerInstrumentation> getInstrumentationClass(
+    Configuration conf) {
+    return conf.getClass("mapred.tasktracker.instrumentation",
+        TaskTrackerMetricsInst.class, TaskTrackerInstrumentation.class);
+  }
+
+  public static void setInstrumentationClass(
+    Configuration conf, Class<? extends TaskTrackerInstrumentation> t) {
+    conf.setClass("mapred.tasktracker.instrumentation",
+        t, TaskTrackerInstrumentation.class);
+  }
+  
+  /**
+   * Removes all contents of temporary storage. Called upon startup, to remove
+   * any leftovers from previous run.
+   * 
+   * Use MRAsyncDiskService.moveAndDeleteAllVolumes instead.
+   * 
+   * @see org.apache.hadoop.mapreduce.util.MRAsyncDiskService#cleanupDirsInAllVolumes()
+   */
+  @Deprecated
+  public void cleanupStorage() throws IOException {
+    this.fConf.deleteLocalFiles(SUBDIR);
+    this.fConf.deleteLocalFiles(TT_PRIVATE_DIR);
+    this.fConf.deleteLocalFiles(TT_LOG_TMP_DIR);
+  }
+
+  // Object on wait which MapEventsFetcherThread is going to wait.
+  private Object waitingOn = new Object();
+
+  private class MapEventsFetcherThread extends Thread {
+
+    private List <FetchStatus> reducesInShuffle() {
+      List <FetchStatus> fList = new ArrayList<FetchStatus>();
+      for (Map.Entry <JobID, RunningJob> item : runningJobs.entrySet()) {
+        RunningJob rjob = item.getValue();
+        if (!rjob.localized) {
+          continue;
+        }
+        JobID jobId = item.getKey();
+        FetchStatus f;
+        synchronized (rjob) {
+          f = rjob.getFetchStatus();
+          for (TaskInProgress tip : rjob.tasks) {
+            Task task = tip.getTask();
+            if (!task.isMapTask()) {
+              if (((ReduceTask)task).getPhase() == 
+                  TaskStatus.Phase.SHUFFLE) {
+                if (rjob.getFetchStatus() == null) {
+                  //this is a new job; we start fetching its map events
+                  f = new FetchStatus(jobId, 
+                                      ((ReduceTask)task).getNumMaps());
+                  rjob.setFetchStatus(f);
+                }
+                f = rjob.getFetchStatus();
+                fList.add(f);
+                break; //no need to check any more tasks belonging to this
+              }
+            }
+          }
+        }
+      }
+      //at this point, we have information about for which of
+      //the running jobs do we need to query the jobtracker for map 
+      //outputs (actually map events).
+      return fList;
+    }
+      
+    @Override
+    public void run() {
+      LOG.info("Starting thread: " + this.getName());
+        
+      while (running) {
+        try {
+          List <FetchStatus> fList = null;
+          synchronized (runningJobs) {
+            while (((fList = reducesInShuffle()).size()) == 0) {
+              try {
+                runningJobs.wait();
+              } catch (InterruptedException e) {
+                LOG.info("Shutting down: " + this.getName());
+                return;
+              }
+            }
+          }
+          // now fetch all the map task events for all the reduce tasks
+          // possibly belonging to different jobs
+          boolean fetchAgain = false; //flag signifying whether we want to fetch
+                                      //immediately again.
+          for (FetchStatus f : fList) {
+            long currentTime = System.currentTimeMillis();
+            try {
+              //the method below will return true when we have not 
+              //fetched all available events yet
+              if (f.fetchMapCompletionEvents(currentTime)) {
+                fetchAgain = true;
+              }
+            } catch (Exception e) {
+              LOG.warn(
+                       "Ignoring exception that fetch for map completion" +
+                       " events threw for " + f.jobId + " threw: " +
+                       StringUtils.stringifyException(e)); 
+            }
+            if (!running) {
+              break;
+            }
+          }
+          synchronized (waitingOn) {
+            try {
+              if (!fetchAgain) {
+                waitingOn.wait(heartbeatInterval);
+              }
+            } catch (InterruptedException ie) {
+              LOG.info("Shutting down: " + this.getName());
+              return;
+            }
+          }
+        } catch (Exception e) {
+          LOG.info("Ignoring exception "  + e.getMessage());
+        }
+      }
+    } 
+  }
+
+  private class FetchStatus {
+    /** The next event ID that we will start querying the JobTracker from*/
+    private IntWritable fromEventId;
+    /** This is the cache of map events for a given job */ 
+    private List<TaskCompletionEvent> allMapEvents;
+    /** What jobid this fetchstatus object is for*/
+    private JobID jobId;
+    private long lastFetchTime;
+    private boolean fetchAgain;
+     
+    public FetchStatus(JobID jobId, int numMaps) {
+      this.fromEventId = new IntWritable(0);
+      this.jobId = jobId;
+      this.allMapEvents = new ArrayList<TaskCompletionEvent>(numMaps);
+    }
+      
+    /**
+     * Reset the events obtained so far.
+     */
+    public void reset() {
+      // Note that the sync is first on fromEventId and then on allMapEvents
+      synchronized (fromEventId) {
+        synchronized (allMapEvents) {
+          fromEventId.set(0); // set the new index for TCE
+          allMapEvents.clear();
+        }
+      }
+    }
+    
+    public TaskCompletionEvent[] getMapEvents(int fromId, int max) {
+        
+      TaskCompletionEvent[] mapEvents = 
+        TaskCompletionEvent.EMPTY_ARRAY;
+      boolean notifyFetcher = false; 
+      synchronized (allMapEvents) {
+        if (allMapEvents.size() > fromId) {
+          int actualMax = Math.min(max, (allMapEvents.size() - fromId));
+          List <TaskCompletionEvent> eventSublist = 
+            allMapEvents.subList(fromId, actualMax + fromId);
+          mapEvents = eventSublist.toArray(mapEvents);
+        } else {
+          // Notify Fetcher thread. 
+          notifyFetcher = true;
+        }
+      }
+      if (notifyFetcher) {
+        synchronized (waitingOn) {
+          waitingOn.notify();
+        }
+      }
+      return mapEvents;
+    }
+      
+    public boolean fetchMapCompletionEvents(long currTime) throws IOException {
+      if (!fetchAgain && (currTime - lastFetchTime) < heartbeatInterval) {
+        return false;
+      }
+      int currFromEventId = 0;
+      synchronized (fromEventId) {
+        currFromEventId = fromEventId.get();
+        List <TaskCompletionEvent> recentMapEvents = 
+          queryJobTracker(fromEventId, jobId, jobClient);
+        synchronized (allMapEvents) {
+          allMapEvents.addAll(recentMapEvents);
+        }
+        lastFetchTime = currTime;
+        if (fromEventId.get() - currFromEventId >= probe_sample_size) {
+          //return true when we have fetched the full payload, indicating
+          //that we should fetch again immediately (there might be more to
+          //fetch
+          fetchAgain = true;
+          return true;
+        }
+      }
+      fetchAgain = false;
+      return false;
+    }
+  }
+
+  private static LocalDirAllocator lDirAlloc = 
+                              new LocalDirAllocator("mapred.local.dir");
+
+  // intialize the job directory
+  RunningJob localizeJob(TaskInProgress tip) 
+  throws IOException, InterruptedException {
+    Task t = tip.getTask();
+    JobID jobId = t.getJobID();
+    RunningJob rjob = addTaskToJob(jobId, tip);
+    InetSocketAddress ttAddr = getTaskTrackerReportAddress();
+    try {
+      synchronized (rjob) {
+        if (!rjob.localized) {
+          while (rjob.localizing) {
+            rjob.wait();
+          }
+          if (!rjob.localized) {
+            //this thread is localizing the job
+            rjob.localizing = true;
+          }
+        }
+      }
+      if (!rjob.localized) {
+        Path localJobConfPath = initializeJob(t, rjob, ttAddr);
+        JobConf localJobConf = new JobConf(localJobConfPath);
+        //to be doubly sure, overwrite the user in the config with the one the TT 
+        //thinks it is
+        localJobConf.setUser(t.getUser());
+        //also reset the #tasks per jvm
+        resetNumTasksPerJvm(localJobConf);
+        //set the base jobconf path in rjob; all tasks will use
+        //this as the base path when they run
+        synchronized (rjob) {
+          rjob.localizedJobConf = localJobConfPath;
+          rjob.jobConf = localJobConf;  
+          rjob.keepJobFiles = ((localJobConf.getKeepTaskFilesPattern() != null) ||
+              localJobConf.getKeepFailedTaskFiles());
+
+          rjob.localized = true;
+        }
+      } 
+    } finally {
+      synchronized (rjob) {
+        if (rjob.localizing) {
+          rjob.localizing = false;
+          rjob.notifyAll();
+        }
+      }
+    }
+    synchronized (runningJobs) {
+      runningJobs.notify(); //notify the fetcher thread
+    }
+    return rjob;
+  }
+
+  /**
+   * Localize the job on this tasktracker. Specifically
+   * <ul>
+   * <li>Cleanup and create job directories on all disks</li>
+   * <li>Download the credentials file</li>
+   * <li>Download the job config file job.xml from the FS</li>
+   * <li>Invokes the {@link TaskController} to do the rest of the job 
+   * initialization</li>
+   * </ul>
+   *
+   * @param t task whose job has to be localized on this TT
+   * @param rjob the {@link RunningJob}
+   * @param ttAddr the tasktracker's RPC address
+   * @return the path to the job configuration to be used for all the tasks
+   *         of this job as a starting point.
+   * @throws IOException
+   */
+  Path initializeJob(final Task t, final RunningJob rjob, 
+      final InetSocketAddress ttAddr)
+  throws IOException, InterruptedException {
+    final JobID jobId = t.getJobID();
+
+    final Path jobFile = new Path(t.getJobFile());
+    final String userName = t.getUser();
+    final Configuration conf = getJobConf();
+
+    // save local copy of JobToken file
+    final String localJobTokenFile = localizeJobTokenFile(t.getUser(), jobId);
+    synchronized (rjob) {
+      rjob.ugi = UserGroupInformation.createRemoteUser(t.getUser());
+
+      Credentials ts = TokenCache.loadTokens(localJobTokenFile, conf);
+      Token<JobTokenIdentifier> jt = TokenCache.getJobToken(ts);
+      if (jt != null) { //could be null in the case of some unit tests
+        getJobTokenSecretManager().addTokenForJob(jobId.toString(), jt);
+      }
+      for (Token<? extends TokenIdentifier> token : ts.getAllTokens()) {
+        rjob.ugi.addToken(token);
+      }
+    }
+
+    FileSystem userFs = getFS(jobFile, jobId, conf);
+
+    // Download the job.xml for this job from the system FS
+    final Path localJobFile =
+        localizeJobConfFile(new Path(t.getJobFile()), userName, userFs, jobId);
+
+    /**
+      * Now initialize the job via task-controller to do the rest of the
+      * job-init. Do this within a doAs since the public distributed cache 
+      * is also set up here.
+      * To support potential authenticated HDFS accesses, we need the tokens
+      */
+    rjob.ugi.doAs(new PrivilegedExceptionAction<Object>() {
+      public Object run() throws IOException, InterruptedException {
+        try {
+          final JobConf localJobConf = new JobConf(localJobFile);
+          // Setup the public distributed cache
+          TaskDistributedCacheManager taskDistributedCacheManager =
+            getTrackerDistributedCacheManager()
+           .newTaskDistributedCacheManager(jobId, localJobConf);
+          rjob.distCacheMgr = taskDistributedCacheManager;
+          taskDistributedCacheManager.setupCache(localJobConf,
+            TaskTracker.getPublicDistributedCacheDir(),
+            TaskTracker.getPrivateDistributedCacheDir(userName));
+
+          // Set some config values
+          localJobConf.set(JobConf.MAPRED_LOCAL_DIR_PROPERTY,
+              getJobConf().get(JobConf.MAPRED_LOCAL_DIR_PROPERTY));
+          if (conf.get("slave.host.name") != null) {
+            localJobConf.set("slave.host.name", conf.get("slave.host.name"));
+          }
+          resetNumTasksPerJvm(localJobConf);
+          localJobConf.setUser(t.getUser());
+
+          // write back the config (this config will have the updates that the
+          // distributed cache manager makes as well)
+          JobLocalizer.writeLocalJobFile(localJobFile, localJobConf);
+          taskController.initializeJob(t.getUser(), jobId.toString(), 
+              new Path(localJobTokenFile), localJobFile, TaskTracker.this,
+              ttAddr);
+        } catch (IOException e) {
+          LOG.warn("Exception while localization " + 
+              StringUtils.stringifyException(e));
+          throw e;
+        } catch (InterruptedException ie) {
+          LOG.warn("Exception while localization " + 
+              StringUtils.stringifyException(ie));
+          throw ie;
+        }
+        return null;
+      }
+    });
+    //search for the conf that the initializeJob created
+    //need to look up certain configs from this conf, like
+    //the distributed cache, profiling, etc. ones
+    Path initializedConf = lDirAlloc.getLocalPathToRead(getLocalJobConfFile(
+           userName, jobId.toString()), getJobConf());
+    return initializedConf;
+  }
+  
+  /** If certain configs are enabled, the jvm-reuse should be disabled
+   * @param localJobConf
+   */
+  static void resetNumTasksPerJvm(JobConf localJobConf) {
+    boolean debugEnabled = false;
+    if (localJobConf.getNumTasksToExecutePerJvm() == 1) {
+      return;
+    }
+    if (localJobConf.getMapDebugScript() != null || 
+        localJobConf.getReduceDebugScript() != null) {
+      debugEnabled = true;
+    }
+    String keepPattern = localJobConf.getKeepTaskFilesPattern();
+     
+    if (debugEnabled || localJobConf.getProfileEnabled() ||
+        keepPattern != null || localJobConf.getKeepFailedTaskFiles()) {
+      //disable jvm reuse
+      localJobConf.setNumTasksToExecutePerJvm(1);
+    }
+  }
+
+  // Remove the log dir from the tasklog cleanup thread
+  void saveLogDir(JobID jobId, JobConf localJobConf)
+      throws IOException {
+    // remove it from tasklog cleanup thread first,
+    // it might be added there because of tasktracker reinit or restart
+    JobStartedEvent jse = new JobStartedEvent(jobId);
+    getUserLogManager().addLogEvent(jse);
+  }
+
+
+  /**
+   * Download the job configuration file from the FS.
+   *
+   * @param jobFile the original location of the configuration file
+   * @param user the user in question
+   * @param userFs the FileSystem created on behalf of the user
+   * @param jobId jobid in question
+   * @return the local file system path of the downloaded file.
+   * @throws IOException
+   */
+  private Path localizeJobConfFile(Path jobFile, String user, 
+      FileSystem userFs, JobID jobId)
+  throws IOException {
+    // Get sizes of JobFile and JarFile
+    // sizes are -1 if they are not present.
+    FileStatus status = null;
+    long jobFileSize = -1;
+    try {
+      status = userFs.getFileStatus(jobFile);
+      jobFileSize = status.getLen();
+    } catch(FileNotFoundException fe) {
+      jobFileSize = -1;
+    }
+    Path localJobFile =
+      lDirAlloc.getLocalPathForWrite(getPrivateDirJobConfFile(user,
+          jobId.toString()), jobFileSize, fConf);
+
+    // Download job.xml
+    userFs.copyToLocalFile(jobFile, localJobFile);
+    return localJobFile;
+  }
+
+  private void launchTaskForJob(TaskInProgress tip, JobConf jobConf,
+                                RunningJob rjob) throws IOException {
+    synchronized (tip) {
+      jobConf.set(JobConf.MAPRED_LOCAL_DIR_PROPERTY,
+                  localStorage.getDirsString());
+      tip.setJobConf(jobConf);
+      tip.setUGI(rjob.ugi);
+      tip.launchTask(rjob);
+    }
+  }
+    
+  public synchronized void shutdown() throws IOException, InterruptedException {
+    shuttingDown = true;
+    close();
+    if (this.server != null) {
+      try {
+        LOG.info("Shutting down StatusHttpServer");
+        this.server.stop();
+        if (sslFactory != null) {
+          sslFactory.destroy();
+        }
+      } catch (Exception e) {
+        LOG.warn("Exception shutting down TaskTracker", e);
+      }
+    }
+  }
+  /**
+   * Close down the TaskTracker and all its components.  We must also shutdown
+   * any running tasks or threads, and cleanup disk space.  A new TaskTracker
+   * within the same process space might be restarted, so everything must be
+   * clean.
+   * @throws InterruptedException 
+   */
+  public synchronized void close() throws IOException, InterruptedException {
+    //
+    // Kill running tasks.  Do this in a 2nd vector, called 'tasksToClose',
+    // because calling jobHasFinished() may result in an edit to 'tasks'.
+    //
+    TreeMap<TaskAttemptID, TaskInProgress> tasksToClose =
+      new TreeMap<TaskAttemptID, TaskInProgress>();
+    tasksToClose.putAll(tasks);
+    for (TaskInProgress tip : tasksToClose.values()) {
+      tip.jobHasFinished(false);
+    }
+    
+    this.running = false;
+
+    // Clear local storage
+    if (asyncDiskService != null) {
+
+      // Clear local storage
+      try {
+        asyncDiskService.cleanupDirsInAllVolumes(dirsToCleanup);
+      } catch (Exception ioe) {
+        LOG.warn("IOException shutting down TaskTracker", ioe);
+      }
+      
+      // Shutdown all async deletion threads with up to 10 seconds of delay
+      asyncDiskService.shutdown();
+      try {
+        if (!asyncDiskService.awaitTermination(10000)) {
+          asyncDiskService.shutdownNow();
+          asyncDiskService = null;
+        }
+      } catch (InterruptedException e) {
+        asyncDiskService.shutdownNow();
+        asyncDiskService = null;
+      }
+    }
+        
+    // Shutdown the fetcher thread
+    this.mapEventsFetcher.interrupt();
+    
+    //stop the launchers
+    this.mapLauncher.interrupt();
+    this.reduceLauncher.interrupt();
+
+    jvmManager.stop();
+    
+    // shutdown RPC connections
+    RPC.stopProxy(jobClient);
+
+    // wait for the fetcher thread to exit
+    for (boolean done = false; !done; ) {
+      try {
+        this.mapEventsFetcher.join();
+        done = true;
+      } catch (InterruptedException e) {
+      }
+    }
+    
+    if (taskReportServer != null) {
+      taskReportServer.stop();
+      taskReportServer = null;
+    }
+    if (healthChecker != null) {
+      //stop node health checker service
+      healthChecker.stop();
+      healthChecker = null;
+    }
+    if (jettyBugMonitor != null) {
+      jettyBugMonitor.shutdown();
+      jettyBugMonitor = null;
+    }
+  }
+
+  /**
+   * For testing
+   */
+  TaskTracker() {
+    server = null;
+    workerThreads = 0;
+    mapRetainSize = TaskLogsTruncater.DEFAULT_RETAIN_SIZE;
+    reduceRetainSize = TaskLogsTruncater.DEFAULT_RETAIN_SIZE;
+  }
+
+  void setConf(JobConf conf) {
+    fConf = conf;
+  }
+
+  void setLocalStorage(LocalStorage in) {
+    localStorage = in;
+  }
+	  
+  void setLocalDirAllocator(LocalDirAllocator in) {
+    localDirAllocator = in;
+  }
+
+  private static class TTHttpServer extends HttpServer {
+
+    public TTHttpServer(String name, String bindAddress, int port,
+        boolean findPort, Configuration conf, AccessControlList adminsAcl)
+        throws IOException {
+      super(name, bindAddress, port, findPort, conf, adminsAcl, null, null);
+    }
+
+    /**
+     * Configure an ssl listener on the server for shuffle.
+     *
+     * @param addr address to listen on.
+     * @param sslFactory SSLFactory to use.
+     */
+    public void addSslListener(InetSocketAddress addr, final SSLFactory sslFactory)
+      throws IOException {
+      if (webServer.isStarted()) {
+        throw new IOException("Failed to add ssl listener");
+      }
+
+      SslSocketConnector sslListener = new SslSocketConnector() {
+        @Override
+        protected SSLServerSocketFactory createFactory() throws Exception {
+          return sslFactory.createSSLServerSocketFactory();
+        }
+      };
+
+      sslListener.setHost(addr.getHostName());
+      sslListener.setPort(addr.getPort());
+      webServer.addConnector(sslListener);
+    }
+
+  }
+  /**
+   * Start with the local machine name, and the default JobTracker
+   */
+  public TaskTracker(JobConf conf) throws IOException, InterruptedException {
+    originalConf = conf;
+    relaxedVersionCheck = conf.getBoolean(
+        "hadoop.relaxed.worker.version.check", true);
+    skipVersionCheck = conf.getBoolean(
+        HADOOP_SKIP_VERSION_CHECK_KEY, false);
+    FILE_CACHE_SIZE = conf.getInt("mapred.tasktracker.file.cache.size", 2000);
+    maxMapSlots = conf.getInt(
+                  "mapred.tasktracker.map.tasks.maximum", 2);
+    maxReduceSlots = conf.getInt(
+                  "mapred.tasktracker.reduce.tasks.maximum", 2);
+    diskHealthCheckInterval = conf.getLong(DISK_HEALTH_CHECK_INTERVAL_PROPERTY,
+                                           DEFAULT_DISK_HEALTH_CHECK_INTERVAL);
+    aclsManager = new ACLsManager(conf, new JobACLsManager(conf), null);
+    this.jobTrackAddr = JobTracker.getAddress(conf);
+    String infoAddr = 
+      NetUtils2.getServerAddress(conf,
+                                "tasktracker.http.bindAddress", 
+                                "tasktracker.http.port",
+                                "mapred.task.tracker.http.address");
+    InetSocketAddress infoSocAddr = NetUtils.createSocketAddr(infoAddr);
+    String httpBindAddress = infoSocAddr.getHostName();
+    int httpPort = infoSocAddr.getPort();
+    this.server = new TTHttpServer("task", httpBindAddress, httpPort,
+        httpPort == 0, conf, aclsManager.getAdminsAcl());
+    this.shuffleServerMetrics = new ShuffleServerMetrics(conf);
+    workerThreads = conf.getInt("tasktracker.http.threads", 40);
+    server.setThreads(1, workerThreads);
+    // let the jsp pages get to the task tracker, config, and other relevant
+    // objects
+    FileSystem local = FileSystem.getLocal(conf);
+    this.localDirAllocator = new LocalDirAllocator("mapred.local.dir");
+    Class<? extends TaskController> taskControllerClass = 
+      conf.getClass("mapred.task.tracker.task-controller", 
+                     DefaultTaskController.class, TaskController.class);
+
+    fConf = new JobConf(conf);
+    localFs = FileSystem.getLocal(fConf);
+    localStorage = new LocalStorage(fConf.getLocalDirs());
+    localStorage.checkDirs(localFs, true);
+    taskController = 
+      (TaskController)ReflectionUtils.newInstance(taskControllerClass, fConf);
+    taskController.setup(localDirAllocator, localStorage);
+    lastNumFailures = localStorage.numFailures();
+
+    // create user log manager
+    setUserLogManager(new UserLogManager(conf, taskController));
+    SecurityUtil.login(originalConf, TT_KEYTAB_FILE, TT_USER_NAME);
+
+    initialize();
+    server.setAttribute("task.tracker", this);
+    server.setAttribute("local.file.system", local);
+
+    server.setAttribute("log", LOG);
+    server.setAttribute("localDirAllocator", localDirAllocator);
+    server.setAttribute("shuffleServerMetrics", shuffleServerMetrics);
+
+    String exceptionStackRegex = conf.get("mapreduce.reduce.shuffle.catch.exception.stack.regex");
+    String exceptionMsgRegex = conf.get("mapreduce.reduce.shuffle.catch.exception.message.regex");
+    server.setAttribute("exceptionStackRegex", exceptionStackRegex);
+    server.setAttribute("exceptionMsgRegex", exceptionMsgRegex);
+    server.addInternalServlet("mapOutput", "/mapOutput", MapOutputServlet.class);
+    server.addServlet("taskLog", "/tasklog", TaskLogServlet.class);
+
+    boolean shuffleSsl = conf.getBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
+                                         JobTracker.SHUFFLE_SSL_ENABLED_DEFAULT);
+    shuffleScheme = (shuffleSsl) ? "https" : "http";
+    server.setAttribute(JobTracker.SHUFFLE_SSL_ENABLED_KEY, shuffleSsl);
+    if (shuffleSsl) {
+      //if web UI is secure, no need ot use a second SSL listener for shuffle
+      if (!HttpConfig.isSecure()) {
+        sslFactory = new SSLFactory(SSLFactory.Mode.SERVER, conf);
+        try {
+          sslFactory.init();
+        } catch (GeneralSecurityException ex) {
+          throw new RuntimeException(ex);
+        }
+        String sslHostname = conf.get(JobTracker.SHUFFLE_SSL_ADDRESS_KEY,
+                                      JobTracker.SHUFFLE_SSL_ADDRESS_DEFAULT);
+        int sslPort = conf.getInt(
+          JobTracker.SHUFFLE_SSL_PORT_KEY, JobTracker.SHUFFLE_SSL_PORT_DEFAULT);
+        InetSocketAddress sslAddr = new InetSocketAddress(sslHostname, sslPort);
+        server.addSslListener(sslAddr, sslFactory);
+        shufflePort = sslPort;
+      }
+    }
+
+    server.start();
+    this.httpPort = server.getPort();
+    if (sslFactory == null) {
+      shufflePort = this.httpPort;
+    }
+    checkJettyPort(httpPort);
+    LOG.info("FILE_CACHE_SIZE for mapOutputServlet set to : " + FILE_CACHE_SIZE);
+    mapRetainSize = conf.getLong(TaskLogsTruncater.MAP_USERLOG_RETAIN_SIZE, 
+        TaskLogsTruncater.DEFAULT_RETAIN_SIZE);
+    reduceRetainSize = conf.getLong(TaskLogsTruncater.REDUCE_USERLOG_RETAIN_SIZE,
+        TaskLogsTruncater.DEFAULT_RETAIN_SIZE);
+  }
+
+  private void checkJettyPort(int port) throws IOException { 
+    //See HADOOP-4744
+    if (port < 0) {
+      shuttingDown = true;
+      throw new IOException("Jetty problem. Jetty didn't bind to a " +
+      		"valid port");
+    }
+  }
+  
+  private void startCleanupThreads() throws IOException {
+    taskCleanupThread.setDaemon(true);
+    taskCleanupThread.start();
+    directoryCleanupThread = CleanupQueue.getInstance();
+  }
+
+  // only used by tests
+  void setCleanupThread(CleanupQueue c) {
+    directoryCleanupThread = c;
+  }
+  
+  CleanupQueue getCleanupThread() {
+    return directoryCleanupThread;
+  }
+
+  /**
+   * The connection to the JobTracker, used by the TaskRunner 
+   * for locating remote files.
+   */
+  public InterTrackerProtocol getJobClient() {
+    return jobClient;
+  }
+        
+  /** Return the port at which the tasktracker bound to */
+  public synchronized InetSocketAddress getTaskTrackerReportAddress() {
+    return taskReportAddress;
+  }
+    
+  /** Queries the job tracker for a set of outputs ready to be copied
+   * @param fromEventId the first event ID we want to start from, this is
+   * modified by the call to this method
+   * @param jobClient the job tracker
+   * @return a set of locations to copy outputs from
+   * @throws IOException
+   */  
+  private List<TaskCompletionEvent> queryJobTracker(IntWritable fromEventId,
+                                                    JobID jobId,
+                                                    InterTrackerProtocol jobClient)
+    throws IOException {
+
+    TaskCompletionEvent t[] = jobClient.getTaskCompletionEvents(
+                                                                jobId,
+                                                                fromEventId.get(),
+                                                                probe_sample_size);
+    //we are interested in map task completion events only. So store
+    //only those
+    List <TaskCompletionEvent> recentMapEvents = 
+      new ArrayList<TaskCompletionEvent>();
+    for (int i = 0; i < t.length; i++) {
+      if (t[i].isMap) {
+        recentMapEvents.add(t[i]);
+      }
+    }
+    fromEventId.set(fromEventId.get() + t.length);
+    return recentMapEvents;
+  }
+
+  /**
+   * @return true if this tasktracker is permitted to connect to
+   *    the given jobtracker version
+   */
+  boolean isPermittedVersion(String jtBuildVersion, String jtVersion) {
+    boolean buildVersionMatch =
+      jtBuildVersion.equals(VersionInfo.getBuildVersion());
+    boolean versionMatch = jtVersion.equals(VersionInfo.getVersion());
+    if (buildVersionMatch && !versionMatch) {
+      throw new AssertionError("Invalid build. The build versions match" +
+          " but the JT version is " + jtVersion +
+          " and the TT version is " + VersionInfo.getVersion());
+    }
+    if (skipVersionCheck) {
+      LOG.info("Permitting tasktracker version '" + VersionInfo.getVersion() +
+          "' and build '" + VersionInfo.getBuildVersion() +
+          "' to connect to jobtracker version '" + jtVersion +
+          "' and build '" + jtBuildVersion + "' because " +
+          HADOOP_SKIP_VERSION_CHECK_KEY +
+          " is enabled");
+      return true;
+    } else {
+      if (relaxedVersionCheck) {
+        if (!buildVersionMatch && versionMatch) {
+          LOG.info("Permitting tasktracker build " + VersionInfo.getBuildVersion() +
+              " to connect to jobtracker build " + jtBuildVersion + " because " +
+              "hadoop.relaxed.worker.version.check is enabled");
+        }
+        return versionMatch;
+      } else {
+        return buildVersionMatch;
+      }
+    }
+  }
+
+  /**
+   * Main service loop.  Will stay in this loop forever.
+   */
+  State offerService() throws Exception {
+    long lastHeartbeat = 0;
+
+    while (running && !shuttingDown) {
+      try {
+        long now = System.currentTimeMillis();
+
+        long waitTime = heartbeatInterval - (now - lastHeartbeat);
+        if (waitTime > 0) {
+          // sleeps for the wait time or 
+          // until there are empty slots to schedule tasks
+          synchronized (finishedCount) {
+            if (finishedCount.get() == 0) {
+              finishedCount.wait(waitTime);
+            }
+            finishedCount.set(0);
+          }
+        }
+
+        // If the TaskTracker is just starting up:
+        // 1. Verify the versions matches with the JobTracker
+        // 2. Get the system directory & filesystem
+        if(justInited) {
+          String jtBuildVersion = jobClient.getBuildVersion();
+          String jtVersion = jobClient.getVIVersion();
+          if (!isPermittedVersion(jtBuildVersion, jtVersion)) {
+            String msg = "Shutting down. Incompatible version or revision." +
+                "TaskTracker version '" + VersionInfo.getVersion() +
+                "' and build '" + VersionInfo.getBuildVersion() +
+                "' and JobTracker version '" + jtVersion +
+                "' and build '" + jtBuildVersion +
+                " and hadoop.relaxed.worker.version.check" +
+                " is " + (relaxedVersionCheck ? "enabled" : "not enabled") +
+                " and " + HADOOP_SKIP_VERSION_CHECK_KEY +
+                " is " + (skipVersionCheck ? "enabled" : "not enabled");
+            LOG.fatal(msg);
+            try {
+              jobClient.reportTaskTrackerError(taskTrackerName, null, msg);
+            } catch(Exception e ) {
+              LOG.info("Problem reporting to jobtracker: " + e);
+            }
+            return State.DENIED;
+          }
+          
+          String dir = jobClient.getSystemDir();
+          if (dir == null) {
+            throw new IOException("Failed to get system directory");
+          }
+          systemDirectory = new Path(dir);
+          systemFS = systemDirectory.getFileSystem(fConf);
+        }
+
+        now = System.currentTimeMillis();
+        if (now > (lastCheckDirsTime + diskHealthCheckInterval)) {
+          localStorage.checkDirs(localFs, false);
+          lastCheckDirsTime = now;
+          int numFailures = localStorage.numFailures();
+          // Re-init the task tracker if there were any new failures
+          if (numFailures > lastNumFailures) {
+            lastNumFailures = numFailures;
+            return State.STALE;
+          }
+        }
+
+        // Send the heartbeat and process the jobtracker's directives
+        HeartbeatResponse heartbeatResponse = transmitHeartBeat(now);
+
+        // Note the time when the heartbeat returned, use this to decide when to send the
+        // next heartbeat   
+        lastHeartbeat = System.currentTimeMillis();
+        
+        // Check if the map-event list needs purging
+        Set<JobID> jobs = heartbeatResponse.getRecoveredJobs();
+        if (jobs.size() > 0) {
+          synchronized (this) {
+            // purge the local map events list
+            for (JobID job : jobs) {
+              RunningJob rjob;
+              synchronized (runningJobs) {
+                rjob = runningJobs.get(job);          
+                if (rjob != null) {
+                  synchronized (rjob) {
+                    FetchStatus f = rjob.getFetchStatus();
+                    if (f != null) {
+                      f.reset();
+                    }
+                  }
+                }
+              }
+            }
+
+            // Mark the reducers in shuffle for rollback
+            synchronized (shouldReset) {
+              for (Map.Entry<TaskAttemptID, TaskInProgress> entry 
+                   : runningTasks.entrySet()) {
+                if (entry.getValue().getStatus().getPhase() == Phase.SHUFFLE) {
+                  this.shouldReset.add(entry.getKey());
+                }
+              }
+            }
+          }
+        }
+        
+        TaskTrackerAction[] actions = heartbeatResponse.getActions();
+        if(LOG.isDebugEnabled()) {
+          LOG.debug("Got heartbeatResponse from JobTracker with responseId: " + 
+                    heartbeatResponse.getResponseId() + " and " + 
+                    ((actions != null) ? actions.length : 0) + " actions");
+        }
+        if (reinitTaskTracker(actions)) {
+          return State.STALE;
+        }
+            
+        // resetting heartbeat interval from the response.
+        heartbeatInterval = heartbeatResponse.getHeartbeatInterval();
+        justStarted = false;
+        justInited = false;
+        if (actions != null){ 
+          for(TaskTrackerAction action: actions) {
+            if (action instanceof LaunchTaskAction) {
+              addToTaskQueue((LaunchTaskAction)action);
+            } else if (action instanceof CommitTaskAction) {
+              CommitTaskAction commitAction = (CommitTaskAction)action;
+              if (!commitResponses.contains(commitAction.getTaskID())) {
+                LOG.info("Received commit task action for " + 
+                          commitAction.getTaskID());
+                commitResponses.add(commitAction.getTaskID());
+              }
+            } else {
+              tasksToCleanup.put(action);
+            }
+          }
+        }
+        markUnresponsiveTasks();
+        killOverflowingTasks();
+            
+        //we've cleaned up, resume normal operation
+        if (!acceptNewTasks && isIdle()) {
+          acceptNewTasks=true;
+        }
+        //The check below may not be required every iteration but we are 
+        //erring on the side of caution here. We have seen many cases where
+        //the call to jetty's getLocalPort() returns different values at 
+        //different times. Being a real paranoid here.
+        checkJettyPort(server.getPort());
+      } catch (InterruptedException ie) {
+        LOG.info("Interrupted. Closing down.");
+        return State.INTERRUPTED;
+      } catch (DiskErrorException de) {
+        String msg = "Exiting task tracker for disk error:\n" +
+          StringUtils.stringifyException(de);
+        LOG.error(msg);
+        synchronized (this) {
+          jobClient.reportTaskTrackerError(taskTrackerName, 
+                                           "DiskErrorException", msg);
+        }
+        // If we caught a DEE here we have no good dirs, therefore shutdown.
+        return State.DENIED;
+      } catch (RemoteException re) {
+        String reClass = re.getClassName();
+        if (DisallowedTaskTrackerException.class.getName().equals(reClass)) {
+          LOG.info("Tasktracker disallowed by JobTracker.");
+          return State.DENIED;
+        }
+      } catch (Exception except) {
+        String msg = "Caught exception: " + 
+          StringUtils.stringifyException(except);
+        LOG.error(msg);
+      }
+    }
+
+    return State.NORMAL;
+  }
+
+  private long previousUpdate = 0;
+
+  void setIndexCache(IndexCache cache) {
+    this.indexCache = cache;
+  }
+
+  /**
+   * Build and transmit the heart beat to the JobTracker
+   * @param now current time
+   * @return false if the tracker was unknown
+   * @throws IOException
+   */
+  HeartbeatResponse transmitHeartBeat(long now) throws IOException {
+    // Send Counters in the status once every COUNTER_UPDATE_INTERVAL
+    boolean sendAllCounters;
+    if (now > (previousUpdate + COUNTER_UPDATE_INTERVAL)) {
+      sendAllCounters = true;
+      previousUpdate = now;
+    }
+    else {
+      sendAllCounters = false;
+    }
+
+    // 
+    // Check if the last heartbeat got through... 
+    // if so then build the heartbeat information for the JobTracker;
+    // else resend the previous status information.
+    //
+    if (status == null) {
+      synchronized (this) {
+        status = new TaskTrackerStatus(taskTrackerName, shuffleScheme, localHostname,
+                                       shufflePort,
+                                       cloneAndResetRunningTaskStatuses(
+                                         sendAllCounters), 
+                                       taskFailures,
+                                       localStorage.numFailures(),
+                                       maxMapSlots,
+                                       maxReduceSlots); 
+      }
+    } else {
+      LOG.info("Resending 'status' to '" + jobTrackAddr.getHostName() +
+               "' with reponseId '" + heartbeatResponseId);
+    }
+      
+    //
+    // Check if we should ask for a new Task
+    //
+    boolean askForNewTask;
+    long localMinSpaceStart;
+    synchronized (this) {
+      askForNewTask = 
+        ((status.countOccupiedMapSlots() < maxMapSlots || 
+          status.countOccupiedReduceSlots() < maxReduceSlots) && 
+         acceptNewTasks); 
+      localMinSpaceStart = minSpaceStart;
+    }
+    if (askForNewTask) {
+      askForNewTask = enoughFreeSpace(localMinSpaceStart);
+      long freeDiskSpace = getFreeSpace();
+      long totVmem = getTotalVirtualMemoryOnTT();
+      long totPmem = getTotalPhysicalMemoryOnTT();
+      long availableVmem = getAvailableVirtualMemoryOnTT();
+      long availablePmem = getAvailablePhysicalMemoryOnTT();
+      long cumuCpuTime = getCumulativeCpuTimeOnTT();
+      long cpuFreq = getCpuFrequencyOnTT();
+      int numCpu = getNumProcessorsOnTT();
+      float cpuUsage = getCpuUsageOnTT();
+
+      status.getResourceStatus().setAvailableSpace(freeDiskSpace);
+      status.getResourceStatus().setTotalVirtualMemory(totVmem);
+      status.getResourceStatus().setTotalPhysicalMemory(totPmem);
+      status.getResourceStatus().setMapSlotMemorySizeOnTT(
+          mapSlotMemorySizeOnTT);
+      status.getResourceStatus().setReduceSlotMemorySizeOnTT(
+          reduceSlotSizeMemoryOnTT);
+      status.getResourceStatus().setAvailableVirtualMemory(availableVmem); 
+      status.getResourceStatus().setAvailablePhysicalMemory(availablePmem);
+      status.getResourceStatus().setCumulativeCpuTime(cumuCpuTime);
+      status.getResourceStatus().setCpuFrequency(cpuFreq);
+      status.getResourceStatus().setNumProcessors(numCpu);
+      status.getResourceStatus().setCpuUsage(cpuUsage);
+    }
+    //add node health information
+    
+    TaskTrackerHealthStatus healthStatus = status.getHealthStatus();
+    synchronized (this) {
+      if (healthChecker != null) {
+        healthChecker.setHealthStatus(healthStatus);
+      } else {
+        healthStatus.setNodeHealthy(true);
+        healthStatus.setLastReported(0L);
+        healthStatus.setHealthReport("");
+      }
+    }
+    //
+    // Xmit the heartbeat
+    //
+    HeartbeatResponse heartbeatResponse = jobClient.heartbeat(status, 
+                                                              justStarted,
+                                                              justInited,
+                                                              askForNewTask, 
+                                                              heartbeatResponseId);
+      
+    //
+    // The heartbeat got through successfully!
+    //
+    heartbeatResponseId = heartbeatResponse.getResponseId();
+      
+    synchronized (this) {
+      for (TaskStatus taskStatus : status.getTaskReports()) {
+        if (taskStatus.getRunState() != TaskStatus.State.RUNNING &&
+            taskStatus.getRunState() != TaskStatus.State.UNASSIGNED &&
+            taskStatus.getRunState() != TaskStatus.State.COMMIT_PENDING &&
+            !taskStatus.inTaskCleanupPhase()) {
+          if (taskStatus.getIsMap()) {
+            mapTotal--;
+          } else {
+            reduceTotal--;
+          }
+          try {
+            myInstrumentation.completeTask(taskStatus.getTaskID());
+          } catch (MetricsException me) {
+            LOG.warn("Caught: " + StringUtils.stringifyException(me));
+          }
+          runningTasks.remove(taskStatus.getTaskID());
+        }
+      }
+      
+      // Clear transient status information which should only
+      // be sent once to the JobTracker
+      for (TaskInProgress tip: runningTasks.values()) {
+        tip.getStatus().clearStatus();
+      }
+    }
+
+    // Force a rebuild of 'status' on the next iteration
+    status = null;                                
+
+    return heartbeatResponse;
+  }
+
+  /**
+   * Return the total virtual memory available on this TaskTracker.
+   * @return total size of virtual memory.
+   */
+  long getTotalVirtualMemoryOnTT() {
+    return totalVirtualMemoryOnTT;
+  }
+
+  /**
+   * Return the total physical memory available on this TaskTracker.
+   * @return total size of physical memory.
+   */
+  long getTotalPhysicalMemoryOnTT() {
+    return totalPhysicalMemoryOnTT;
+  }
+
+  /**
+   * Return the free virtual memory available on this TaskTracker.
+   * @return total size of free virtual memory.
+   */
+  long getAvailableVirtualMemoryOnTT() {
+    long availableVirtualMemoryOnTT = TaskTrackerStatus.UNAVAILABLE;
+    if (resourceCalculatorPlugin != null) {
+      availableVirtualMemoryOnTT =
+              resourceCalculatorPlugin.getAvailableVirtualMemorySize();
+    }
+    return availableVirtualMemoryOnTT;
+  }
+
+  /**
+   * Return the free physical memory available on this TaskTracker.
+   * @return total size of free physical memory in bytes
+   */
+  long getAvailablePhysicalMemoryOnTT() {
+    long availablePhysicalMemoryOnTT = TaskTrackerStatus.UNAVAILABLE;
+    if (resourceCalculatorPlugin != null) {
+      availablePhysicalMemoryOnTT =
+              resourceCalculatorPlugin.getAvailablePhysicalMemorySize();
+    }
+    return availablePhysicalMemoryOnTT;
+  }
+
+  /**
+   * Return the cumulative CPU used time on this TaskTracker since system is on
+   * @return cumulative CPU used time in millisecond
+   */
+  long getCumulativeCpuTimeOnTT() {
+    long cumulativeCpuTime = TaskTrackerStatus.UNAVAILABLE;
+    if (resourceCalculatorPlugin != null) {
+      cumulativeCpuTime = resourceCalculatorPlugin.getCumulativeCpuTime();
+    }
+    return cumulativeCpuTime;
+  }
+
+  /**
+   * Return the number of Processors on this TaskTracker
+   * @return number of processors
+   */
+  int getNumProcessorsOnTT() {
+    int numProcessors = TaskTrackerStatus.UNAVAILABLE;
+    if (resourceCalculatorPlugin != null) {
+      numProcessors = resourceCalculatorPlugin.getNumProcessors();
+    }
+    return numProcessors;
+  }
+
+  /**
+   * Return the CPU frequency of this TaskTracker
+   * @return CPU frequency in kHz
+   */
+  long getCpuFrequencyOnTT() {
+    long cpuFrequency = TaskTrackerStatus.UNAVAILABLE;
+    if (resourceCalculatorPlugin != null) {
+      cpuFrequency = resourceCalculatorPlugin.getCpuFrequency();
+    }
+    return cpuFrequency;
+  }
+
+  /**
+   * Return the CPU usage in % of this TaskTracker
+   * @return CPU usage in %
+   */
+  float getCpuUsageOnTT() {
+    float cpuUsage = TaskTrackerStatus.UNAVAILABLE;
+    if (resourceCalculatorPlugin != null) {
+      cpuUsage = resourceCalculatorPlugin.getCpuUsage();
+    }
+    return cpuUsage;
+  }
+  
+  long getTotalMemoryAllottedForTasksOnTT() {
+    return totalMemoryAllottedForTasks;
+  }
+
+  long getRetainSize(org.apache.hadoop.mapreduce.TaskAttemptID tid) {
+    return tid.isMap() ? mapRetainSize : reduceRetainSize;
+  }
+  
+  /**
+   * @return The amount of physical memory that will not be used for running
+   *         tasks in bytes. Returns JobConf.DISABLED_MEMORY_LIMIT if it is not
+   *         configured.
+   */
+  long getReservedPhysicalMemoryOnTT() {
+    return reservedPhysicalMemoryOnTT;
+  }
+
+  /**
+   * Check if the jobtracker directed a 'reset' of the tasktracker.
+   * 
+   * @param actions the directives of the jobtracker for the tasktracker.
+   * @return <code>true</code> if tasktracker is to be reset, 
+   *         <code>false</code> otherwise.
+   */
+  private boolean reinitTaskTracker(TaskTrackerAction[] actions) {
+    if (actions != null) {
+      for (TaskTrackerAction action : actions) {
+        if (action.getActionId() == 
+            TaskTrackerAction.ActionType.REINIT_TRACKER) {
+          LOG.info("Recieved RenitTrackerAction from JobTracker");
+          return true;
+        }
+      }
+    }
+    return false;
+  }
+    
+  /**
+   * Kill any tasks that have not reported progress in the last X seconds.
+   */
+  private synchronized void markUnresponsiveTasks() throws IOException {
+    long now = System.currentTimeMillis();
+    for (TaskInProgress tip: runningTasks.values()) {
+      if (tip.getRunState() == TaskStatus.State.RUNNING ||
+          tip.getRunState() == TaskStatus.State.COMMIT_PENDING ||
+          tip.isCleaningup()) {
+        // Check the per-job timeout interval for tasks;
+        // an interval of '0' implies it is never timed-out
+        long jobTaskTimeout = tip.getTaskTimeout();
+        if (jobTaskTimeout == 0) {
+          continue;
+        }
+          
+        // Check if the task has not reported progress for a 
+        // time-period greater than the configured time-out
+        long timeSinceLastReport = now - tip.getLastProgressReport();
+        if (timeSinceLastReport > jobTaskTimeout && !tip.wasKilled) {
+          String msg = 
+            "Task " + tip.getTask().getTaskID() + " failed to report status for " 
+            + (timeSinceLastReport / 1000) + " seconds. Killing!";
+          LOG.info(tip.getTask().getTaskID() + ": " + msg);
+          ReflectionUtils.logThreadInfo(LOG, "lost task", 30);
+          tip.reportDiagnosticInfo(msg);
+          myInstrumentation.timedoutTask(tip.getTask().getTaskID());
+          purgeTask(tip, true);
+        }
+      }
+    }
+  }
+
+  /**
+   * The task tracker is done with this job, so we need to clean up.
+   * @param action The action with the job
+   * @throws IOException
+   */
+  synchronized void purgeJob(KillJobAction action) throws IOException {
+    JobID jobId = action.getJobID();
+    LOG.info("Received 'KillJobAction' for job: " + jobId);
+    RunningJob rjob = null;
+    synchronized (runningJobs) {
+      rjob = runningJobs.get(jobId);
+    }
+      
+    if (rjob == null) {
+      LOG.warn("Unknown job " + jobId + " being deleted.");
+    } else {
+      synchronized (rjob) {
+        // decrement the reference counts for the items this job references
+        rjob.distCacheMgr.release();
+        // Add this tips of this job to queue of tasks to be purged 
+        for (TaskInProgress tip : rjob.tasks) {
+          tip.jobHasFinished(false);
+          Task t = tip.getTask();
+          if (t.isMapTask()) {
+            indexCache.removeMap(tip.getTask().getTaskID().toString());
+          }
+        }
+        // Delete the job directory for this  
+        // task if the job is done/failed
+        if (!rjob.keepJobFiles) {
+          removeJobFiles(rjob.ugi.getShortUserName(), rjob.getJobID());
+        }
+        // add job to user log manager
+        long now = System.currentTimeMillis();
+        JobCompletedEvent jca = new JobCompletedEvent(rjob
+            .getJobID(), now, UserLogCleaner.getUserlogRetainHours(rjob
+            .getJobConf()));
+        getUserLogManager().addLogEvent(jca);
+
+        // Remove this job 
+        rjob.tasks.clear();
+        // Close all FileSystems for this job
+        try {
+          FileSystem.closeAllForUGI(rjob.getUGI());
+        } catch (IOException ie) {
+          LOG.warn("Ignoring exception " + StringUtils.stringifyException(ie) + 
+              " while closing FileSystem for " + rjob.getUGI());
+        }
+      }
+    }
+
+    synchronized(runningJobs) {
+      runningJobs.remove(jobId);
+    }
+    getJobTokenSecretManager().removeTokenForJob(jobId.toString());  
+    distributedCacheManager.removeTaskDistributedCacheManager(jobId);
+  }
+
+  /**
+   * This job's files are no longer needed on this TT, remove them.
+   *
+   * @param rjob
+   * @throws IOException
+   */
+  void removeJobFiles(String user, JobID jobId) throws IOException {
+    String userDir = getUserDir(user);
+    String jobDir = getLocalJobDir(user, jobId.toString());
+    PathDeletionContext jobCleanup = 
+      new TaskController.DeletionContext(getTaskController(), false, user, 
+                                         jobDir.substring(userDir.length()));
+    directoryCleanupThread.addToQueue(jobCleanup);
+    
+    for (String str : localStorage.getDirs()) {
+      Path ttPrivateJobDir = FileSystem.getLocal(fConf).makeQualified(
+        new Path(str, TaskTracker.getPrivateDirForJob(user, jobId.toString())));
+      PathDeletionContext ttPrivateJobCleanup =
+        new CleanupQueue.PathDeletionContext(ttPrivateJobDir, fConf);
+      directoryCleanupThread.addToQueue(ttPrivateJobCleanup);
+    }
+  }
+
+  /**
+   * Remove the tip and update all relevant state.
+   * 
+   * @param tip {@link TaskInProgress} to be removed.
+   * @param wasFailure did the task fail or was it killed?
+   */
+  private void purgeTask(TaskInProgress tip, boolean wasFailure) 
+  throws IOException {
+    if (tip != null) {
+      LOG.info("About to purge task: " + tip.getTask().getTaskID());
+        
+      // Remove the task from running jobs, 
+      // removing the job if it's the last task
+      removeTaskFromJob(tip.getTask().getJobID(), tip);
+      tip.jobHasFinished(wasFailure);
+      if (tip.getTask().isMapTask()) {
+        indexCache.removeMap(tip.getTask().getTaskID().toString());
+      }
+    }
+  }
+
+  /** Check if we're dangerously low on disk space
+   * If so, kill jobs to free up space and make sure
+   * we don't accept any new tasks
+   * Try killing the reduce jobs first, since I believe they
+   * use up most space
+   * Then pick the one with least progress
+   */
+  private void killOverflowingTasks() throws IOException {
+    long localMinSpaceKill;
+    synchronized(this){
+      localMinSpaceKill = minSpaceKill;  
+    }
+    if (!enoughFreeSpace(localMinSpaceKill)) {
+      acceptNewTasks=false; 
+      //we give up! do not accept new tasks until
+      //all the ones running have finished and they're all cleared up
+      synchronized (this) {
+        TaskInProgress killMe = findTaskToKill(null);
+
+        if (killMe!=null) {
+          String msg = "Tasktracker running out of space." +
+            " Killing task.";
+          LOG.info(killMe.getTask().getTaskID() + ": " + msg);
+          killMe.reportDiagnosticInfo(msg);
+          purgeTask(killMe, false);
+        }
+      }
+    }
+  }
+
+  /**
+   * Pick a task to kill to free up memory/disk-space 
+   * @param tasksToExclude tasks that are to be excluded while trying to find a
+   *          task to kill. If null, all runningTasks will be searched.
+   * @return the task to kill or null, if one wasn't found
+   */
+  synchronized TaskInProgress findTaskToKill(List<TaskAttemptID> tasksToExclude) {
+    TaskInProgress killMe = null;
+    for (Iterator it = runningTasks.values().iterator(); it.hasNext();) {
+      TaskInProgress tip = (TaskInProgress) it.next();
+
+      if (tasksToExclude != null
+          && tasksToExclude.contains(tip.getTask().getTaskID())) {
+        // exclude this task
+        continue;
+      }
+
+      if ((tip.getRunState() == TaskStatus.State.RUNNING ||
+           tip.getRunState() == TaskStatus.State.COMMIT_PENDING) &&
+          !tip.wasKilled) {
+                
+        if (killMe == null) {
+          killMe = tip;
+
+        } else if (!tip.getTask().isMapTask()) {
+          //reduce task, give priority
+          if (killMe.getTask().isMapTask() || 
+              (tip.getTask().getProgress().get() < 
+               killMe.getTask().getProgress().get())) {
+
+            killMe = tip;
+          }
+
+        } else if (killMe.getTask().isMapTask() &&
+                   tip.getTask().getProgress().get() < 
+                   killMe.getTask().getProgress().get()) {
+          //map task, only add if the progress is lower
+
+          killMe = tip;
+        }
+      }
+    }
+    return killMe;
+  }
+
+  /**
+   * Check if any of the local directories has enough
+   * free space  (more than minSpace)
+   * 
+   * If not, do not try to get a new task assigned 
+   * @return
+   * @throws IOException 
+   */
+  private boolean enoughFreeSpace(long minSpace) throws IOException {
+    if (minSpace == 0) {
+      return true;
+    }
+    return minSpace < getFreeSpace();
+  }
+  
+  private long getFreeSpace() throws IOException {
+    long biggestSeenSoFar = 0;
+    String[] localDirs = localStorage.getDirs();
+    for (int i = 0; i < localDirs.length; i++) {
+      DF df = null;
+      if (localDirsDf.containsKey(localDirs[i])) {
+        df = localDirsDf.get(localDirs[i]);
+      } else {
+        df = new DF(new File(localDirs[i]), fConf);
+        localDirsDf.put(localDirs[i], df);
+      }
+
+      long availOnThisVol = df.getAvailable();
+      if (availOnThisVol > biggestSeenSoFar) {
+        biggestSeenSoFar = availOnThisVol;
+      }
+    }
+    
+    //Should ultimately hold back the space we expect running tasks to use but 
+    //that estimate isn't currently being passed down to the TaskTrackers    
+    return biggestSeenSoFar;
+  }
+    
+  private TaskLauncher mapLauncher;
+  private TaskLauncher reduceLauncher;
+  public JvmManager getJvmManagerInstance() {
+    return jvmManager;
+  }
+
+  // called from unit test  
+  void setJvmManagerInstance(JvmManager jvmManager) {
+    this.jvmManager = jvmManager;
+  }
+
+  private void addToTaskQueue(LaunchTaskAction action) {
+    if (action.getTask().isMapTask()) {
+      mapLauncher.addToTaskQueue(action);
+    } else {
+      reduceLauncher.addToTaskQueue(action);
+    }
+  }
+  
+  class TaskLauncher extends Thread {
+    private IntWritable numFreeSlots;
+    private final int maxSlots;
+    private List<TaskInProgress> tasksToLaunch;
+
+    public TaskLauncher(TaskType taskType, int numSlots) {
+      this.maxSlots = numSlots;
+      this.numFreeSlots = new IntWritable(numSlots);
+      this.tasksToLaunch = new LinkedList<TaskInProgress>();
+      setDaemon(true);
+      setName("TaskLauncher for " + taskType + " tasks");
+    }
+
+    public void addToTaskQueue(LaunchTaskAction action) {
+      synchronized (tasksToLaunch) {
+        TaskInProgress tip = registerTask(action, this);
+        tasksToLaunch.add(tip);
+        tasksToLaunch.notifyAll();
+      }
+    }
+    
+    public void cleanTaskQueue() {
+      tasksToLaunch.clear();
+    }
+    
+    public void addFreeSlots(int numSlots) {
+      synchronized (numFreeSlots) {
+        numFreeSlots.set(numFreeSlots.get() + numSlots);
+        assert (numFreeSlots.get() <= maxSlots);
+        LOG.info("addFreeSlot : current free slots : " + numFreeSlots.get());
+        numFreeSlots.notifyAll();
+      }
+    }
+    
+    void notifySlots() {
+      synchronized (numFreeSlots) {
+        numFreeSlots.notifyAll();
+      }
+    }
+
+    int getNumWaitingTasksToLaunch() {
+      synchronized (tasksToLaunch) {
+        return tasksToLaunch.size();
+      }
+    }
+
+    public void run() {
+      while (!Thread.interrupted()) {
+        try {
+          TaskInProgress tip;
+          Task task;
+          synchronized (tasksToLaunch) {
+            while (tasksToLaunch.isEmpty()) {
+              tasksToLaunch.wait();
+            }
+            //get the TIP
+            tip = tasksToLaunch.remove(0);
+            task = tip.getTask();
+            LOG.info("Trying to launch : " + tip.getTask().getTaskID() + 
+                     " which needs " + task.getNumSlotsRequired() + " slots");
+          }
+          //wait for free slots to run
+          synchronized (numFreeSlots) {
+            boolean canLaunch = true;
+            while (numFreeSlots.get() < task.getNumSlotsRequired()) {
+              //Make sure that there is no kill task action for this task!
+              //We are not locking tip here, because it would reverse the
+              //locking order!
+              //Also, Lock for the tip is not required here! because :
+              // 1. runState of TaskStatus is volatile
+              // 2. Any notification is not missed because notification is
+              // synchronized on numFreeSlots. So, while we are doing the check,
+              // if the tip is half way through the kill(), we don't miss
+              // notification for the following wait().
+              if (!tip.canBeLaunched()) {
+                //got killed externally while still in the launcher queue
+                LOG.info("Not blocking slots for " + task.getTaskID()
+                    + " as it got killed externally. Task's state is "
+                    + tip.getRunState());
+                canLaunch = false;
+                break;
+              }
+              LOG.info("TaskLauncher : Waiting for " + task.getNumSlotsRequired() + 
+                       " to launch " + task.getTaskID() + ", currently we have " + 
+                       numFreeSlots.get() + " free slots");
+              numFreeSlots.wait();
+            }
+            if (!canLaunch) {
+              continue;
+            }
+            LOG.info("In TaskLauncher, current free slots : " + numFreeSlots.get()+
+                     " and trying to launch "+tip.getTask().getTaskID() + 
+                     " which needs " + task.getNumSlotsRequired() + " slots");
+            numFreeSlots.set(numFreeSlots.get() - task.getNumSlotsRequired());
+            assert (numFreeSlots.get() >= 0);
+          }
+          synchronized (tip) {
+            //to make sure that there is no kill task action for this
+            if (!tip.canBeLaunched()) {
+              //got killed externally while still in the launcher queue
+              LOG.info("Not launching task " + task.getTaskID() + " as it got"
+                + " killed externally. Task's state is " + tip.getRunState());
+              addFreeSlots(task.getNumSlotsRequired());
+              continue;
+            }
+            tip.slotTaken = true;
+          }
+          //got a free slot. launch the task
+          startNewTask(tip);
+        } catch (InterruptedException e) { 
+          return; // ALL DONE
+        } catch (Throwable th) {
+          LOG.error("TaskLauncher error " + 
+              StringUtils.stringifyException(th));
+        }
+      }
+    }
+  }
+  private TaskInProgress registerTask(LaunchTaskAction action, 
+      TaskLauncher launcher) {
+    Task t = action.getTask();
+    LOG.info("LaunchTaskAction (registerTask): " + t.getTaskID() +
+             " task's state:" + t.getState());
+    TaskInProgress tip = new TaskInProgress(t, this.fConf, launcher);
+    synchronized (this) {
+      tasks.put(t.getTaskID(), tip);
+      runningTasks.put(t.getTaskID(), tip);
+      boolean isMap = t.isMapTask();
+      if (isMap) {
+        mapTotal++;
+      } else {
+        reduceTotal++;
+      }
+    }
+    return tip;
+  }
+  /**
+   * Start a new task.
+   * All exceptions are handled locally, so that we don't mess up the
+   * task tracker.
+   * @throws InterruptedException 
+   */
+  void startNewTask(TaskInProgress tip) throws InterruptedException {
+    try {
+      RunningJob rjob = localizeJob(tip);
+      tip.getTask().setJobFile(rjob.localizedJobConf.toString());
+      // Localization is done. Neither rjob.jobConf nor rjob.ugi can be null
+      launchTaskForJob(tip, new JobConf(rjob.jobConf), rjob); 
+    } catch (Throwable e) {
+      String msg = ("Error initializing " + tip.getTask().getTaskID() + 
+                    ":\n" + StringUtils.stringifyException(e));
+      LOG.warn(msg);
+      tip.reportDiagnosticInfo(msg);
+      try {
+        tip.kill(true);
+        tip.cleanup(true);
+      } catch (IOException ie2) {
+        LOG.info("Error cleaning up " + tip.getTask().getTaskID(), ie2);
+      } catch (InterruptedException ie2) {
+        LOG.info("Error cleaning up " + tip.getTask().getTaskID(), ie2);
+      }
+        
+      // Careful! 
+      // This might not be an 'Exception' - don't handle 'Error' here!
+      if (e instanceof Error) {
+        throw ((Error) e);
+      }
+    }
+  }
+  
+  void addToMemoryManager(TaskAttemptID attemptId, boolean isMap,
+                          JobConf conf) {
+    if (!isTaskMemoryManagerEnabled()) {
+      return; // Skip this if TaskMemoryManager is not enabled.
+    }
+    // Obtain physical memory limits from the job configuration
+    long physicalMemoryLimit =
+      conf.getLong(isMap ? JobContext.MAP_MEMORY_PHYSICAL_MB :
+                   JobContext.REDUCE_MEMORY_PHYSICAL_MB,
+                   JobConf.DISABLED_MEMORY_LIMIT);
+    if (physicalMemoryLimit > 0) {
+      physicalMemoryLimit *= 1024L * 1024L;
+    }
+
+    // Obtain virtual memory limits from the job configuration
+    long virtualMemoryLimit = isMap ?
+      conf.getMemoryForMapTask() * 1024 * 1024 :
+      conf.getMemoryForReduceTask() * 1024 * 1024;
+
+    taskMemoryManager.addTask(attemptId, virtualMemoryLimit,
+                              physicalMemoryLimit);
+  }
+
+  void removeFromMemoryManager(TaskAttemptID attemptId) {
+    // Remove the entry from taskMemoryManagerThread's data structures.
+    if (isTaskMemoryManagerEnabled()) {
+      taskMemoryManager.removeTask(attemptId);
+    }
+  }
+
+  /** 
+   * Notify the tasktracker to send an out-of-band heartbeat.
+   */
+  private void notifyTTAboutTaskCompletion() {
+    if (oobHeartbeatOnTaskCompletion) {
+      synchronized (finishedCount) {
+        int value = finishedCount.get();
+        finishedCount.set(value+1);
+        finishedCount.notify();
+      }
+    }
+  }
+  
+  /**
+   * The server retry loop.  
+   * This while-loop attempts to connect to the JobTracker.  It only 
+   * loops when the old TaskTracker has gone bad (its state is
+   * stale somehow) and we need to reinitialize everything.
+   */
+  public void run() {
+    try {
+      getUserLogManager().start();
+      startCleanupThreads();
+      boolean denied = false;
+      while (running && !shuttingDown) {
+        boolean staleState = false;
+        try {
+          // This while-loop attempts reconnects if we get network errors
+          while (running && !staleState && !shuttingDown && !denied) {
+            try {
+              State osState = offerService();
+              if (osState == State.STALE) {
+                staleState = true;
+              } else if (osState == State.DENIED) {
+                denied = true;
+              }
+            } catch (Exception ex) {
+              if (!shuttingDown) {
+                LOG.info("Lost connection to JobTracker [" +
+                         jobTrackAddr + "].  Retrying...", ex);
+                try {
+                  Thread.sleep(5000);
+                } catch (InterruptedException ie) {
+                }
+              }
+            }
+          }
+        } finally {
+          // If denied we'll close via shutdown below. We should close
+          // here even if shuttingDown as shuttingDown can be set even
+          // if shutdown is not called.
+          if (!denied) {
+            close();
+          }
+        }
+        if (shuttingDown) { return; }
+        if (denied) { break; }
+        LOG.warn("Reinitializing local state");
+        initialize();
+      }
+      if (denied) {
+        shutdown();
+      }
+    } catch (IOException iex) {
+      LOG.error("Got fatal exception while reinitializing TaskTracker: " +
+                StringUtils.stringifyException(iex));
+      return;
+    } catch (InterruptedException i) {
+      LOG.error("Got interrupted while reinitializing TaskTracker: " +
+          i.getMessage());
+      return;
+    }
+  }
+    
+  ///////////////////////////////////////////////////////
+  // TaskInProgress maintains all the info for a Task that
+  // lives at this TaskTracker.  It maintains the Task object,
+  // its TaskStatus, and the TaskRunner.
+  ///////////////////////////////////////////////////////
+  class TaskInProgress {
+    Task task;
+    long lastProgressReport;
+    StringBuffer diagnosticInfo = new StringBuffer();
+    private TaskRunner runner;
+    volatile boolean done = false;
+    volatile boolean wasKilled = false;
+    private JobConf ttConf;
+    private JobConf localJobConf;
+    private boolean keepFailedTaskFiles;
+    private boolean alwaysKeepTaskFiles;
+    private TaskStatus taskStatus; 
+    private long taskTimeout;
+    private String debugCommand;
+    private volatile boolean slotTaken = false;
+    private TaskLauncher launcher;
+
+    // The ugi of the user who is running the job. This contains all the tokens
+    // too which will be populated during job-localization
+    private UserGroupInformation ugi;
+
+    UserGroupInformation getUGI() {
+      return ugi;
+    }
+
+    void setUGI(UserGroupInformation userUGI) {
+      ugi = userUGI;
+    }
+
+    /**
+     */
+    public TaskInProgress(Task task, JobConf conf) {
+      this(task, conf, null);
+    }
+    
+    public TaskInProgress(Task task, JobConf conf, TaskLauncher launcher) {
+      this.task = task;
+      this.launcher = launcher;
+      this.lastProgressReport = System.currentTimeMillis();
+      this.ttConf = conf;
+      localJobConf = null;
+      taskStatus = TaskStatus.createTaskStatus(task.isMapTask(), task.getTaskID(), 
+                                               0.0f, 
+                                               task.getNumSlotsRequired(),
+                                               task.getState(),
+                                               diagnosticInfo.toString(), 
+                                               "initializing",  
+                                               getName(), 
+                                               task.isTaskCleanupTask() ? 
+                                                 TaskStatus.Phase.CLEANUP :  
+                                               task.isMapTask()? TaskStatus.Phase.MAP:
+                                               TaskStatus.Phase.SHUFFLE,
+                                               task.getCounters()); 
+      taskTimeout = (10 * 60 * 1000);
+    }
+        
+    void localizeTask(Task task) throws IOException{
+
+      // Do the task-type specific localization
+//TODO: are these calls really required
+      task.localizeConfiguration(localJobConf);
+      
+      task.setConf(localJobConf);
+    }
+        
+    /**
+     */
+    public Task getTask() {
+      return task;
+    }
+    
+    TaskRunner getTaskRunner() {
+      return runner;
+    }
+
+    void setTaskRunner(TaskRunner rnr) {
+      this.runner = rnr;
+    }
+
+    public synchronized void setJobConf(JobConf lconf){
+      this.localJobConf = lconf;
+      keepFailedTaskFiles = localJobConf.getKeepFailedTaskFiles();
+      taskTimeout = localJobConf.getLong("mapred.task.timeout", 
+                                         10 * 60 * 1000);
+      if (task.isMapTask()) {
+        debugCommand = localJobConf.getMapDebugScript();
+      } else {
+        debugCommand = localJobConf.getReduceDebugScript();
+      }
+      String keepPattern = localJobConf.getKeepTaskFilesPattern();
+      if (keepPattern != null) {
+        alwaysKeepTaskFiles = 
+          Pattern.matches(keepPattern, task.getTaskID().toString());
+      } else {
+        alwaysKeepTaskFiles = false;
+      }
+    }
+        
+    public synchronized JobConf getJobConf() {
+      return localJobConf;
+    }
+        
+    /**
+     */
+    public synchronized TaskStatus getStatus() {
+      taskStatus.setDiagnosticInfo(diagnosticInfo.toString());
+      if (diagnosticInfo.length() > 0) {
+        diagnosticInfo = new StringBuffer();
+      }
+      
+      return taskStatus;
+    }
+
+    /**
+     * Kick off the task execution
+     */
+    public synchronized void launchTask(RunningJob rjob) throws IOException {
+      if (this.taskStatus.getRunState() == TaskStatus.State.UNASSIGNED ||
+          this.taskStatus.getRunState() == TaskStatus.State.FAILED_UNCLEAN ||
+          this.taskStatus.getRunState() == TaskStatus.State.KILLED_UNCLEAN) {
+        localizeTask(task);
+        if (this.taskStatus.getRunState() == TaskStatus.State.UNASSIGNED) {
+          this.taskStatus.setRunState(TaskStatus.State.RUNNING);
+        }
+        setTaskRunner(task.createRunner(TaskTracker.this, this, rjob));
+        this.runner.start();
+        long now = System.currentTimeMillis();
+        this.taskStatus.setStartTime(now);
+        this.lastProgressReport = now;
+      } else {
+        LOG.info("Not launching task: " + task.getTaskID() + 
+            " since it's state is " + this.taskStatus.getRunState());
+      }
+    }
+
+    boolean isCleaningup() {
+   	  return this.taskStatus.inTaskCleanupPhase();
+    }
+    
+    // checks if state has been changed for the task to be launched
+    boolean canBeLaunched() {
+      return (getRunState() == TaskStatus.State.UNASSIGNED ||
+          getRunState() == TaskStatus.State.FAILED_UNCLEAN ||
+          getRunState() == TaskStatus.State.KILLED_UNCLEAN);
+    }
+
+    /**
+     * The task is reporting its progress
+     */
+    public synchronized void reportProgress(TaskStatus taskStatus) 
+    {
+      LOG.info(task.getTaskID() + " " + taskStatus.getProgress() + 
+          "% " + taskStatus.getStateString());
+      // task will report its state as
+      // COMMIT_PENDING when it is waiting for commit response and 
+      // when it is committing.
+      // cleanup attempt will report its state as FAILED_UNCLEAN/KILLED_UNCLEAN
+      if (this.done || 
+          (this.taskStatus.getRunState() != TaskStatus.State.RUNNING &&
+          this.taskStatus.getRunState() != TaskStatus.State.COMMIT_PENDING &&
+          !isCleaningup()) ||
+          ((this.taskStatus.getRunState() == TaskStatus.State.COMMIT_PENDING ||
+           this.taskStatus.getRunState() == TaskStatus.State.FAILED_UNCLEAN ||
+           this.taskStatus.getRunState() == TaskStatus.State.KILLED_UNCLEAN) &&
+           (taskStatus.getRunState() == TaskStatus.State.RUNNING ||
+            taskStatus.getRunState() == TaskStatus.State.UNASSIGNED))) {
+        //make sure we ignore progress messages after a task has 
+        //invoked TaskUmbilicalProtocol.done() or if the task has been
+        //KILLED/FAILED/FAILED_UNCLEAN/KILLED_UNCLEAN
+        //Also ignore progress update if the state change is from 
+        //COMMIT_PENDING/FAILED_UNCLEAN/KILLED_UNCLEA to RUNNING or UNASSIGNED
+        LOG.info(task.getTaskID() + " Ignoring status-update since " +
+                 ((this.done) ? "task is 'done'" : 
+                                ("runState: " + this.taskStatus.getRunState()))
+                 ); 
+        return;
+      }
+      
+      this.taskStatus.statusUpdate(taskStatus);
+      this.lastProgressReport = System.currentTimeMillis();
+    }
+
+    /**
+     */
+    public long getLastProgressReport() {
+      return lastProgressReport;
+    }
+
+    /**
+     */
+    public TaskStatus.State getRunState() {
+      return taskStatus.getRunState();
+    }
+
+    /**
+     * The task's configured timeout.
+     * 
+     * @return the task's configured timeout.
+     */
+    public long getTaskTimeout() {
+      return taskTimeout;
+    }
+        
+    /**
+     * The task has reported some diagnostic info about its status
+     */
+    public synchronized void reportDiagnosticInfo(String info) {
+      this.diagnosticInfo.append(info);
+    }
+    
+    public synchronized void reportNextRecordRange(SortedRanges.Range range) {
+      this.taskStatus.setNextRecordRange(range);
+    }
+
+    /**
+     * The task is reporting that it's done running
+     */
+    public synchronized void reportDone() {
+      if (isCleaningup()) {
+        if (this.taskStatus.getRunState() == TaskStatus.State.FAILED_UNCLEAN) {
+          this.taskStatus.setRunState(TaskStatus.State.FAILED);
+        } else if (this.taskStatus.getRunState() == 
+                   TaskStatus.State.KILLED_UNCLEAN) {
+          this.taskStatus.setRunState(TaskStatus.State.KILLED);
+        }
+      } else {
+        this.taskStatus.setRunState(TaskStatus.State.SUCCEEDED);
+      }
+      this.taskStatus.setProgress(1.0f);
+      this.taskStatus.setFinishTime(System.currentTimeMillis());
+      this.done = true;
+      jvmManager.taskFinished(runner);
+      runner.signalDone();
+      LOG.info("Task " + task.getTaskID() + " is done.");
+      LOG.info("reported output size for " + task.getTaskID() +  "  was " + taskStatus.getOutputSize());
+
+    }
+    
+    public boolean wasKilled() {
+      return wasKilled;
+    }
+
+    /**
+     * A task is reporting in as 'done'.
+     * 
+     * We need to notify the tasktracker to send an out-of-band heartbeat.
+     * If isn't <code>commitPending</code>, we need to finalize the task
+     * and release the slot it's occupied.
+     * 
+     * @param commitPending is the task-commit pending?
+     */
+    void reportTaskFinished(boolean commitPending) {
+      if (!commitPending) {
+        taskFinished();
+        releaseSlot();
+      }
+      notifyTTAboutTaskCompletion();
+    }
+
+    /* State changes:
+     * RUNNING/COMMIT_PENDING -> FAILED_UNCLEAN/FAILED/KILLED_UNCLEAN/KILLED
+     * FAILED_UNCLEAN -> FAILED
+     * KILLED_UNCLEAN -> KILLED 
+     */
+    private void setTaskFailState(boolean wasFailure) {
+      // go FAILED_UNCLEAN -> FAILED and KILLED_UNCLEAN -> KILLED always
+      if (taskStatus.getRunState() == TaskStatus.State.FAILED_UNCLEAN) {
+        taskStatus.setRunState(TaskStatus.State.FAILED);
+      } else if (taskStatus.getRunState() == 
+                 TaskStatus.State.KILLED_UNCLEAN) {
+        taskStatus.setRunState(TaskStatus.State.KILLED);
+      } else if (task.isMapOrReduce() && 
+                 taskStatus.getPhase() != TaskStatus.Phase.CLEANUP) {
+        if (wasFailure) {
+          taskStatus.setRunState(TaskStatus.State.FAILED_UNCLEAN);
+        } else {
+          taskStatus.setRunState(TaskStatus.State.KILLED_UNCLEAN);
+        }
+      } else {
+        if (wasFailure) {
+          taskStatus.setRunState(TaskStatus.State.FAILED);
+        } else {
+          taskStatus.setRunState(TaskStatus.State.KILLED);
+        }
+      }
+    }
+    
+    /**
+     * The task has actually finished running.
+     */
+    public void taskFinished() {
+      long start = System.currentTimeMillis();
+
+      //
+      // Wait until task reports as done.  If it hasn't reported in,
+      // wait for a second and try again.
+      //
+      while (!done && (System.currentTimeMillis() - start < WAIT_FOR_DONE)) {
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException ie) {
+        }
+      }
+
+      //
+      // Change state to success or failure, depending on whether
+      // task was 'done' before terminating
+      //
+      boolean needCleanup = false;
+      synchronized (this) {
+        // Remove the task from MemoryManager, if the task SUCCEEDED or FAILED.
+        // KILLED tasks are removed in method kill(), because Kill 
+        // would result in launching a cleanup attempt before 
+        // TaskRunner returns; if remove happens here, it would remove
+        // wrong task from memory manager.
+        if (done || !wasKilled) {
+          removeFromMemoryManager(task.getTaskID());
+        }
+        if (!done) {
+          if (!wasKilled) {
+            taskFailures++;
+            setTaskFailState(true);
+            // call the script here for the failed tasks.
+            if (debugCommand != null) {
+              String taskStdout ="";
+              String taskStderr ="";
+              String taskSyslog ="";
+              String jobConf = task.getJobFile();
+              try {
+                Map<LogName, LogFileDetail> allFilesDetails = TaskLog
+                    .getAllLogsFileDetails(task.getTaskID(), task
+                        .isTaskCleanupTask());
+                // get task's stdout file
+                taskStdout =
+                    TaskLog.getRealTaskLogFilePath(
+                        allFilesDetails.get(LogName.STDOUT).location,
+                        LogName.STDOUT);
+                // get task's stderr file
+                taskStderr =
+                    TaskLog.getRealTaskLogFilePath(
+                        allFilesDetails.get(LogName.STDERR).location,
+                        LogName.STDERR);
+                // get task's syslog file
+                taskSyslog =
+                    TaskLog.getRealTaskLogFilePath(
+                        allFilesDetails.get(LogName.SYSLOG).location,
+                        LogName.SYSLOG);
+              } catch(IOException e){
+                LOG.warn("Exception finding task's stdout/err/syslog files");
+              }
+              File workDir = null;
+              try {
+                workDir =
+                    new File(lDirAlloc.getLocalPathToRead(
+                        TaskTracker.getLocalTaskDir(task.getUser(), task
+                            .getJobID().toString(), task.getTaskID()
+                            .toString(), task.isTaskCleanupTask())
+                            + Path.SEPARATOR + MRConstants.WORKDIR,
+                        localJobConf).toString());
+              } catch (IOException e) {
+                LOG.warn("Working Directory of the task " + task.getTaskID() +
+                                " doesnt exist. Caught exception " +
+                          StringUtils.stringifyException(e));
+              }
+              // Build the command  
+              File stdout = TaskLog.getTaskLogFile(task.getTaskID(), task
+                  .isTaskCleanupTask(), TaskLog.LogName.DEBUGOUT);
+              // add pipes program as argument if it exists.
+              String program ="";
+              String executable = Submitter.getExecutable(localJobConf);
+              if ( executable != null) {
+            	try {
+            	  program = new URI(executable).getFragment();
+            	} catch (URISyntaxException ur) {
+            	  LOG.warn("Problem in the URI fragment for pipes executable");
+            	}	  
+              }
+              String [] debug = debugCommand.split(" ");
+              Vector<String> vargs = new Vector<String>();
+              for (String component : debug) {
+                vargs.add(component);
+              }
+              vargs.add(taskStdout);
+              vargs.add(taskStderr);
+              vargs.add(taskSyslog);
+              vargs.add(jobConf);
+              vargs.add(program);
+              try {
+                List<String>  wrappedCommand = TaskLog.captureDebugOut
+                                                          (vargs, stdout);
+                // run the script.
+                try {
+                  runScript(wrappedCommand, workDir);
+                } catch (IOException ioe) {
+                  LOG.warn("runScript failed with: " + StringUtils.
+                                                      stringifyException(ioe));
+                }
+              } catch(IOException e) {
+                LOG.warn("Error in preparing wrapped debug command");
+              }
+
+              // add all lines of debug out to diagnostics
+              try {
+                int num = localJobConf.getInt("mapred.debug.out.lines", -1);
+                addDiagnostics(FileUtil.makeShellPath(stdout),num,"DEBUG OUT");
+              } catch(IOException ioe) {
+                LOG.warn("Exception in add diagnostics!");
+              }
+
+              // Debug-command is run. Do the post-debug-script-exit debug-logs
+              // processing. Truncate the logs.
+              JvmFinishedEvent jvmFinished = new JvmFinishedEvent(new JVMInfo(
+                  TaskLog.getAttemptDir(task.getTaskID(), task
+                      .isTaskCleanupTask()), Arrays.asList(task)));
+              getUserLogManager().addLogEvent(jvmFinished);
+            }
+          }
+          taskStatus.setProgress(0.0f);
+        }
+        this.taskStatus.setFinishTime(System.currentTimeMillis());
+        needCleanup = (taskStatus.getRunState() == TaskStatus.State.FAILED || 
+                taskStatus.getRunState() == TaskStatus.State.FAILED_UNCLEAN ||
+                taskStatus.getRunState() == TaskStatus.State.KILLED_UNCLEAN || 
+                taskStatus.getRunState() == TaskStatus.State.KILLED);
+      }
+
+      //
+      // If the task has failed, or if the task was killAndCleanup()'ed,
+      // we should clean up right away.  We only wait to cleanup
+      // if the task succeeded, and its results might be useful
+      // later on to downstream job processing.
+      //
+      if (needCleanup) {
+        removeTaskFromJob(task.getJobID(), this);
+      }
+      try {
+        cleanup(needCleanup);
+      } catch (IOException ie) {
+      }
+
+    }
+    
+
+    /**
+     * Runs the script given in args
+     * @param args script name followed by its argumnets
+     * @param dir current working directory.
+     * @throws IOException
+     */
+    public void runScript(List<String> args, File dir) throws IOException {
+      ShellCommandExecutor shexec = 
+              new ShellCommandExecutor(args.toArray(new String[0]), dir);
+      shexec.execute();
+      int exitCode = shexec.getExitCode();
+      if (exitCode != 0) {
+        throw new IOException("Task debug script exit with nonzero status of " 
+                              + exitCode + ".");
+      }
+    }
+
+    /**
+     * Add last 'num' lines of the given file to the diagnostics.
+     * if num =-1, all the lines of file are added to the diagnostics.
+     * @param file The file from which to collect diagnostics.
+     * @param num The number of lines to be sent to diagnostics.
+     * @param tag The tag is printed before the diagnostics are printed. 
+     */
+    public void addDiagnostics(String file, int num, String tag) {
+      RandomAccessFile rafile = null;
+      try {
+        rafile = new RandomAccessFile(file,"r");
+        int no_lines =0;
+        String line = null;
+        StringBuffer tail = new StringBuffer();
+        tail.append("\n-------------------- "+tag+"---------------------\n");
+        String[] lines = null;
+        if (num >0) {
+          lines = new String[num];
+        }
+        while ((line = rafile.readLine()) != null) {
+          no_lines++;
+          if (num >0) {
+            if (no_lines <= num) {
+              lines[no_lines-1] = line;
+            }
+            else { // shift them up
+              for (int i=0; i<num-1; ++i) {
+                lines[i] = lines[i+1];
+              }
+              lines[num-1] = line;
+            }
+          }
+          else if (num == -1) {
+            tail.append(line); 
+            tail.append("\n");
+          }
+        }
+        int n = no_lines > num ?num:no_lines;
+        if (num >0) {
+          for (int i=0;i<n;i++) {
+            tail.append(lines[i]);
+            tail.append("\n");
+          }
+        }
+        if(n!=0)
+          reportDiagnosticInfo(tail.toString());
+      } catch (FileNotFoundException fnfe){
+        LOG.warn("File "+file+ " not found");
+      } catch (IOException ioe){
+        LOG.warn("Error reading file "+file);
+      } finally {
+         try {
+           if (rafile != null) {
+             rafile.close();
+           }
+         } catch (IOException ioe) {
+           LOG.warn("Error closing file "+file);
+         }
+      }
+    }
+    
+    /**
+     * We no longer need anything from this task, as the job has
+     * finished.  If the task is still running, kill it and clean up.
+     * 
+     * @param wasFailure did the task fail, as opposed to was it killed by
+     *                   the framework
+     */
+    public void jobHasFinished(boolean wasFailure) throws IOException {
+      // Kill the task if it is still running
+      synchronized(this){
+        if (getRunState() == TaskStatus.State.RUNNING ||
+            getRunState() == TaskStatus.State.UNASSIGNED ||
+            getRunState() == TaskStatus.State.COMMIT_PENDING ||
+            isCleaningup()) {
+          try {
+            kill(wasFailure);
+          } catch (InterruptedException e) {
+            throw new IOException("Interrupted while killing " +
+                getTask().getTaskID(), e);
+          }
+        }
+      }
+      
+      // Cleanup on the finished task
+      cleanup(true);
+    }
+
+    /**
+     * Something went wrong and the task must be killed.
+     * @param wasFailure was it a failure (versus a kill request)?
+     * @throws InterruptedException 
+     */
+    public synchronized void kill(boolean wasFailure
+                                  ) throws IOException, InterruptedException {
+      if (taskStatus.getRunState() == TaskStatus.State.RUNNING ||
+          taskStatus.getRunState() == TaskStatus.State.COMMIT_PENDING ||
+          isCleaningup()) {
+        wasKilled = true;
+        if (wasFailure) {
+          taskFailures++;
+        }
+        // runner could be null if task-cleanup attempt is not localized yet
+        if (runner != null) {
+          runner.kill();
+        }
+        setTaskFailState(wasFailure);
+      } else if (taskStatus.getRunState() == TaskStatus.State.UNASSIGNED) {
+        if (wasFailure) {
+          taskFailures++;
+          taskStatus.setRunState(TaskStatus.State.FAILED);
+        } else {
+          taskStatus.setRunState(TaskStatus.State.KILLED);
+        }
+      }
+      taskStatus.setFinishTime(System.currentTimeMillis());
+      removeFromMemoryManager(task.getTaskID());
+      releaseSlot();
+      notifyTTAboutTaskCompletion();
+    }
+    
+    private synchronized void releaseSlot() {
+      if (slotTaken) {
+        if (launcher != null) {
+          launcher.addFreeSlots(task.getNumSlotsRequired());
+        }
+        slotTaken = false;
+      } else {
+        // wake up the launcher. it may be waiting to block slots for this task.
+        if (launcher != null) {
+          launcher.notifySlots();
+        }
+      }
+    }
+
+    /**
+     * The map output has been lost.
+     */
+    private synchronized void mapOutputLost(String failure
+                                           ) throws IOException {
+      if (taskStatus.getRunState() == TaskStatus.State.COMMIT_PENDING || 
+          taskStatus.getRunState() == TaskStatus.State.SUCCEEDED) {
+        // change status to failure
+        LOG.info("Reporting output lost:"+task.getTaskID());
+        taskStatus.setRunState(TaskStatus.State.FAILED);
+        taskStatus.setProgress(0.0f);
+        reportDiagnosticInfo("Map output lost, rescheduling: " + 
+                             failure);
+        runningTasks.put(task.getTaskID(), this);
+        mapTotal++;
+      } else {
+        LOG.warn("Output already reported lost:"+task.getTaskID());
+      }
+    }
+
+    /**
+     * We no longer need anything from this task.  Either the 
+     * controlling job is all done and the files have been copied
+     * away, or the task failed and we don't need the remains.
+     * Any calls to cleanup should not lock the tip first.
+     * cleanup does the right thing- updates tasks in Tasktracker
+     * by locking tasktracker first and then locks the tip.
+     * 
+     * if needCleanup is true, the whole task directory is cleaned up.
+     * otherwise the current working directory of the task 
+     * i.e. &lt;taskid&gt;/work is cleaned up.
+     */
+    void cleanup(boolean needCleanup) throws IOException {
+      TaskAttemptID taskId = task.getTaskID();
+      LOG.debug("Cleaning up " + taskId);
+
+
+      synchronized (TaskTracker.this) {
+        if (needCleanup) {
+          // see if tasks data structure is holding this tip.
+          // tasks could hold the tip for cleanup attempt, if cleanup attempt 
+          // got launched before this method.
+          if (tasks.get(taskId) == this) {
+            tasks.remove(taskId);
+          }
+        }
+        synchronized (this){
+          if (alwaysKeepTaskFiles ||
+              (taskStatus.getRunState() == TaskStatus.State.FAILED && 
+               keepFailedTaskFiles)) {
+            return;
+          }
+        }
+      }
+      synchronized (this) {
+        // localJobConf could be null if localization has not happened
+        // then no cleanup will be required.
+        if (localJobConf == null) {
+          return;
+        }
+        try {
+          removeTaskFiles(needCleanup);
+        } catch (Throwable ie) {
+          LOG.info("Error cleaning up task runner: "
+              + StringUtils.stringifyException(ie));
+        }
+      }
+    }
+
+    /**
+     * Some or all of the files from this task are no longer required. Remove
+     * them via CleanupQueue.
+     * 
+     * @param removeOutputs remove outputs as well as output
+     * @param taskId
+     * @throws IOException 
+     */
+    void removeTaskFiles(boolean removeOutputs) throws IOException {
+      if (localJobConf.getNumTasksToExecutePerJvm() == 1) {
+        String user = ugi.getShortUserName();
+        int userDirLen = TaskTracker.getUserDir(user).length();
+        String jobId = task.getJobID().toString();
+        String taskId = task.getTaskID().toString();
+        boolean cleanup = task.isTaskCleanupTask();
+        String taskDir;
+        if (!removeOutputs) {
+          taskDir = TaskTracker.getTaskWorkDir(user, jobId, taskId, cleanup);
+        } else {
+          taskDir = TaskTracker.getLocalTaskDir(user, jobId, taskId, cleanup);
+        }
+        PathDeletionContext item =
+          new TaskController.DeletionContext(taskController, false, user,
+                                             taskDir.substring(userDirLen));          
+        directoryCleanupThread.addToQueue(item);
+      }
+    }
+        
+    @Override
+    public boolean equals(Object obj) {
+      return (obj instanceof TaskInProgress) &&
+        task.getTaskID().equals
+        (((TaskInProgress) obj).getTask().getTaskID());
+    }
+        
+    @Override
+    public int hashCode() {
+      return task.getTaskID().hashCode();
+    }
+  }
+ 
+  private void validateJVM(TaskInProgress tip, JvmContext jvmContext, TaskAttemptID taskid) throws IOException {
+    if (jvmContext == null) {
+      LOG.warn("Null jvmContext. Cannot verify Jvm. validateJvm throwing exception");
+      throw new IOException("JvmValidate Failed. JvmContext is null - cannot validate JVM");
+    }
+    if (!jvmManager.validateTipToJvm(tip, jvmContext.jvmId)) {
+      throw new IOException("JvmValidate Failed. Ignoring request from task: " + taskid + ", with JvmId: " + jvmContext.jvmId);
+    }
+  }
+
+  /**
+   * Check that the current UGI is the JVM authorized to report
+   * for this particular job.
+   *
+   * @throws IOException for unauthorized access
+   */
+  private void ensureAuthorizedJVM(org.apache.hadoop.mapreduce.JobID jobId)
+  throws IOException {
+    String currentJobId = 
+      UserGroupInformation.getCurrentUser().getUserName();
+    if (!currentJobId.equals(jobId.toString())) {
+      throw new IOException ("JVM with " + currentJobId + 
+          " is not authorized for " + jobId);
+    }
+  }
+
+    
+  // ///////////////////////////////////////////////////////////////
+  // TaskUmbilicalProtocol
+  /////////////////////////////////////////////////////////////////
+
+  /**
+   * Called upon startup by the child process, to fetch Task data.
+   */
+  public synchronized JvmTask getTask(JvmContext context) 
+  throws IOException {
+    ensureAuthorizedJVM(context.jvmId.getJobId());
+    JVMId jvmId = context.jvmId;
+    LOG.debug("JVM with ID : " + jvmId + " asked for a task");
+    // save pid of task JVM sent by child
+    jvmManager.setPidToJvm(jvmId, context.pid);
+    if (!jvmManager.isJvmKnown(jvmId)) {
+      LOG.info("Killing unknown JVM " + jvmId);
+      return new JvmTask(null, true);
+    }
+    RunningJob rjob = runningJobs.get(jvmId.getJobId());
+    if (rjob == null) { //kill the JVM since the job is dead
+      LOG.info("Killing JVM " + jvmId + " since job " + jvmId.getJobId() +
+               " is dead");
+      try {
+        jvmManager.killJvm(jvmId);
+      } catch (InterruptedException e) {
+        LOG.warn("Failed to kill " + jvmId, e);
+      }
+      return new JvmTask(null, true);
+    }
+    TaskInProgress tip = jvmManager.getTaskForJvm(jvmId);
+    if (tip == null) {
+      return new JvmTask(null, false);
+    }
+    if (tasks.get(tip.getTask().getTaskID()) != null) { //is task still present
+      LOG.info("JVM with ID: " + jvmId + " given task: " + 
+          tip.getTask().getTaskID());
+      return new JvmTask(tip.getTask(), false);
+    } else {
+      LOG.info("Killing JVM with ID: " + jvmId + " since scheduled task: " + 
+          tip.getTask().getTaskID() + " is " + tip.taskStatus.getRunState());
+      return new JvmTask(null, true);
+    }
+  }
+
+  /**
+   * Called periodically to report Task progress, from 0.0 to 1.0.
+   */
+  public synchronized boolean statusUpdate(TaskAttemptID taskid, 
+                                              TaskStatus taskStatus,
+                                              JvmContext jvmContext)
+  throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      try {
+        validateJVM(tip, jvmContext, taskid);
+      } catch (IOException ie) {
+        LOG.warn("Failed validating JVM", ie);
+        return false;
+      }
+      tip.reportProgress(taskStatus);
+      return true;
+    } else {
+      LOG.warn("Progress from unknown child task: "+taskid);
+      return false;
+    }
+  }
+
+  /**
+   * Called when the task dies before completion, and we want to report back
+   * diagnostic info
+   */
+  public synchronized void reportDiagnosticInfo(TaskAttemptID taskid,
+      String info, JvmContext jvmContext) throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskid);
+      tip.reportDiagnosticInfo(info);
+    } else {
+      LOG.warn("Error from unknown child task: "+taskid+". Ignored.");
+    }
+  }
+
+  /**
+   * Same as reportDiagnosticInfo but does not authorize caller. This is used
+   * internally within MapReduce, whereas reportDiagonsticInfo may be called
+   * via RPC.
+   */
+  synchronized void internalReportDiagnosticInfo(TaskAttemptID taskid, String info) throws IOException {
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      tip.reportDiagnosticInfo(info);
+    } else {
+      LOG.warn("Error from unknown child task: "+taskid+". Ignored.");
+    }
+  }
+  
+  public synchronized void reportNextRecordRange(TaskAttemptID taskid, 
+      SortedRanges.Range range, JvmContext jvmContext) throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskid);
+      tip.reportNextRecordRange(range);
+    } else {
+      LOG.warn("reportNextRecordRange from unknown child task: "+taskid+". " +
+      		"Ignored.");
+    }
+  }
+
+  /** Child checking to see if we're alive.  Normally does nothing.*/
+  public synchronized boolean ping(TaskAttemptID taskid, JvmContext jvmContext)
+      throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskid);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  /**
+   * Task is reporting that it is in commit_pending
+   * and it is waiting for the commit Response
+   */
+  public synchronized void commitPending(TaskAttemptID taskid,
+                                         TaskStatus taskStatus,
+                                         JvmContext jvmContext) 
+  throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    LOG.info("Task " + taskid + " is in commit-pending," +"" +
+             " task state:" +taskStatus.getRunState());
+    // validateJVM is done in statusUpdate
+    if (!statusUpdate(taskid, taskStatus, jvmContext)) {
+      throw new IOException("Task not found for taskid: " + taskid);
+    }
+    reportTaskFinished(taskid, true);
+  }
+  
+  /**
+   * Child checking whether it can commit 
+   */
+  public synchronized boolean canCommit(TaskAttemptID taskid,
+      JvmContext jvmContext) throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    TaskInProgress tip = tasks.get(taskid);
+    validateJVM(tip, jvmContext, taskid);
+    return commitResponses.contains(taskid); // don't remove it now
+  }
+  
+  /**
+   * The task is done.
+   */
+  public synchronized void done(TaskAttemptID taskid, JvmContext jvmContext) 
+  throws IOException {
+    ensureAuthorizedJVM(taskid.getJobID());
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskid);
+      commitResponses.remove(taskid);
+      tip.reportDone();
+    } else {
+      LOG.warn("Unknown child task done: "+taskid+". Ignored.");
+    }
+  }
+
+
+  /** 
+   * A reduce-task failed to shuffle the map-outputs. Kill the task.
+   */  
+  public synchronized void shuffleError(TaskAttemptID taskId, String message, JvmContext jvmContext) 
+  throws IOException { 
+    ensureAuthorizedJVM(taskId.getJobID());
+    TaskInProgress tip = runningTasks.get(taskId);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskId);
+      LOG.fatal("Task: " + taskId + " - Killed due to Shuffle Failure: "
+          + message);
+      tip.reportDiagnosticInfo("Shuffle Error: " + message);
+      purgeTask(tip, true);
+    } else {
+      LOG.warn("Unknown child task shuffleError: " + taskId + ". Ignored.");
+    }
+  }
+
+  /** 
+   * A child task had a local filesystem error. Kill the task.
+   */  
+  public synchronized void fsError(TaskAttemptID taskId, String message,
+      JvmContext jvmContext) throws IOException {
+    ensureAuthorizedJVM(taskId.getJobID());
+    TaskInProgress tip = runningTasks.get(taskId);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskId);
+      LOG.fatal("Task: " + taskId + " - Killed due to FSError: " + message);
+      tip.reportDiagnosticInfo("FSError: " + message);
+      purgeTask(tip, true);
+    } else {
+      LOG.warn("Unknown child task fsError: "+taskId+". Ignored.");
+    }
+  }
+
+  /**
+   * Version of fsError() that does not do authorization checks, called by
+   * the TaskRunner.
+   */
+  synchronized void internalFsError(TaskAttemptID taskId, String message)
+  throws IOException {
+    LOG.fatal("Task: " + taskId + " - Killed due to FSError: " + message);
+    TaskInProgress tip = runningTasks.get(taskId);
+    tip.reportDiagnosticInfo("FSError: " + message);
+    purgeTask(tip, true);
+  }
+
+  /** 
+   * A child task had a fatal error. Kill the task.
+   */  
+  public synchronized void fatalError(TaskAttemptID taskId, String msg,
+      JvmContext jvmContext) throws IOException {
+    ensureAuthorizedJVM(taskId.getJobID());
+    TaskInProgress tip = runningTasks.get(taskId);
+    if (tip != null) {
+      validateJVM(tip, jvmContext, taskId);
+      LOG.fatal("Task: " + taskId + " - Killed : " + msg);
+      tip.reportDiagnosticInfo("Error: " + msg);
+      purgeTask(tip, true);
+    } else {
+      LOG.warn("Unknown child task fatalError: "+taskId+". Ignored.");
+    }
+  }
+
+  public synchronized MapTaskCompletionEventsUpdate getMapCompletionEvents(
+      JobID jobId, int fromEventId, int maxLocs, TaskAttemptID id,
+      JvmContext jvmContext) throws IOException {
+    TaskInProgress tip = runningTasks.get(id);
+    if (tip == null) {
+      throw new IOException("Unknown task; " + id
+          + ". Ignoring getMapCompletionEvents Request");
+    }
+    validateJVM(tip, jvmContext, id);
+    ensureAuthorizedJVM(jobId);
+    TaskCompletionEvent[]mapEvents = TaskCompletionEvent.EMPTY_ARRAY;
+    synchronized (shouldReset) {
+      if (shouldReset.remove(id)) {
+        return new MapTaskCompletionEventsUpdate(mapEvents, true);
+      }
+    }
+    RunningJob rjob;
+    synchronized (runningJobs) {
+      rjob = runningJobs.get(jobId);          
+      if (rjob != null) {
+        synchronized (rjob) {
+          FetchStatus f = rjob.getFetchStatus();
+          if (f != null) {
+            mapEvents = f.getMapEvents(fromEventId, maxLocs);
+          }
+        }
+      }
+    }
+    return new MapTaskCompletionEventsUpdate(mapEvents, false);
+  }
+    
+  /////////////////////////////////////////////////////
+  //  Called by TaskTracker thread after task process ends
+  /////////////////////////////////////////////////////
+  /**
+   * The task is no longer running.  It may not have completed successfully
+   */
+  void reportTaskFinished(TaskAttemptID taskid, boolean commitPending) {
+    TaskInProgress tip;
+    synchronized (this) {
+      tip = tasks.get(taskid);
+    }
+    if (tip != null) {
+      tip.reportTaskFinished(commitPending);
+    } else {
+      LOG.warn("Unknown child task finished: "+taskid+". Ignored.");
+    }
+  }
+  
+
+  /**
+   * A completed map task's output has been lost.
+   */
+  public synchronized void mapOutputLost(TaskAttemptID taskid,
+                                         String errorMsg) throws IOException {
+    TaskInProgress tip = tasks.get(taskid);
+    if (tip != null) {
+      tip.mapOutputLost(errorMsg);
+    } else {
+      LOG.warn("Unknown child with bad map output: "+taskid+". Ignored.");
+    }
+  }
+    
+  /**
+   *  The datastructure for initializing a job
+   */
+  static class RunningJob{
+    private JobID jobid; 
+    private JobConf jobConf;
+    private Path localizedJobConf;
+    // keep this for later use
+    volatile Set<TaskInProgress> tasks;
+    //the 'localizing' and 'localized' fields have the following
+    //state transitions (first entry is for 'localizing')
+    //{false,false} -> {true,false} -> {false,true}
+    volatile boolean localized;
+    boolean localizing;
+    boolean keepJobFiles;
+    UserGroupInformation ugi;
+    FetchStatus f;
+    TaskDistributedCacheManager distCacheMgr;
+    
+    RunningJob(JobID jobid) {
+      this.jobid = jobid;
+      localized = false;
+      localizing = false;
+      tasks = new HashSet<TaskInProgress>();
+      keepJobFiles = false;
+    }
+      
+    JobID getJobID() {
+      return jobid;
+    }
+      
+    UserGroupInformation getUGI() {
+      return ugi;
+    }
+
+    void setFetchStatus(FetchStatus f) {
+      this.f = f;
+    }
+      
+    FetchStatus getFetchStatus() {
+      return f;
+    }
+
+    JobConf getJobConf() {
+      return jobConf;
+    }
+  }
+
+  /**
+   * Get the name for this task tracker.
+   * @return the string like "tracker_mymachine:50010"
+   */
+  String getName() {
+    return taskTrackerName;
+  }
+    
+  private synchronized List<TaskStatus> cloneAndResetRunningTaskStatuses(
+                                          boolean sendCounters) {
+    List<TaskStatus> result = new ArrayList<TaskStatus>(runningTasks.size());
+    for(TaskInProgress tip: runningTasks.values()) {
+      TaskStatus status = tip.getStatus();
+      status.setIncludeAllCounters(sendCounters);
+      // send counters for finished or failed tasks and commit pending tasks
+      if (status.getRunState() != TaskStatus.State.RUNNING) {
+        status.setIncludeAllCounters(true);
+      }
+      result.add((TaskStatus)status.clone());
+      status.clearStatus();
+    }
+    return result;
+  }
+  /**
+   * Get the list of tasks that will be reported back to the 
+   * job tracker in the next heartbeat cycle.
+   * @return a copy of the list of TaskStatus objects
+   */
+  synchronized List<TaskStatus> getRunningTaskStatuses() {
+    List<TaskStatus> result = new ArrayList<TaskStatus>(runningTasks.size());
+    for(TaskInProgress tip: runningTasks.values()) {
+      result.add(tip.getStatus());
+    }
+    return result;
+  }
+
+  /**
+   * Get the list of stored tasks on this task tracker.
+   * @return
+   */
+  synchronized List<TaskStatus> getNonRunningTasks() {
+    List<TaskStatus> result = new ArrayList<TaskStatus>(tasks.size());
+    for(Map.Entry<TaskAttemptID, TaskInProgress> task: tasks.entrySet()) {
+      if (!runningTasks.containsKey(task.getKey())) {
+        result.add(task.getValue().getStatus());
+      }
+    }
+    return result;
+  }
+
+
+  /**
+   * Get the list of tasks from running jobs on this task tracker.
+   * @return a copy of the list of TaskStatus objects
+   */
+  synchronized List<TaskStatus> getTasksFromRunningJobs() {
+    List<TaskStatus> result = new ArrayList<TaskStatus>(tasks.size());
+    for (Map.Entry <JobID, RunningJob> item : runningJobs.entrySet()) {
+      RunningJob rjob = item.getValue();
+      synchronized (rjob) {
+        for (TaskInProgress tip : rjob.tasks) {
+          result.add(tip.getStatus());
+        }
+      }
+    }
+    return result;
+  }
+  
+  /**
+   * Get the default job conf for this tracker.
+   */
+  JobConf getJobConf() {
+    return fConf;
+  }
+    
+  /**
+   * Is this task tracker idle?
+   * @return has this task tracker finished and cleaned up all of its tasks?
+   */
+  public synchronized boolean isIdle() {
+    return tasks.isEmpty() && tasksToCleanup.isEmpty();
+  }
+    
+  /**
+   * Start the TaskTracker, point toward the indicated JobTracker
+   */
+  public static void main(String argv[]) throws Exception {
+    StringUtils.startupShutdownMessage(TaskTracker.class, argv, LOG);
+    if (argv.length != 0) {
+      System.out.println("usage: TaskTracker");
+      System.exit(-1);
+    }
+    try {
+      JobConf conf=new JobConf();
+      // enable the server to track time spent waiting on locks
+      ReflectionUtils.setContentionTracing
+        (conf.getBoolean("tasktracker.contention.tracking", false));
+      TaskTracker tt = new TaskTracker(conf);
+      MBeanUtil.registerMBean("TaskTracker", "TaskTrackerInfo", tt);
+      tt.run();
+    } catch (Throwable e) {
+      LOG.error("Can not start task tracker because "+
+                StringUtils.stringifyException(e));
+      System.exit(-1);
+    }
+  }
+  
+  static class LRUCache<K, V> {
+    private int cacheSize;
+    private LinkedHashMap<K, V> map;
+	
+    public LRUCache(int cacheSize) {
+      this.cacheSize = cacheSize;
+      this.map = new LinkedHashMap<K, V>(cacheSize, 0.75f, true) {
+          protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
+	    return size() > LRUCache.this.cacheSize;
+	  }
+      };
+    }
+	
+    public synchronized V get(K key) {
+      return map.get(key);
+    }
+	
+    public synchronized void put(K key, V value) {
+      map.put(key, value);
+    }
+	
+    public synchronized int size() {
+      return map.size();
+    }
+	
+    public Iterator<Entry<K, V>> getIterator() {
+      return new LinkedList<Entry<K, V>>(map.entrySet()).iterator();
+    }
+   
+    public synchronized void clear() {
+      map.clear();
+    }
+  }
+
+  /**
+   * This class is used in TaskTracker's Jetty to serve the map outputs
+   * to other nodes.
+   */
+  public static class MapOutputServlet extends HttpServlet {
+    private static final long serialVersionUID = 1L;
+    private static final int MAX_BYTES_TO_READ = 64 * 1024;
+    // work around jetty internal buffering issues
+    private static final int RESPONSE_BUFFER_SIZE = MAX_BYTES_TO_READ + 16;
+    private static LRUCache<String, Path> fileCache = new LRUCache<String, Path>(FILE_CACHE_SIZE);
+    private static LRUCache<String, Path> fileIndexCache = new LRUCache<String, Path>(FILE_CACHE_SIZE);
+
+
+    private boolean shuffleSsl = false;
+
+    @Override
+    public void init() throws ServletException {
+      super.init();
+      shuffleSsl =
+        (Boolean) getServletContext().getAttribute(
+          JobTracker.SHUFFLE_SSL_ENABLED_KEY);
+    }
+
+    @Override
+    public void doGet(HttpServletRequest request, 
+                      HttpServletResponse response
+                      ) throws ServletException, IOException {
+      if (shuffleSsl && !request.isSecure()) {
+        response.sendError(HttpServletResponse.SC_FORBIDDEN,
+          "Encrypted Shuffle is enabled, shuffle is only served over HTTPS");
+        return;
+      }
+      String mapId = request.getParameter("map");
+      String reduceId = request.getParameter("reduce");
+      String jobId = request.getParameter("job");
+
+      if (jobId == null) {
+        throw new IOException("job parameter is required");
+      }
+
+      if (mapId == null || reduceId == null) {
+        throw new IOException("map and reduce parameters are required");
+      }
+      ServletContext context = getServletContext();
+      int reduce = Integer.parseInt(reduceId);
+      byte[] buffer = new byte[MAX_BYTES_TO_READ];
+      // true iff IOException was caused by attempt to access input
+      boolean isInputException = true;
+      OutputStream outStream = null;
+      FileInputStream mapOutputIn = null;
+ 
+      long totalRead = 0;
+      ShuffleServerMetrics shuffleMetrics =
+        (ShuffleServerMetrics) context.getAttribute("shuffleServerMetrics");
+      TaskTracker tracker = 
+        (TaskTracker) context.getAttribute("task.tracker");
+      String exceptionStackRegex =
+        (String) context.getAttribute("exceptionStackRegex");
+      String exceptionMsgRegex =
+        (String) context.getAttribute("exceptionMsgRegex");
+
+      verifyRequest(request, response, tracker, jobId);
+
+      long startTime = 0;
+      try {
+        shuffleMetrics.serverHandlerBusy();
+        if(ClientTraceLog.isInfoEnabled())
+          startTime = System.nanoTime();
+        response.setContentType("application/octet-stream");
+        outStream = response.getOutputStream();
+        JobConf conf = (JobConf) context.getAttribute("conf");
+        LocalDirAllocator lDirAlloc = 
+          (LocalDirAllocator)context.getAttribute("localDirAllocator");
+        FileSystem rfs = ((LocalFileSystem)
+            context.getAttribute("local.file.system")).getRaw();
+
+      String userName = null;
+      String runAsUserName = null;
+      synchronized (tracker.runningJobs) {
+        RunningJob rjob = tracker.runningJobs.get(JobID.forName(jobId));
+        if (rjob == null) {
+          throw new IOException("Unknown job " + jobId + "!!");
+        }
+        userName = rjob.jobConf.getUser();
+        runAsUserName = tracker.getTaskController().getRunAsUser(rjob.jobConf);
+      }
+      // Index file
+      String intermediateOutputDir = TaskTracker.getIntermediateOutputDir(userName, jobId, mapId);
+      String indexKey = intermediateOutputDir + "/file.out.index";
+      Path indexFileName = fileIndexCache.get(indexKey);
+      if (indexFileName == null) {
+        indexFileName = lDirAlloc.getLocalPathToRead(indexKey, conf);
+        fileIndexCache.put(indexKey, indexFileName);
+      }
+
+      // Map-output file
+      String fileKey = intermediateOutputDir + "/file.out";
+      Path mapOutputFileName = fileCache.get(fileKey);
+      if (mapOutputFileName == null) {
+        mapOutputFileName = lDirAlloc.getLocalPathToRead(fileKey, conf);
+        fileCache.put(fileKey, mapOutputFileName);
+      }
+
+        /**
+         * Read the index file to get the information about where
+         * the map-output for the given reducer is available. 
+         */
+        IndexRecord info = 
+          tracker.indexCache.getIndexInformation(mapId, reduce,indexFileName, 
+              runAsUserName);
+          
+        //set the custom "from-map-task" http header to the map task from which
+        //the map output data is being transferred
+        response.setHeader(FROM_MAP_TASK, mapId);
+        
+        //set the custom "Raw-Map-Output-Length" http header to 
+        //the raw (decompressed) length
+        response.setHeader(RAW_MAP_OUTPUT_LENGTH,
+            Long.toString(info.rawLength));
+
+        //set the custom "Map-Output-Length" http header to 
+        //the actual number of bytes being transferred
+        response.setHeader(MAP_OUTPUT_LENGTH,
+            Long.toString(info.partLength));
+
+        //set the custom "for-reduce-task" http header to the reduce task number
+        //for which this map output is being transferred
+        response.setHeader(FOR_REDUCE_TASK, Integer.toString(reduce));
+
+        response.setBufferSize(RESPONSE_BUFFER_SIZE);
+
+        /**
+         * Read the data from the sigle map-output file and
+         * send it to the reducer.
+         */
+        //open the map-output file
+        String filePath = mapOutputFileName.toUri().getPath();
+        mapOutputIn = SecureIOUtils.openForRead(
+            new File(filePath), runAsUserName, null);
+
+        // readahead if possible
+        ReadaheadRequest curReadahead = null;
+
+        //seek to the correct offset for the reduce
+        mapOutputIn.skip(info.startOffset);
+        long rem = info.partLength;
+        long offset = info.startOffset;
+        while (rem > 0) {
+          if (tracker.manageOsCacheInShuffle && tracker.readaheadPool != null) {
+            curReadahead = tracker.readaheadPool.readaheadStream(
+              filePath, mapOutputIn.getFD(),
+              offset, tracker.readaheadLength,
+              info.startOffset + info.partLength,
+              curReadahead);
+          }
+          int len =
+            mapOutputIn.read(buffer, 0, (int)Math.min(rem, MAX_BYTES_TO_READ));
+          if (len < 0) {
+            break;
+          }
+          rem -= len;
+          offset += len;
+          try {
+            shuffleMetrics.outputBytes(len);
+            outStream.write(buffer, 0, len);
+            outStream.flush();
+          } catch (IOException ie) {
+            isInputException = false;
+            throw ie;
+          }
+          totalRead += len;
+        }
+
+        if (curReadahead != null) {
+          curReadahead.cancel();
+        }
+        
+        // drop cache if possible
+        if (tracker.manageOsCacheInShuffle && info.partLength > 0) {
+          NativeIO.posixFadviseIfPossible(mapOutputIn.getFD(),
+              info.startOffset, info.partLength, NativeIO.POSIX_FADV_DONTNEED);
+        }
+
+        if (LOG.isDebugEnabled()) {
+          LOG.info("Sent out " + totalRead + " bytes for reduce: " + reduce + 
+                 " from map: " + mapId + " given " + info.partLength + "/" + 
+                 info.rawLength);
+        }
+      } catch (IOException ie) {
+        Log log = (Log) context.getAttribute("log");
+        String errorMsg = ("getMapOutput(" + mapId + "," + reduceId + 
+                           ") failed :\n"+
+                           StringUtils.stringifyException(ie));
+        log.warn(errorMsg);
+        checkException(ie, exceptionMsgRegex, exceptionStackRegex,
+            shuffleMetrics);
+        if (isInputException) {
+          tracker.mapOutputLost(TaskAttemptID.forName(mapId), errorMsg);
+        }
+        response.sendError(HttpServletResponse.SC_GONE, errorMsg);
+        shuffleMetrics.failedOutput();
+        throw ie;
+      } finally {
+        if (null != mapOutputIn) {
+          mapOutputIn.close();
+        }
+        final long endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;
+        shuffleMetrics.serverHandlerFree();
+        if (ClientTraceLog.isInfoEnabled()) {
+          ClientTraceLog.info(String.format(MR_CLIENTTRACE_FORMAT,
+                request.getLocalAddr() + ":" + request.getLocalPort(),
+                request.getRemoteAddr() + ":" + request.getRemotePort(),
+                totalRead, "MAPRED_SHUFFLE", mapId, endTime-startTime));
+        }
+      }
+      outStream.close();
+      shuffleMetrics.successOutput();
+    }
+    
+    protected void checkException(IOException ie, String exceptionMsgRegex,
+        String exceptionStackRegex, ShuffleServerMetrics shuffleMetrics) {
+      // parse exception to see if it looks like a regular expression you
+      // configure. If both msgRegex and StackRegex set then make sure both
+      // match, otherwise only the one set has to match.
+      if (exceptionMsgRegex != null) {
+        String msg = ie.getMessage();
+        if (msg == null || !msg.matches(exceptionMsgRegex)) {
+          return;
+        }
+      }
+      if (exceptionStackRegex != null
+          && !checkStackException(ie, exceptionStackRegex)) {
+        return;
+      }
+      shuffleMetrics.exceptionsCaught();
+    }
+
+    private boolean checkStackException(IOException ie,
+        String exceptionStackRegex) {
+      StackTraceElement[] stack = ie.getStackTrace();
+
+      for (StackTraceElement elem : stack) {
+        String stacktrace = elem.toString();
+        if (stacktrace.matches(exceptionStackRegex)) {
+          return true;
+        }
+      }
+      return false;
+    }
+
+    /**
+     * verify that request has correct HASH for the url
+     * and also add a field to reply header with hash of the HASH
+     * @param request
+     * @param response
+     * @param jt the job token
+     * @throws IOException
+     */
+    private void verifyRequest(HttpServletRequest request, 
+        HttpServletResponse response, TaskTracker tracker, String jobId) 
+    throws IOException {
+      SecretKey tokenSecret = tracker.getJobTokenSecretManager()
+          .retrieveTokenSecret(jobId);
+      // string to encrypt
+      String enc_str = SecureShuffleUtils.buildMsgFrom(request);
+      
+      // hash from the fetcher
+      String urlHashStr = request.getHeader(SecureShuffleUtils.HTTP_HEADER_URL_HASH);
+      if(urlHashStr == null) {
+        response.sendError(HttpServletResponse.SC_UNAUTHORIZED);
+        throw new IOException("fetcher cannot be authenticated " + 
+            request.getRemoteHost());
+      }
+      int len = urlHashStr.length();
+      LOG.debug("verifying request. enc_str="+enc_str+"; hash=..."+
+          urlHashStr.substring(len-len/2, len-1)); // half of the hash for debug
+
+      // verify - throws exception
+      try {
+        SecureShuffleUtils.verifyReply(urlHashStr, enc_str, tokenSecret);
+      } catch (IOException ioe) {
+        response.sendError(HttpServletResponse.SC_UNAUTHORIZED);
+        throw ioe;
+      }
+      
+      // verification passed - encode the reply
+      String reply = SecureShuffleUtils.generateHash(urlHashStr.getBytes(), tokenSecret);
+      response.addHeader(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH, reply);
+      
+      len = reply.length();
+      LOG.debug("Fetcher request verfied. enc_str="+enc_str+";reply="
+          +reply.substring(len-len/2, len-1));
+    }
+  }
+  
+
+  // get the full paths of the directory in all the local disks.
+  Path[] getLocalFiles(JobConf conf, String subdir) throws IOException{
+    String[] localDirs = conf.getLocalDirs();
+    Path[] paths = new Path[localDirs.length];
+    FileSystem localFs = FileSystem.getLocal(conf);
+    boolean subdirNeeded = (subdir != null) && (subdir.length() > 0);
+    for (int i = 0; i < localDirs.length; i++) {
+      paths[i] = (subdirNeeded) ? new Path(localDirs[i], subdir)
+                                : new Path(localDirs[i]);
+      paths[i] = paths[i].makeQualified(localFs);
+    }
+    return paths;
+  }
+
+  // only used by tests
+  FileSystem getLocalFileSystem(){
+    return localFs;
+  }
+
+  // only used by tests
+  void setLocalFileSystem(FileSystem fs){
+    localFs = (LocalFileSystem)fs;
+  }
+
+  int getMaxCurrentMapTasks() {
+    return maxMapSlots;
+  }
+  
+  int getMaxCurrentReduceTasks() {
+    return maxReduceSlots;
+  }
+
+  int getNumDirFailures() {
+    return localStorage.numFailures();
+  }
+
+  //called from unit test
+  synchronized void setMaxMapSlots(int mapSlots) {
+    maxMapSlots = mapSlots;
+  }
+
+  //called from unit test
+  synchronized void setMaxReduceSlots(int reduceSlots) {
+    maxReduceSlots = reduceSlots;
+  }
+
+  /**
+   * Is the TaskMemoryManager Enabled on this system?
+   * @return true if enabled, false otherwise.
+   */
+  public boolean isTaskMemoryManagerEnabled() {
+    return taskMemoryManagerEnabled;
+  }
+  
+  public TaskMemoryManagerThread getTaskMemoryManager() {
+    return taskMemoryManager;
+  }
+  
+  synchronized TaskInProgress getRunningTask(TaskAttemptID tid) {
+    return runningTasks.get(tid);
+  }
+
+  /**
+   * Normalize the negative values in configuration
+   * 
+   * @param val
+   * @return normalized val
+   */
+  private long normalizeMemoryConfigValue(long val) {
+    if (val < 0) {
+      val = JobConf.DISABLED_MEMORY_LIMIT;
+    }
+    return val;
+  }
+
+  /**
+   * Memory-related setup
+   */
+  private void initializeMemoryManagement() {
+
+    //handling @deprecated
+    if (fConf.get(MAPRED_TASKTRACKER_VMEM_RESERVED_PROPERTY) != null) {
+      LOG.warn(
+        JobConf.deprecatedString(
+          MAPRED_TASKTRACKER_VMEM_RESERVED_PROPERTY));
+    }
+
+    //handling @deprecated
+    if (fConf.get(MAPRED_TASKTRACKER_PMEM_RESERVED_PROPERTY) != null) {
+      LOG.warn(
+        JobConf.deprecatedString(
+          MAPRED_TASKTRACKER_PMEM_RESERVED_PROPERTY));
+    }
+
+    //handling @deprecated
+    if (fConf.get(JobConf.MAPRED_TASK_DEFAULT_MAXVMEM_PROPERTY) != null) {
+      LOG.warn(
+        JobConf.deprecatedString(
+          JobConf.MAPRED_TASK_DEFAULT_MAXVMEM_PROPERTY));
+    }
+
+    //handling @deprecated
+    if (fConf.get(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY) != null) {
+      LOG.warn(
+        JobConf.deprecatedString(
+          JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY));
+    }
+
+    // Use TT_RESOURCE_CALCULATOR_PLUGIN if it is configured.
+    Class<? extends MemoryCalculatorPlugin> clazz = 
+      fConf.getClass(MAPRED_TASKTRACKER_MEMORY_CALCULATOR_PLUGIN_PROPERTY, 
+                     null, MemoryCalculatorPlugin.class); 
+    MemoryCalculatorPlugin memoryCalculatorPlugin = 
+      (clazz == null 
+       ? null 
+       : MemoryCalculatorPlugin.getMemoryCalculatorPlugin(clazz, fConf)); 
+    if (memoryCalculatorPlugin != null || resourceCalculatorPlugin != null) {
+      totalVirtualMemoryOnTT = 
+        (memoryCalculatorPlugin == null 
+         ? resourceCalculatorPlugin.getVirtualMemorySize() 
+         : memoryCalculatorPlugin.getVirtualMemorySize());
+      if (totalVirtualMemoryOnTT <= 0) {
+        LOG.warn("TaskTracker's totalVmem could not be calculated. "
+                 + "Setting it to " + JobConf.DISABLED_MEMORY_LIMIT);
+        totalVirtualMemoryOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+      }
+      totalPhysicalMemoryOnTT = 
+        (memoryCalculatorPlugin == null 
+         ? resourceCalculatorPlugin.getPhysicalMemorySize() 
+         : memoryCalculatorPlugin.getPhysicalMemorySize());
+      if (totalPhysicalMemoryOnTT <= 0) {
+        LOG.warn("TaskTracker's totalPmem could not be calculated. "
+                 + "Setting it to " + JobConf.DISABLED_MEMORY_LIMIT);
+        totalPhysicalMemoryOnTT = JobConf.DISABLED_MEMORY_LIMIT;
+      }
+    }
+
+    mapSlotMemorySizeOnTT =
+        fConf.getLong(
+            JobTracker.MAPRED_CLUSTER_MAP_MEMORY_MB_PROPERTY,
+            JobConf.DISABLED_MEMORY_LIMIT);
+    reduceSlotSizeMemoryOnTT =
+        fConf.getLong(
+            JobTracker.MAPRED_CLUSTER_REDUCE_MEMORY_MB_PROPERTY,
+            JobConf.DISABLED_MEMORY_LIMIT);
+    totalMemoryAllottedForTasks =
+        maxMapSlots * mapSlotMemorySizeOnTT + maxReduceSlots
+            * reduceSlotSizeMemoryOnTT;
+    if (totalMemoryAllottedForTasks < 0) {
+      //adding check for the old keys which might be used by the administrator
+      //while configuration of the memory monitoring on TT
+      long memoryAllotedForSlot = fConf.normalizeMemoryConfigValue(
+          fConf.getLong(JobConf.MAPRED_TASK_DEFAULT_MAXVMEM_PROPERTY, 
+              JobConf.DISABLED_MEMORY_LIMIT));
+      long limitVmPerTask = fConf.normalizeMemoryConfigValue(
+          fConf.getLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
+              JobConf.DISABLED_MEMORY_LIMIT));
+      if(memoryAllotedForSlot == JobConf.DISABLED_MEMORY_LIMIT) {
+        totalMemoryAllottedForTasks = JobConf.DISABLED_MEMORY_LIMIT; 
+      } else {
+        if(memoryAllotedForSlot > limitVmPerTask) {
+          LOG.info("DefaultMaxVmPerTask is mis-configured. " +
+          		"It shouldn't be greater than task limits");
+          totalMemoryAllottedForTasks = JobConf.DISABLED_MEMORY_LIMIT;
+        } else {
+          totalMemoryAllottedForTasks = (maxMapSlots + 
+              maxReduceSlots) *  (memoryAllotedForSlot/(1024 * 1024));
+        }
+      }
+    }
+    if (totalMemoryAllottedForTasks > totalPhysicalMemoryOnTT) {
+      LOG.info("totalMemoryAllottedForTasks > totalPhysicalMemoryOnTT."
+          + " Thrashing might happen.");
+    } else if (totalMemoryAllottedForTasks > totalVirtualMemoryOnTT) {
+      LOG.info("totalMemoryAllottedForTasks > totalVirtualMemoryOnTT."
+          + " Thrashing might happen.");
+    }
+
+    reservedPhysicalMemoryOnTT =
+      fConf.getLong(TT_RESERVED_PHYSICALMEMORY_MB,
+                    JobConf.DISABLED_MEMORY_LIMIT);
+    reservedPhysicalMemoryOnTT =
+      reservedPhysicalMemoryOnTT == JobConf.DISABLED_MEMORY_LIMIT ?
+      JobConf.DISABLED_MEMORY_LIMIT :
+      reservedPhysicalMemoryOnTT * 1024 * 1024; // normalize to bytes
+
+    // start the taskMemoryManager thread only if enabled
+    setTaskMemoryManagerEnabledFlag();
+    if (isTaskMemoryManagerEnabled()) {
+      taskMemoryManager = new TaskMemoryManagerThread(this);
+      taskMemoryManager.setDaemon(true);
+      taskMemoryManager.start();
+    }
+  }
+
+  void setTaskMemoryManagerEnabledFlag() {
+    if (!ProcfsBasedProcessTree.isAvailable()) {
+      LOG.info("ProcessTree implementation is missing on this system. "
+          + "TaskMemoryManager is disabled.");
+      taskMemoryManagerEnabled = false;
+      return;
+    }
+
+    if (reservedPhysicalMemoryOnTT == JobConf.DISABLED_MEMORY_LIMIT
+        && totalMemoryAllottedForTasks == JobConf.DISABLED_MEMORY_LIMIT) {
+      taskMemoryManagerEnabled = false;
+      LOG.warn("TaskTracker's totalMemoryAllottedForTasks is -1 and " +
+               "reserved physical memory is not configured. " +
+               "TaskMemoryManager is disabled.");
+      return;
+    }
+
+    taskMemoryManagerEnabled = true;
+  }
+
+  /**
+   * Clean-up the task that TaskMemoryMangerThread requests to do so.
+   * @param tid
+   * @param wasFailure mark the task as failed or killed. 'failed' if true,
+   *          'killed' otherwise
+   * @param diagnosticMsg
+   */
+  synchronized void cleanUpOverMemoryTask(TaskAttemptID tid, boolean wasFailure,
+      String diagnosticMsg) {
+    TaskInProgress tip = runningTasks.get(tid);
+    if (tip != null) {
+      tip.reportDiagnosticInfo(diagnosticMsg);
+      try {
+        purgeTask(tip, wasFailure); // Marking it as failed/killed.
+      } catch (IOException ioe) {
+        LOG.warn("Couldn't purge the task of " + tid + ". Error : " + ioe);
+      }
+    }
+  }
+  
+  /**
+   * Wrapper method used by TaskTracker to check if {@link  NodeHealthCheckerService}
+   * can be started
+   * @param conf configuration used to check if service can be started
+   * @return true if service can be started
+   */
+  private boolean shouldStartHealthMonitor(Configuration conf) {
+    return NodeHealthCheckerService.shouldRun(conf);
+  }
+  
+  /**
+   * Wrapper method used to start {@link NodeHealthCheckerService} for 
+   * Task Tracker
+   * @param conf Configuration used by the service.
+   */
+  private void startHealthMonitor(Configuration conf) {
+    healthChecker = new NodeHealthCheckerService(conf);
+    healthChecker.start();
+  }
+  
+  TrackerDistributedCacheManager getTrackerDistributedCacheManager() {
+    return distributedCacheManager;
+  }
+
+    /**
+     * Download the job-token file from the FS and save on local fs.
+     * @param user
+     * @param jobId
+     * @return the local file system path of the downloaded file.
+     * @throws IOException
+     */
+  private String localizeJobTokenFile(String user, JobID jobId)
+        throws IOException {
+      // check if the tokenJob file is there..
+      Path skPath = new Path(systemDirectory, 
+          jobId.toString()+"/"+TokenCache.JOB_TOKEN_HDFS_FILE);
+      
+      FileStatus status = null;
+      long jobTokenSize = -1;
+      status = systemFS.getFileStatus(skPath); //throws FileNotFoundException
+      jobTokenSize = status.getLen();
+      
+      Path localJobTokenFile =
+          lDirAlloc.getLocalPathForWrite(getPrivateDirJobTokenFile(user, 
+              jobId.toString()), jobTokenSize, fConf);
+    
+      String localJobTokenFileStr = localJobTokenFile.toUri().getPath();
+      if(LOG.isDebugEnabled())
+        LOG.debug("localizingJobTokenFile from sd="+skPath.toUri().getPath() + 
+            " to " + localJobTokenFileStr);
+      
+      // Download job_token
+      systemFS.copyToLocalFile(skPath, localJobTokenFile);      
+      return localJobTokenFileStr;
+    }
+
+    JobACLsManager getJobACLsManager() {
+      return aclsManager.getJobACLsManager();
+    }
+    
+    ACLsManager getACLsManager() {
+      return aclsManager;
+    }
+
+  // Begin MXBean implementation
+  @Override
+  public String getHostname() {
+    return localHostname;
+  }
+
+  @Override
+  public String getVersion() {
+    return VersionInfo.getVersion() +", r"+ VersionInfo.getRevision();
+  }
+
+  @Override
+  public String getConfigVersion() {
+    return originalConf.get(CONF_VERSION_KEY, CONF_VERSION_DEFAULT);
+  }
+
+  @Override
+  public String getJobTrackerUrl() {
+    return originalConf.get("mapred.job.tracker");
+  }
+
+  @Override
+  public int getRpcPort() {
+    return taskReportAddress.getPort();
+  }
+
+  @Override
+  public int getHttpPort() {
+    return httpPort;
+  }
+
+  @Override
+  public boolean isHealthy() {
+    boolean healthy = true;
+    TaskTrackerHealthStatus hs = new TaskTrackerHealthStatus();
+    if (healthChecker != null) {
+      healthChecker.setHealthStatus(hs);
+      healthy = hs.isNodeHealthy();
+    }    
+    return healthy;
+  }
+
+  @Override
+  public String getTasksInfoJson() {
+    return getTasksInfo().toJson();
+  }
+
+  InfoMap getTasksInfo() {
+    InfoMap map = new InfoMap();
+    int failed = 0;
+    int commitPending = 0;
+    for (TaskStatus st : getNonRunningTasks()) {
+      if (st.getRunState() == TaskStatus.State.FAILED ||
+          st.getRunState() == TaskStatus.State.FAILED_UNCLEAN) {
+        ++failed;
+      } else if (st.getRunState() == TaskStatus.State.COMMIT_PENDING) {
+        ++commitPending;
+      }
+    }
+    map.put("running", runningTasks.size());
+    map.put("failed", failed);
+    map.put("commit_pending", commitPending);
+    return map;
+  }
+  // End MXBean implemenation
+
+  @Override
+  public void 
+  updatePrivateDistributedCacheSizes(org.apache.hadoop.mapreduce.JobID jobId,
+                                     long[] sizes
+                                     ) throws IOException {
+    ensureAuthorizedJVM(jobId);
+    distributedCacheManager.setArchiveSizes(jobId, sizes);
+  }
+
+}
diff -rupN ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapreduce/JobContext.java
--- ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java	2012-11-02 02:56:46.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapreduce/JobContext.java	2013-09-15 07:43:01.421928000 +0200
@@ -75,6 +75,9 @@ public interface JobContext {
    */
   public Configuration getConfiguration();
   
+  public static final String SHUFFLE_CONSUMER_PLUGIN_ATTR =
+    "mapreduce.job.reduce.shuffle.consumer.plugin.class";
+
   /**
    * Get credentials for the job.
    * @return credentials for the job
diff -rupN ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java.orig ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapreduce/JobContext.java.orig
--- ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java.orig	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/mapred/org/apache/hadoop/mapreduce/JobContext.java.orig	2012-11-02 02:56:46.000000000 +0200
@@ -0,0 +1,324 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+import java.net.URI;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * A read-only view of the job that is provided to the tasks while they
+ * are running.
+ */
+public interface JobContext {
+  // Put all of the attribute names in here so that Job and JobContext are
+  // consistent.
+  public static final String INPUT_FORMAT_CLASS_ATTR = 
+    "mapreduce.inputformat.class";
+  public static final String MAP_CLASS_ATTR = "mapreduce.map.class";
+  public static final String COMBINE_CLASS_ATTR = "mapreduce.combine.class";
+  public static final String REDUCE_CLASS_ATTR = "mapreduce.reduce.class";
+  public static final String OUTPUT_FORMAT_CLASS_ATTR = 
+    "mapreduce.outputformat.class";
+  public static final String PARTITIONER_CLASS_ATTR = 
+    "mapreduce.partitioner.class";
+  public static final String JAR_UNPACK_PATTERN = "mapreduce.job.jar.unpack.pattern";
+
+  public static final String JOB_NAMENODES = "mapreduce.job.hdfs-servers";
+
+  public static final String JOB_ACL_VIEW_JOB = "mapreduce.job.acl-view-job";
+  public static final String JOB_ACL_MODIFY_JOB =
+    "mapreduce.job.acl-modify-job";
+
+  public static final String CACHE_FILE_VISIBILITIES = 
+    "mapreduce.job.cache.files.visibilities";
+  public static final String CACHE_ARCHIVES_VISIBILITIES = 
+    "mapreduce.job.cache.archives.visibilities";
+  
+  public static final String JOB_CANCEL_DELEGATION_TOKEN = 
+    "mapreduce.job.complete.cancel.delegation.tokens";
+  public static final String USER_LOG_RETAIN_HOURS = 
+    "mapred.userlog.retain.hours";
+  public static final String MAPREDUCE_TASK_CLASSPATH_PRECEDENCE = 
+    "mapreduce.task.classpath.user.precedence";
+  
+  public static final String MAP_MEMORY_PHYSICAL_MB =
+    "mapreduce.map.memory.physical.mb";
+  public static final String REDUCE_MEMORY_PHYSICAL_MB = 
+     "mapreduce.reduce.memory.physical.mb";
+
+  /**
+   * Return the configuration for the job.
+   * @return the shared configuration object
+   */
+  public Configuration getConfiguration();
+  
+  /**
+   * Get credentials for the job.
+   * @return credentials for the job
+   */
+  public Credentials getCredentials();
+
+  /**
+   * Get the unique ID for the job.
+   * @return the object with the job id
+   */
+  public JobID getJobID();
+  
+  /**
+   * Get configured the number of reduce tasks for this job. Defaults to 
+   * <code>1</code>.
+   * @return the number of reduce tasks for this job.
+   */
+  public int getNumReduceTasks();
+  
+  /**
+   * Get the current working directory for the default file system.
+   * 
+   * @return the directory name.
+   */
+  public Path getWorkingDirectory() throws IOException;
+  
+  /**
+   * Get the key class for the job output data.
+   * @return the key class for the job output data.
+   */
+  public Class<?> getOutputKeyClass();
+  
+  /**
+   * Get the value class for job outputs.
+   * @return the value class for job outputs.
+   */
+  public Class<?> getOutputValueClass();
+
+  /**
+   * Get the key class for the map output data. If it is not set, use the
+   * (final) output key class. This allows the map output key class to be
+   * different than the final output key class.
+   * @return the map output key class.
+   */
+  public Class<?> getMapOutputKeyClass();
+
+  /**
+   * Get the value class for the map output data. If it is not set, use the
+   * (final) output value class This allows the map output value class to be
+   * different than the final output value class.
+   *  
+   * @return the map output value class.
+   */
+  public Class<?> getMapOutputValueClass();
+
+  /**
+   * Get the user-specified job name. This is only used to identify the 
+   * job to the user.
+   * 
+   * @return the job's name, defaulting to "".
+   */
+  public String getJobName();
+  
+  /**
+   * Get the boolean value for the property that specifies which classpath
+   * takes precedence when tasks are launched. True - user's classes takes
+   * precedence. False - system's classes takes precedence.
+   * @return true if user's classes should take precedence
+   */
+  public boolean userClassesTakesPrecedence();
+
+  /**
+   * Get the {@link InputFormat} class for the job.
+   * 
+   * @return the {@link InputFormat} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends InputFormat<?,?>> getInputFormatClass() 
+     throws ClassNotFoundException;
+
+  /**
+   * Get the {@link Mapper} class for the job.
+   * 
+   * @return the {@link Mapper} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Mapper<?,?,?,?>> getMapperClass() 
+     throws ClassNotFoundException;
+
+  /**
+   * Get the combiner class for the job.
+   * 
+   * @return the combiner class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Reducer<?,?,?,?>> getCombinerClass() 
+     throws ClassNotFoundException;
+
+  /**
+   * Get the {@link Reducer} class for the job.
+   * 
+   * @return the {@link Reducer} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Reducer<?,?,?,?>> getReducerClass() 
+     throws ClassNotFoundException;
+
+  /**
+   * Get the {@link OutputFormat} class for the job.
+   * 
+   * @return the {@link OutputFormat} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends OutputFormat<?,?>> getOutputFormatClass() 
+     throws ClassNotFoundException;
+
+  /**
+   * Get the {@link Partitioner} class for the job.
+   * 
+   * @return the {@link Partitioner} class for the job.
+   */
+  @SuppressWarnings("unchecked")
+  public Class<? extends Partitioner<?,?>> getPartitionerClass() 
+     throws ClassNotFoundException;
+
+  /**
+   * Get the {@link RawComparator} comparator used to compare keys.
+   * 
+   * @return the {@link RawComparator} comparator used to compare keys.
+   */
+  public RawComparator<?> getSortComparator();
+
+  /**
+   * Get the pathname of the job's jar.
+   * @return the pathname
+   */
+  public String getJar();
+
+  /** 
+   * Get the user defined {@link RawComparator} comparator for 
+   * grouping keys of inputs to the reduce.
+   * 
+   * @return comparator set by the user for grouping values.
+   * @see Job#setGroupingComparatorClass(Class) for details.  
+   */
+  public RawComparator<?> getGroupingComparator();
+  
+  /**
+   * Get whether job-setup and job-cleanup is needed for the job 
+   * 
+   * @return boolean 
+   */
+  public boolean getJobSetupCleanupNeeded();
+
+  /**
+   * Get whether the task profiling is enabled.
+   * @return true if some tasks will be profiled
+   */
+  public boolean getProfileEnabled();
+  
+  /**
+   * 
+   * @return the parameters to pass to the task child to configure profiling
+   */
+  public String getProfileParams();
+
+ /**
+  * Get the reported username for this job.
+  * 
+  * @return the username
+  */
+  public String getUser();
+ 
+ /**
+  * This method checks to see if symlinks are to be create for the 
+  * localized cache files in the current working directory 
+  * @return true if symlinks are to be created- else return false
+  */
+  public boolean getSymlink();
+ 
+ /**
+  * Get the archive entries in classpath as an array of Path
+  */
+  public Path[] getArchiveClassPaths();
+
+ /**
+  * Get cache archives set in the Configuration
+  * @return A URI array of the caches set in the Configuration
+  * @throws IOException
+  */
+  public URI[] getCacheArchives() throws IOException;
+  
+  /**
+   * Get cache files set in the Configuration
+   * @throws IOException
+   */
+  
+  public URI[] getCacheFiles() throws IOException;
+
+  /**
+   * Return the path array of the localized caches
+   * @return A path array of localized caches
+   * @throws IOException
+   */
+  public Path[] getLocalCacheArchives() throws IOException;
+
+  /**
+   * Return the path array of the localized files
+   * @return A path array of localized files
+   * @throws IOException
+   */
+  public Path[] getLocalCacheFiles() throws IOException;
+
+  /**
+   * Get the file entries in classpath as an array of Path
+   */
+  public Path[] getFileClassPaths();
+  
+  /**
+   * Get the timestamps of the archives.  Used by internal
+   * @return a string array of timestamps 
+   * @throws IOException
+   */
+  public String[] getArchiveTimestamps();
+  
+  /**
+   * Get the timestamps of the files.  Used by internal
+   * @return a string array of timestamps 
+   * @throws IOException
+   */
+  public String[] getFileTimestamps();
+
+  /** 
+   * Get the configured number of maximum attempts that will be made to run a
+   *  
+   * @return the max number of attempts per map task.
+   */
+  public int getMaxMapAttempts();
+
+  /** 
+   * Get the configured number of maximum attempts  that will be made to run a
+   * 
+   * @return the max number of attempts per reduce task.
+   */
+  public int getMaxReduceAttempts();
+
+}
diff -rupN ./src/test/org/apache/hadoop/mapred/TestReduceTaskFetchFail.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/test/org/apache/hadoop/mapred/TestReduceTaskFetchFail.java
--- ./src/test/org/apache/hadoop/mapred/TestReduceTaskFetchFail.java	2012-11-02 02:56:46.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/test/org/apache/hadoop/mapred/TestReduceTaskFetchFail.java	2013-09-15 07:43:01.433937000 +0200
@@ -39,10 +39,8 @@ public class TestReduceTaskFetchFail {
     public String getJobFile() { return "/foo"; }
 
     public class TestReduceCopier extends ReduceCopier {
-      public TestReduceCopier(TaskUmbilicalProtocol umbilical, JobConf conf,
-                        TaskReporter reporter
-                        )throws ClassNotFoundException, IOException {
-        super(umbilical, conf, reporter);
+      public void init(ShuffleConsumerPlugin.Context context)throws ClassNotFoundException, IOException {
+        super.init(context);
       }
 
       public void checkAndInformJobTracker(int failures, TaskAttemptID mapId, boolean readError) {
@@ -69,8 +67,10 @@ public class TestReduceTaskFetchFail {
     TaskAttemptID tid =  new TaskAttemptID();
     TestReduceTask rTask = new TestReduceTask();
     rTask.setConf(conf);
+    ShuffleConsumerPlugin.Context context = new ShuffleConsumerPlugin.Context(rTask, mockUmbilical, conf, mockTaskReporter);
 
-    ReduceTask.ReduceCopier reduceCopier = rTask.new TestReduceCopier(mockUmbilical, conf, mockTaskReporter);
+    ReduceTask.ReduceCopier reduceCopier = rTask.new TestReduceCopier();
+    reduceCopier.init(context);
     reduceCopier.checkAndInformJobTracker(1, tid, false);
 
     verify(mockTaskReporter, never()).progress();
@@ -82,7 +82,9 @@ public class TestReduceTaskFetchFail {
     conf.setInt("mapreduce.reduce.shuffle.maxfetchfailures", 3);
 
     rTask.setConf(conf);
-    reduceCopier = rTask.new TestReduceCopier(mockUmbilical, conf, mockTaskReporter);
+    context = new ShuffleConsumerPlugin.Context(rTask, mockUmbilical, conf, mockTaskReporter);
+    reduceCopier = rTask.new TestReduceCopier();
+    reduceCopier.init(context);
 
     reduceCopier.checkAndInformJobTracker(1, tid, false);
     verify(mockTaskReporter, times(1)).progress();
@@ -103,7 +105,9 @@ public class TestReduceTaskFetchFail {
     conf.setBoolean("mapreduce.reduce.shuffle.notify.readerror", false);
 
     rTask.setConf(conf);
-    reduceCopier = rTask.new TestReduceCopier(mockUmbilical, conf, mockTaskReporter);
+    context = new ShuffleConsumerPlugin.Context(rTask, mockUmbilical, conf, mockTaskReporter);
+    reduceCopier = rTask.new TestReduceCopier();
+    reduceCopier.init(context);
 
     reduceCopier.checkAndInformJobTracker(7, tid, true);
     verify(mockTaskReporter, times(4)).progress();
diff -rupN ./src/test/org/apache/hadoop/mapred/TestShufflePlugin.java ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/test/org/apache/hadoop/mapred/TestShufflePlugin.java
--- ./src/test/org/apache/hadoop/mapred/TestShufflePlugin.java	1970-01-01 02:00:00.000000000 +0200
+++ ../../hadoop-cdh-patched/hadoop-2.0.0-mr1-cdh4.1.2/src/test/org/apache/hadoop/mapred/TestShufflePlugin.java	2013-09-15 07:43:01.435925000 +0200
@@ -0,0 +1,188 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+import static org.mockito.Mockito.*;
+
+import org.apache.hadoop.mapred.Task.TaskReporter;
+import org.apache.hadoop.fs.LocalFileSystem;
+import java.io.IOException;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.mapreduce.JobContext;
+
+/**
+ * A JUnit test for testing availability and accessibility of main API that is needed
+ * for sub-classes of ShuffleProviderPlugin and ShuffleConsumerPlugin.
+ * The importance of this test is for preserving API with 3rd party plugins.
+ */
+public class TestShufflePlugin {
+
+  static class TestShuffleConsumerPlugin implements ShuffleConsumerPlugin {
+    @Override
+    public void init(ShuffleConsumerPlugin.Context context) {
+      // just verify that Context has kept its public interface
+      context.getReduceTask();
+      context.getConf();
+      context.getUmbilical();
+      context.getReporter();
+    }
+    @Override
+    public boolean fetchOutputs() throws IOException{
+      return true;
+    }
+    @Override
+    public Throwable getMergeThrowable(){
+      return null;
+    }
+    @Override
+    public RawKeyValueIterator createKVIterator(JobConf job, FileSystem fs, Reporter reporter) throws IOException{
+      return null;
+    }
+    @Override
+    public void close(){
+    }
+  }
+
+  @Test
+  /**
+   * A testing method instructing core hadoop to load an external ShuffleConsumerPlugin
+   * as if it came from a 3rd party.
+   */
+  public void testConsumerPluginAbility() {
+
+    try{
+      // create JobConf with mapreduce.job.shuffle.consumer.plugin=TestShuffleConsumerPlugin
+      JobConf jobConf = new JobConf();
+      jobConf.setClass(JobContext.SHUFFLE_CONSUMER_PLUGIN_ATTR,
+          TestShufflePlugin.TestShuffleConsumerPlugin.class,
+          ShuffleConsumerPlugin.class);
+
+      ShuffleConsumerPlugin shuffleConsumerPlugin = null;
+      Class<? extends ShuffleConsumerPlugin> clazz =
+          jobConf.getClass(JobContext.SHUFFLE_CONSUMER_PLUGIN_ATTR, null, ShuffleConsumerPlugin.class);
+      assertNotNull("Unable to get " + JobContext.SHUFFLE_CONSUMER_PLUGIN_ATTR, clazz);
+
+      // load 3rd party plugin through core's factory method
+      shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, jobConf);
+      assertNotNull("Unable to load " + JobContext.SHUFFLE_CONSUMER_PLUGIN_ATTR, shuffleConsumerPlugin);
+    }
+    catch (Exception e) {
+      assertTrue("Threw exception:" + e, false);
+    }
+  }
+
+  static class TestShuffleProviderPlugin implements ShuffleProviderPlugin {
+    @Override
+    public void initialize(TaskTracker tt) {
+    }
+    @Override
+    public void destroy(){
+    }
+  }
+
+  @Test
+  /**
+   * A testing method instructing core hadoop to load an external ShuffleConsumerPlugin
+   * as if it came from a 3rd party.
+   */
+  public void testProviderPluginAbility() {
+
+    try{
+      // create JobConf with mapreduce.job.shuffle.provider.plugin=TestShuffleProviderPlugin
+      JobConf jobConf = new JobConf();
+      jobConf.setClass(TaskTracker.SHUFFLE_PROVIDER_PLUGIN_CLASSES,
+          TestShufflePlugin.TestShuffleProviderPlugin.class,
+          ShuffleProviderPlugin.class);
+
+      ShuffleProviderPlugin shuffleProviderPlugin = null;
+      Class<? extends ShuffleProviderPlugin> clazz =
+          jobConf.getClass(TaskTracker.SHUFFLE_PROVIDER_PLUGIN_CLASSES, null, ShuffleProviderPlugin.class);
+      assertNotNull("Unable to get " + TaskTracker.SHUFFLE_PROVIDER_PLUGIN_CLASSES, clazz);
+
+      // load 3rd party plugin through core's factory method
+      shuffleProviderPlugin = ReflectionUtils.newInstance(clazz, jobConf);
+      assertNotNull("Unable to load " + TaskTracker.SHUFFLE_PROVIDER_PLUGIN_CLASSES, shuffleProviderPlugin);
+    }
+    catch (Exception e) {
+      assertTrue("Threw exception:" + e, false);
+    }
+  }
+
+  @Test
+  /**
+   * A method for testing availability and accessibility of API that is needed for sub-classes of ShuffleProviderPlugin
+   */
+  public void testProvider() {
+    //mock creation
+    ShuffleProviderPlugin mockShuffleProvider = mock(ShuffleProviderPlugin.class);
+    TaskTracker mockTT = mock(TaskTracker.class);
+    TaskController mockTaskController = mock(TaskController.class);
+
+    mockShuffleProvider.initialize(mockTT);
+    mockShuffleProvider.destroy();
+    try {
+      mockTT.getJobConf();
+      mockTT.getJobConf(mock(JobID.class));
+      mockTT.getIntermediateOutputDir("","","");
+      mockTT.getTaskController();
+      mockTaskController.getRunAsUser(mock(JobConf.class));
+    }
+    catch (Exception e){
+      assertTrue("Threw exception:" + e, false);
+    }
+  }
+
+  @Test
+  /**
+   * A method for testing availability and accessibility of API that is needed for sub-classes of ShuffleConsumerPlugin
+   */
+  public void testConsumer() {
+    //mock creation
+    ShuffleConsumerPlugin mockShuffleConsumer = mock(ShuffleConsumerPlugin.class);
+    ReduceTask mockReduceTask = mock(ReduceTask.class);
+    JobConf mockJobConf = mock(JobConf.class);
+    TaskUmbilicalProtocol mockUmbilical = mock(TaskUmbilicalProtocol.class);
+    TaskReporter mockReporter = mock(TaskReporter.class);
+    LocalFileSystem mockLocalFileSystem = mock(LocalFileSystem.class);
+
+    mockReduceTask.getTaskID();
+    mockReduceTask.getJobID();
+    mockReduceTask.getNumMaps();
+    mockReduceTask.getPartition();
+    mockReduceTask.getJobFile();
+    mockReduceTask.getJvmContext();
+
+    mockReporter.progress();
+
+    try {
+      String [] dirs = mockJobConf.getLocalDirs();
+      ShuffleConsumerPlugin.Context context = new ShuffleConsumerPlugin.Context(mockReduceTask, mockUmbilical, mockJobConf, mockReporter);
+      mockShuffleConsumer.init(context);
+      mockShuffleConsumer.fetchOutputs();
+      mockShuffleConsumer.createKVIterator(mockJobConf, mockLocalFileSystem.getRaw(), mockReporter);
+      mockShuffleConsumer.close();
+    }
+    catch (Exception e){
+      assertTrue("Threw exception:" + e, false);
+    }
+  }
+}
