diff -rupN ./src/mapred/mapred-default.xml ./src/mapred/mapred-default.xml
--- ./src/mapred/mapred-default.xml	2012-11-02 02:56:46.000000000 +0200
+++ ./src/mapred/mapred-default.xml	2013-09-15 07:43:01.391926000 +0200
@@ -219,6 +219,24 @@
 </property>
 
 <property>
+  <name>mapreduce.job.reduce.shuffle.consumer.plugin.class</name>
+  <value>org.apache.hadoop.mapred.ReduceTask$ReduceCopier</value>
+  <description>Name of the class whose instance will be used
+   to send shuffle requests by reducetasks of this job.
+   The class must be an instance of org.apache.hadoop.mapred.ShuffleConsumerPlugin.
+  </description>
+</property>
+
+<property>
+  <name>mapreduce.shuffle.provider.plugin.classes</name>
+  <value>org.apache.hadoop.mapred.TaskTracker$DefaultShuffleProvider</value>
+  <description>A comma-separated list of classes that should be loaded as ShuffleProviderPlugin(s).
+   A ShuffleProviderPlugin can serve shuffle requests from reducetasks.
+   Each class in the list must be an instance of org.apache.hadoop.mapred.ShuffleProviderPlugin.
+  </description>
+</property>
+
+<property>
   <name>mapred.tasktracker.taskmemorymanager.monitoring-interval</name>
   <value>5000</value>
   <description>The interval, in milliseconds, for which the tasktracker waits
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java
--- ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java	2012-11-02 02:56:46.000000000 +0200
+++ ./src/mapred/org/apache/hadoop/mapred/ReduceTask.java	2013-09-15 07:43:01.402925000 +0200
@@ -77,6 +77,7 @@ import org.apache.hadoop.io.compress.Com
 import org.apache.hadoop.io.compress.Decompressor;
 import org.apache.hadoop.io.compress.DefaultCodec;
 import org.apache.hadoop.mapred.IFile.*;
+import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapred.Merger.Segment;
 import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
 import org.apache.hadoop.mapred.TaskTracker.TaskInProgress;
@@ -94,7 +95,7 @@ import org.apache.hadoop.util.StringUtil
 import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
 
 /** A Reduce task. */
-class ReduceTask extends Task {
+public class ReduceTask extends Task {
 
   static {                                        // register a ctor
     WritableFactories.setFactory
@@ -106,7 +107,6 @@ class ReduceTask extends Task {
   
   private static final Log LOG = LogFactory.getLog(ReduceTask.class.getName());
   private int numMaps;
-  private ReduceCopier reduceCopier;
 
   private CompressionCodec codec;
 
@@ -394,16 +394,33 @@ class ReduceTask extends Task {
 
     // Initialize the codec
     codec = initCodec();
+    ShuffleConsumerPlugin shuffleConsumerPlugin = null;
 
     boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
     if (!isLocal) {
-      reduceCopier = new ReduceCopier(umbilical, job, reporter);
-      if (!reduceCopier.fetchOutputs()) {
-        if(reduceCopier.mergeThrowable instanceof FSError) {
-          throw (FSError)reduceCopier.mergeThrowable;
+      // loads ShuffleConsumerPlugin according to configuration file
+      // +++ NOTE: This code support load of 3rd party plugins at runtime +++
+      //
+      Class<? extends ShuffleConsumerPlugin> clazz =
+               job.getClass(JobContext.SHUFFLE_CONSUMER_PLUGIN_ATTR, ReduceCopier.class, ShuffleConsumerPlugin.class);
+
+      if (clazz != ReduceCopier.class) {
+        shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);
+      }
+      else {
+        shuffleConsumerPlugin = this.new ReduceCopier(); // default plugin is an inner class of ReduceTask
+      }
+      LOG.info(" Using ShuffleConsumerPlugin : " + shuffleConsumerPlugin);
+
+      ShuffleConsumerPlugin.Context context = new ShuffleConsumerPlugin.Context(ReduceTask.this, umbilical, conf, reporter);
+      shuffleConsumerPlugin.init(context);
+
+      if (!shuffleConsumerPlugin.fetchOutputs()) {
+        if(shuffleConsumerPlugin.getMergeThrowable() instanceof FSError) {
+          throw (FSError)shuffleConsumerPlugin.getMergeThrowable();
         }
         throw new IOException("Task: " + getTaskID() + 
-            " - The reduce copier failed", reduceCopier.mergeThrowable);
+            " - The ShuffleConsumerPlugin " + clazz.getSimpleName() + " failed", shuffleConsumerPlugin.getMergeThrowable());
       }
     }
     copyPhase.complete();                         // copy is already complete
@@ -417,7 +434,7 @@ class ReduceTask extends Task {
           !conf.getKeepFailedTaskFiles(), job.getInt("io.sort.factor", 100),
           new Path(getTaskID().toString()), job.getOutputKeyComparator(),
           reporter, spilledRecordsCounter, null)
-      : reduceCopier.createKVIterator(job, rfs, reporter);
+      : shuffleConsumerPlugin.createKVIterator(job, rfs, reporter);
         
     // free up the data structures
     mapOutputFilesOnDisk.clear();
@@ -436,6 +453,9 @@ class ReduceTask extends Task {
       runOldReducer(job, umbilical, reporter, rIter, comparator, 
                     keyClass, valueClass);
     }
+    if (shuffleConsumerPlugin != null) {
+      shuffleConsumerPlugin.close();
+    }
     done(umbilical, reporter);
 
     if (sslFactory != null) {
@@ -602,11 +622,11 @@ class ReduceTask extends Task {
     OTHER_ERROR
   };
 
-  class ReduceCopier<K, V> implements MRConstants {
+  class ReduceCopier<K, V> implements ShuffleConsumerPlugin, MRConstants {
 
     /** Reference to the umbilical object */
     private TaskUmbilicalProtocol umbilical;
-    private final TaskReporter reporter;
+    private TaskReporter reporter;
     
     /** Reference to the task object */
     
@@ -688,18 +708,18 @@ class ReduceTask extends Task {
     /**
      * When we accumulate maxInMemOutputs number of files in ram, we merge/spill
      */
-    private final int maxInMemOutputs;
+    private int maxInMemOutputs;
 
     /**
      * Usage threshold for in-memory output accumulation.
      */
-    private final float maxInMemCopyPer;
+    private float maxInMemCopyPer;
 
     /**
      * Maximum memory usage of map outputs to merge from memory into
      * the reduce, in bytes.
      */
-    private final long maxInMemReduce;
+    private long maxInMemReduce;
 
     /**
      * The threads for fetching the files.
@@ -749,7 +769,7 @@ class ReduceTask extends Task {
     /**
      * Maximum number of fetch failures before reducer aborts.
      */
-    private final int abortFailureLimit;
+    private int abortFailureLimit;
 
     /**
      * Initial penalty time in ms for a fetch failure.
@@ -1866,16 +1886,16 @@ class ReduceTask extends Task {
       URLClassLoader loader = new URLClassLoader(urls, parent);
       conf.setClassLoader(loader);
     }
-    
-    public ReduceCopier(TaskUmbilicalProtocol umbilical, JobConf conf,
-                        TaskReporter reporter
-                        )throws ClassNotFoundException, IOException {
+
+    @Override
+    public void init (ShuffleConsumerPlugin.Context context)throws ClassNotFoundException, IOException {
       
+      JobConf conf = context.getConf();
+      this.reporter = context.getReporter();
+      this.umbilical = context.getUmbilical();
+      this.reduceTask = context.getReduceTask();
       configureClasspath(conf);
-      this.reporter = reporter;
       this.shuffleClientMetrics = new ShuffleClientMetrics(conf);
-      this.umbilical = umbilical;      
-      this.reduceTask = ReduceTask.this;
 
       this.scheduledCopies = new ArrayList<MapOutputLocation>(100);
       this.copyResults = new ArrayList<CopyResult>(100);    
@@ -1935,12 +1955,22 @@ class ReduceTask extends Task {
       this.reportReadErrorImmediately = 
         conf.getBoolean("mapreduce.reduce.shuffle.notify.readerror", true);
     }
-    
+
+    @Override
+    public Throwable getMergeThrowable() {
+      return mergeThrowable;
+    }
+
+    @Override
+    public void close(){
+    }
+
     private boolean busyEnough(int numInFlight) {
       return numInFlight > maxInFlight;
     }
     
     
+    @Override
     public boolean fetchOutputs() throws IOException {
       int totalFailures = 0;
       int            numInFlight = 0, numCopied = 0;
@@ -2382,8 +2412,9 @@ class ReduceTask extends Task {
      * first merge pass. If not, then said outputs must be written to disk
      * first.
      */
+    @Override
     @SuppressWarnings("unchecked")
-    private RawKeyValueIterator createKVIterator(
+    public RawKeyValueIterator createKVIterator(
         JobConf job, FileSystem fs, Reporter reporter) throws IOException {
 
       // merge config params
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java ./src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java
--- ./src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java	1970-01-01 02:00:00.000000000 +0200
+++ ./src/mapred/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java	2013-09-15 07:43:01.405933000 +0200
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import java.io.IOException;
+import org.apache.hadoop.mapred.Task.TaskReporter;
+import org.apache.hadoop.fs.FileSystem;
+
+/**
+ * ShuffleConsumerPlugin for serving Reducers.  It may shuffle MOF files from
+ * either the built-in provider (MapOutputServlet) or from a 3rd party ShuffleProviderPlugin.
+ *
+ */
+@InterfaceAudience.LimitedPrivate("MapReduce")
+@InterfaceStability.Unstable
+public interface ShuffleConsumerPlugin {
+
+  /**
+   * initialize this instance after it was created by factory.
+   */
+  public void init(Context context) throws ClassNotFoundException, IOException;
+
+  /**
+   * fetch output of mappers from TaskTrackers
+   * @return true iff success.  In case of failure an appropriate Throwable may be available thru getMergeThrowable() member
+   */
+  public boolean fetchOutputs() throws IOException;
+
+  /**
+   * @ret reference to a Throwable object (if merge throws an exception)
+   */
+  public Throwable getMergeThrowable();
+
+  /**
+   * Create a RawKeyValueIterator from copied map outputs.
+   *
+   * The iterator returned must satisfy the following constraints:
+   *   1. Fewer than io.sort.factor files may be sources
+   *   2. No more than maxInMemReduce bytes of map outputs may be resident
+   *      in memory when the reduce begins
+   *
+   * If we must perform an intermediate merge to satisfy (1), then we can
+   * keep the excluded outputs from (2) in memory and include them in the
+   * first merge pass. If not, then said outputs must be written to disk
+   * first.
+   */
+  public RawKeyValueIterator createKVIterator(JobConf job, FileSystem fs, Reporter reporter) throws IOException;
+
+  /**
+   * close and clean any resource associated with this object.
+   */
+  public void close();
+
+  @InterfaceAudience.LimitedPrivate("MapReduce")
+  @InterfaceStability.Unstable
+  public static class Context {
+    private final ReduceTask reduceTask;
+    private final TaskUmbilicalProtocol umbilical;
+    private final JobConf conf;
+    private final TaskReporter reporter;
+
+    public Context(ReduceTask reduceTask, TaskUmbilicalProtocol umbilical, JobConf conf, TaskReporter reporter){
+      this.reduceTask = reduceTask;
+      this.umbilical = umbilical;
+      this.conf = conf;
+      this.reporter = reporter;
+    }
+
+    public ReduceTask getReduceTask() {
+      return reduceTask;
+    }
+    public JobConf getConf() {
+      return conf;
+    }
+    public TaskUmbilicalProtocol getUmbilical() {
+      return umbilical;
+    }
+    public TaskReporter getReporter() {
+      return reporter;
+    }
+  }
+}
diff -rupN ./src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java ./src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java
--- ./src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java	1970-01-01 02:00:00.000000000 +0200
+++ ./src/mapred/org/apache/hadoop/mapred/ShuffleProviderPlugin.java	2013-09-15 07:43:01.407925000 +0200
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * This interface is implemented by objects that are able to answer shuffle requests which are
+ * sent from a matching Shuffle Consumer that lives in context of a ReduceTask object.
+ *
+ * ShuffleProviderPlugin object will be notified on the following events:
+ * initialize, destroy.
+ *
+ * NOTE: This interface is also used when loading 3rd party plugins at runtime
+ *
+ */
+@InterfaceAudience.LimitedPrivate("MapReduce")
+@InterfaceStability.Unstable
+public interface ShuffleProviderPlugin {
+  /**
+   * Do constructor work here.
+   * This method is invoked by the TaskTracker Constructor
+   */
+  public void initialize(TaskTracker taskTracker);
+
+  /**
+   * close and cleanup any resource, including threads and disk space.
+   * This method is invoked by TaskTracker.shutdown
+   */
+  public void destroy();
+}
diff -rupN ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java
--- ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java	2012-11-02 02:56:46.000000000 +0200
+++ ./src/mapred/org/apache/hadoop/mapred/TaskTracker.java	2013-09-15 07:43:01.415925000 +0200
@@ -163,6 +163,9 @@ public class TaskTracker implements MRCo
   private String shuffleScheme;
   private int shufflePort;
 
+  public static final String SHUFFLE_PROVIDER_PLUGIN_CLASSES = "mapreduce.shuffle.provider.plugin.classes";
+  final private ShuffleProviderPlugin shuffleProviderPlugin = new MultiShuffleProviderPlugin();
+
   static enum State {NORMAL, STALE, INTERRUPTED, DENIED}
 
   static final FsPermission LOCAL_DIR_PERMISSION =
@@ -282,6 +285,52 @@ public class TaskTracker implements MRCo
     }
   }
 
+  public static class DefaultShuffleProvider implements ShuffleProviderPlugin {
+    public void initialize(TaskTracker tt) {
+      tt.server.addInternalServlet("mapOutput", "/mapOutput", MapOutputServlet.class);
+    }
+
+    public void destroy() {
+    }
+  }
+
+  private static class MultiShuffleProviderPlugin implements ShuffleProviderPlugin {
+
+    private ShuffleProviderPlugin[] plugins;
+
+    public void initialize(TaskTracker tt) {
+      Configuration conf = tt.getJobConf();
+      Class<?>[] klasses = conf.getClasses(SHUFFLE_PROVIDER_PLUGIN_CLASSES, DefaultShuffleProvider.class);
+
+      plugins = new ShuffleProviderPlugin[klasses.length];
+      for (int i = 0; i < klasses.length; i++) {
+        try{
+          LOG.info(" Loading ShuffleProviderPlugin: " + klasses[i]);
+          plugins[i] =  (ShuffleProviderPlugin)ReflectionUtils.newInstance(klasses[i], conf);
+          plugins[i].initialize(tt);
+        }
+        catch(Throwable t) {
+          LOG.warn("Exception instantiating/initializing a ShuffleProviderPlugin: " + klasses[i], t);
+          plugins[i] =  null;
+        }
+      }
+    }
+
+    public void destroy() {
+      if (plugins != null) {
+          for (ShuffleProviderPlugin plugin : plugins) {
+            try {
+              if (plugin != null) {
+                plugin.destroy();
+              }
+            } catch (Throwable t) {
+              LOG.warn("Exception destroying a ShuffleProviderPlugin: " + plugin, t);
+            }
+          }
+        }
+      }
+    }
+
   private LocalStorage localStorage;
   private long lastCheckDirsTime;
   private int lastNumFailures;
@@ -735,7 +784,7 @@ public class TaskTracker implements MRCo
     + TaskTracker.LOCAL_SPLIT_FILE;
   }
 
-  static String getIntermediateOutputDir(String user, String jobid,
+  public static String getIntermediateOutputDir(String user, String jobid,
       String taskid) {
     return getLocalTaskDir(user, jobid, taskid) + Path.SEPARATOR
     + TaskTracker.OUTPUT;
@@ -1512,6 +1561,14 @@ public class TaskTracker implements MRCo
   public synchronized void shutdown() throws IOException, InterruptedException {
     shuttingDown = true;
     close();
+    if (this.shuffleProviderPlugin != null) {
+      try {
+        LOG.info("Shutting down shuffleProviderPlugin");
+        this.shuffleProviderPlugin.destroy();
+      } catch (Exception e) {
+        LOG.warn("Exception shutting down shuffleProviderPlugin", e);
+      }
+    }
     if (this.server != null) {
       try {
         LOG.info("Shutting down StatusHttpServer");
@@ -1723,7 +1780,7 @@ public class TaskTracker implements MRCo
     String exceptionMsgRegex = conf.get("mapreduce.reduce.shuffle.catch.exception.message.regex");
     server.setAttribute("exceptionStackRegex", exceptionStackRegex);
     server.setAttribute("exceptionMsgRegex", exceptionMsgRegex);
-    server.addInternalServlet("mapOutput", "/mapOutput", MapOutputServlet.class);
+    shuffleProviderPlugin.initialize(this);
     server.addServlet("taskLog", "/tasklog", TaskLogServlet.class);
 
     boolean shuffleSsl = conf.getBoolean(JobTracker.SHUFFLE_SSL_ENABLED_KEY,
@@ -4000,9 +4057,22 @@ public class TaskTracker implements MRCo
   }
   
   /**
+   * Get the specific job conf for a running job.
+   */
+  public JobConf getJobConf(JobID jobId) throws IOException {
+    synchronized (runningJobs) {
+      RunningJob rjob = runningJobs.get(jobId);
+      if (rjob == null) {
+        throw new IOException("Unknown job " + jobId + "!!");
+      }
+      return rjob.getJobConf();
+    }
+  }
+
+  /**
    * Get the default job conf for this tracker.
    */
-  JobConf getJobConf() {
+  public JobConf getJobConf() {
     return fConf;
   }
     
@@ -4148,16 +4218,10 @@ public class TaskTracker implements MRCo
         FileSystem rfs = ((LocalFileSystem)
             context.getAttribute("local.file.system")).getRaw();
 
-      String userName = null;
-      String runAsUserName = null;
-      synchronized (tracker.runningJobs) {
-        RunningJob rjob = tracker.runningJobs.get(JobID.forName(jobId));
-        if (rjob == null) {
-          throw new IOException("Unknown job " + jobId + "!!");
-        }
-        userName = rjob.jobConf.getUser();
-        runAsUserName = tracker.getTaskController().getRunAsUser(rjob.jobConf);
-      }
+      JobConf jobConf = tracker.getJobConf(JobID.forName(jobId));
+      String userName = jobConf.getUser();
+      String runAsUserName = tracker.getTaskController().getRunAsUser(jobConf);
+
       // Index file
       String intermediateOutputDir = TaskTracker.getIntermediateOutputDir(userName, jobId, mapId);
       String indexKey = intermediateOutputDir + "/file.out.index";
diff -rupN ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java
--- ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java	2012-11-02 02:56:46.000000000 +0200
+++ ./src/mapred/org/apache/hadoop/mapreduce/JobContext.java	2013-09-15 07:43:01.421928000 +0200
@@ -75,6 +75,9 @@ public interface JobContext {
    */
   public Configuration getConfiguration();
   
+  public static final String SHUFFLE_CONSUMER_PLUGIN_ATTR =
+    "mapreduce.job.reduce.shuffle.consumer.plugin.class";
+
   /**
    * Get credentials for the job.
    * @return credentials for the job
