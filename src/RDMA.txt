Currently, we only support integrated test. 

##########  Prerequisites ##############
1. InfiniBand environment:
Before tests, please make sure there are three nodes fully
functional with InfiniBand and IPoIB enabled. For example, 
in our case, they are cs17(master), cs19, and cs20.

2. Code location
In order to run hadoop, the hadoop source code directory must have 
the same path on all the nodes and the user must have the write 
permission for that directory.

2-a. Assume our working directory is "/data/ywang-hadoop/" on cs17.
"/data" is mounted on the local filesystem cross all the nodes.
take cs17, cs19 and cs20 as examples in this tutorial(cs17 is the 
master, cs19 and cs20 are the slaves). 

2-b. Check out code on cs17. 
#ssh cs17
#cd /data/ywang-hadoop

#mkdir rdma-hadoop  /*top directory for the codes*/
#mkdir hadoop       /*for storing all related hadoop data */ 

#cd rdma-hadoop 
#svn co https://pasl.eng.auburn.edu/svn/hadoop/base-0.20.2/trunk .
#ls /data/ywang-hadoop/rdma-hadoop
bin          conf      ivy.xml      NOTICE.txt  
build.xml    conf_dev  lib          README.txt
CHANGES.txt  ivy       LICENSE.txt  src

2-c. Please DO remember to copy "/data/ywang-hadoop" to 
   all the slave nodes (cs19 and cs20) after you compiled
   and configured hadoop. Refer step 11 in the following section.

3. Password-less SSH login
Password-less ssh login among all nodes must be established before tests. 

4. JDK and ant
The two essential things for compiling the source code are :
1). latest version of Java must be installed and JAVA_HOME and PATH variables
    must be correctly configured.
2). latest ant version must be installed.

##########  Integrated Test ##############
1. Change to the top directory("/data/ywang-hadoop/rdma-hadoop").
# cd <top-directory>/

2. Compile Hadoop and create an example program
# ant jar
# ant examples
# cp build/hadoop-0.20.3-dev-examples.jar examples.jar

3. Build rdma implementation
# cd src/c++/rdma
# sh autogen.sh

4. Go back to the top directory 
# cd ../../../

5. Choose several nodes as slaves, a.k.a TaskTrackers
# cat > conf/slaves <<EOF
cs19
cs20
EOF

6. Choose a master node as the JobTracker and masternode
# cat > conf/master <<EOF
cs17
EOF

7. Change the conf/core-site.xml file with the name of your master node.
Please note that you need to change this path
"/data/ywang-hadoop/hadoop"  
to your own directory for storing all related hadoop data 
and change "cs17" to your master node.
For example, here is the content of our core-site.xml
# cat > conf/core-site.xml  <<EOF
<configuration>
     <property>
         <name>fs.default.name</name>
         <value>hdfs://cs17:9000</value>
     </property>
     <property>
         <name>hadoop.tmp.dir</name>
         <value>/data/ywang-hadoop/hadoop</value>
     </property>
</configuration>
EOF

8. Change the conf/mapred-site.xml with the name of your master node
For example, here is the content of our mapreduce-site.xml
1: Please note that you need to change these paths
"/data/ywang-hadoop/rdma-hadoop/src/c++/rdma/MOFSupplier"
"/data/ywang-hadoop/rdma-hadoop/src/c++/rdma/NetMerger"
"/data/ywang-hadoop/hadoop/rdmalogs/"
to the exact path of your compiled MOFSupplier, NetMerger and the log directory. 
Please also change "cs17" to your master node.
2: You also need to change the "ib0" to your own IB/10GE/1GE network interface name
3: You need to change the "mapred.local.dir" to your own defined directories
that store intermediate map output files.
*******Parameters******************    ********************Value/Meaning********************

mapred.rdma.setting                    1: enable rdma; 0: disable rdma
mapred.netmerger.merge.approach        1: on-disk merge(not available); 0: on-line merge    
mapred.rdma.mofsupplier                your compiled MOFSupplier location
mapred.rdma.netmerger                  your compiled NetMerger location
mapred.taskTracker.rdma.server.port    launching netmerger and mofsupplier C++ cmd line tag
mapred.rdma.cma.port                   for rdma connection 
mapred.netmerger.listener.port         listen to the launch of number hadoop reduce task
  
# cat > conf/mapred-site.xml <<EOF
<configuration>
      <property>
           <name>mapred.job.tracker</name>
           <value>cs17:9001</value>
      </property>

      <property>
           <name>mapred.tasktracker.dns.interface</name>
           <value>ib0</value>
      </property>
      <property>
           <name>mapred.local.dir</name>
           <value>${hadoop.tmp.dir}/mapred/local,/data1/ywang-hadoop</value>
      </property>
      <property>
           <!-- Maximum current mappers running on each datanode -->
           <name>mapred.tasktracker.map.tasks.maximum</name>
           <value>8</value>
      </property>
      <property>
           <!-- Maximum current reducers running on each datanode -->
           <name>mapred.tasktracker.reduce.tasks.maximum</name>
           <value>4</value>
      </property>
      <property>
           <name>mapred.map.tasks.speculative.execution</name>
           <value>false</value>
      </property>
      <property>
           <name>mapred.reduce.tasks.speculative.execution</name>
           <value>false</value>
      </property>
      <property>
           <!-- mapred.rdma.setting represents how users uses rdma
           0: disable rdma (use original hadoop).
           1: use rdma with rdma-merger. -->
           <name>mapred.rdma.setting</name>
           <value>1</value>
      </property>
      <property>
           <!-- merging approach
           0: on-disk merge (not enabled yet)
           1: online merge
           C++ cmd line tag: -a
           -->
           <name>mapred.netmerger.merge.approach</name>
           <value>1</value>
      </property>
      <property>
           <!-- mapred.rdma.mofsupplier defines the location of C++ driver
           for launching the rdma map task side program -->
           <name>mapred.rdma.mofsupplier</name>
	   <value>/data/ywang-hadoop/rdma-hadoop/src/c++/rdma/MOFSupplier</value>
      </property>
      <property>
           <!-- the port number used by task tracker to launch netmerger and mofsupplier
           C++ cmd line tag: -j
           -->
           <name>mapred.taskTracker.rdma.server.port</name>
           <value>9010</value>
      </property>
      <property>
           <!-- mapred.rdma.netmerger defines the location of C++ driver
           for launching the rdma reduce task side program -->
           <name>mapred.rdma.netmerger</name>
           <value>/data/ywang-hadoop/rdma-hadoop/src/c++/rdma/NetMerger</value>
      </property>
      <property>
            <!-- defines the port number used to do the rdma connection 
            C++ cmd line tag: -r
            -->            
            <name>mapred.rdma.cma.port</name>
            <value>9011</value>
      </property>
      <property>
            <!-- 
             the log directory for rdma
             C++ cmd line tag: -g
             -->
            <name>mapred.rdma.log.dir</name>
            <value>/data/ywang-hadoop/hadoop/rdmalogs/</value>
      </property>
      <property>
            <!-- the port number netmerger uses to listen to the launch
            of number hadoop reduce task.
            C++ cmd line tag: -l
            -->
            <name>mapred.netmerger.listener.port</name>
            <value>9012</value>
      </property>
</configuration>
EOF

9. Change the conf/hdfs-site.xml with correct number of input replication
By default, replication "1" is enough for basic testing.
Please notice that you need to change "ib0" to your IB/10GE/1GE network interface name
# cat > conf/hdfs-site.xml <<EOF
<configuration>
     <property>
         <name>dfs.replication</name>
         <value>1</value>
     </property>
     <property>
         <!-- DFS block size -->
         <name>dfs.block.size</name>
         <value>268435456</value>
     </property>
     <property>
        <name>dfs.datanode.dns.interface</name>
        <value>ib0</value>
     </property>
</configuration>
EOF

10. Set environmental variables in conf/hadoop-env.sh, for example:
export JAVA_HOME=/your/path/to/JDK/home

11. Copy the whole working directory to all the slave nodes(cs19, cs20) 
#scp -r /data/ywang-hadoop/ ywang@cs19:/data
#scp -r /data/ywang-hadoop/ ywang@cs20:/data


12. Go to hadoop top directory on master node. 
# ssh cs17
# cd <top-directory>/

13. Format Hadoop storage, by default it is on local /tmp directory
# bin/hadoop namenode -format

14. Starting hadoop HDFS
# bin/start-dfs.sh


15. Starting hadoop mapreduce
# bin/start-mapred.sh

16. Prepare the input. Do remember to wait for two minutes because
the map side takes time to get fully started
# bin/hadoop fs -put conf input

17. run a sample program
# bin/hadoop jar examples.jar grep input output 'dfs[a-z.]+'

##########  Appendix ##############
0. For compiling everything from the top directory
-- Be sure to run ``make distclean'' in src/c++/rdma
-- Then run the following command at the top directory
   ant -verbose -Dcompile.c++=yes jar examples

1. for re-runing the basic tests, the following steps must be followed
1) stop all the processes that are running among all the nodes.
   "jps" command is useful to check what kind of processes are running on a
   single node.
   #jps
   run the specially designed bash files under current /bin directory,
   whose names are stop-*.sh
2) clean up the logs under trunk/logs/ 
3) clean up the hadoop storage. By default it's on local /tmp directory.

2. Know issues:
-- No progress on reduce tasks even though map can progress to 100%
   This is because of the lack of c++ Merger modules.
   The fetched segments are not channeled to reduce tasks yet.
-- Occasionally, code can throw java exceptions. We believe this
   is also because of the lack of fully funcational ReduceTask.

