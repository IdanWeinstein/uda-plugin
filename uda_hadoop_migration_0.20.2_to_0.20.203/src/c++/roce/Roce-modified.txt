Preparing package ready for distribution.

1. Install JDK and ant. The two essential things for compiling the source code are :
 latest version of Java must be installed and JAVA_HOME and PATH variables must be correctly configured.
	a) Install JDK
		mkdir /usr/lib64/java
		cd /usr/lib64/java
		bash /.autodirect/mtrswgwork/alekseys/Downloads/jdk-6u25-linux-x64.bin
		Define JAVA_HOME:
			export JAVA_HOME=/usr/lib64/java/jdk1.6.0_25

	b) Compile ant
		cd /usr/src
		tar xfj /.autodirect/mtrswgwork/alekseys/Downloads/apache-ant-1.8.2-src.tar.bz2
		cd apache-ant-1.8.2
		patch -p0 < /.autodirect/mtrswgwork/alekseys/Downloads/ant-build-xml.patch
		sh build.sh -Ddist.dir=/usr/local/ant-1.8.2
		export ANT_HOME=/usr/local/ant-1.8.2

2. Prepare hadoop package witn Roce compiled. Two files - MOFSupplier and NetMerger - will be created under 'bin' directory as result of this operation and will be copied to the final package.
	a) Obtain modifed hadoop sources from internal Git server (ssh://sol.voltaire.com/home/alekseys/scm/hadoop.git)
	b) cd hadoop 
	c) ant -Djava5.home=/usr/libjava/jdk1.6.0_25 package
	d) Change to 'build/hadoop-direct-1.0' directory and run 'ant clean' command
	e) Create tar-archive to distribute it by running 'tar cfz hadoop-direct-1.0.tar.gz hadoop-direct-1.0'

3. Follow to instructions bellow distribute hadoop between nodes and to configure it.



Distribution and configuration.

1. Example nodes:
cs17(master), cs19, and cs20.

2. Password-less SSH login  among all nodes

3. Working directory is "/data/ywang-hadoop/" on cs17.
"/data" is mounted on the local filesystem cross all the nodes.
#ssh cs17
#cd /data/ywang-hadoop
#mkdir roce-hadoop  hadoop /*top directory for the codes*/
#cd roce-hadoop 
#svn co https://pasl.eng.auburn.edu/svn/hadoop/base-0.20.2/trunk .

5. Copy "/data/ywang-hadoop" to all the slave nodes (cs19 and cs20) after you compiled and configured hadoop. Refer step 11 in the following section.




##########  Integrated Test ##############
1. Change to the top directory("/data/ywang-hadoop/roce-hadoop").
# cd <top-directory>/

1. Compile Hadoop and create an example program
# ant jar
# ant examples
# cp build/hadoop-0.20.3-dev-examples.jar examples.jar

1. Go back to the top directory 
# cd ../../../

1. 5. Choose several nodes as slaves, a.k.a TaskTrackers
# cat > conf/slaves <<EOF
cs19
cs20
EOF

1. 6. Choose a master node as the JobTracker and masternode
# cat > conf/master <<EOF
cs17
EOF


Copy from conf_dev/core-site.xml template for core-site.xml
7. Change the conf/core-site.xml file with the name of your master node.
Please note that you need to change this path
"/data/ywang-hadoop/hadoop"  
to your own directory for storing all related hadoop data 
and change "cs17" to your master node.
For example, here is the content of our core-site.xml
# cat > conf/core-site.xml  <<EOF
<configuration>
     <property>
         <name>fs.default.name</name>
         <value>hdfs://cs17:9000</value>
     </property>
     <property>
         <name>hadoop.tmp.dir</name>
         <value>/data/ywang-hadoop/hadoop</value>
     </property>
</configuration>
EOF

8. Change the conf/mapred-site.xml with the name of your master node
For example, here is the content of our mapreduce-site.xml
Please note that you need to change these paths
"/data/ywang-hadoop/roce-hadoop/src/c++/roce/MOFSupplier"
"/data/ywang-hadoop/roce-hadoop/src/c++/roce/NetMerger"
"/data/ywang-hadoop/hadoop/rocelogs/"
to the exact path of your compiled MOFSupplier, NetMerger and the log directory. 
Please also change "cs17" to your master node.

*******Parameters******************    ********************Value/Meaning********************

mapred.roce.setting                    1: enable roce; 0: disable roce
mapred.netmerger.merge.approach        1: on-disk merge(not available); 0: on-line merge    
mapred.roce.mofsupplier                your compiled MOFSupplier location
mapred.roce.netmerger                  your compiled NetMerger location
mapred.taskTracker.roce.server.port    launching netmerger and mofsupplier C++ cmd line tag
mapred.roce.cma.port                   for rdma connection 
mapred.netmerger.listener.port         listen to the launch of number hadoop reduce task
  
# cat > conf/mapred-site.xml <<EOF
<configuration>
      <property>
           <name>mapred.job.tracker</name>
           <value>cs17:9001</value>
      </property>
      <property>
           <!-- Maximum current mappers running on each datanode -->
           <name>mapred.tasktracker.map.tasks.maximum</name>
           <value>8</value>
      </property>
      <property>
           <!-- Maximum current reducers running on each datanode -->
           <name>mapred.tasktracker.reduce.tasks.maximum</name>
           <value>4</value>
      </property>
      <property>
           <name>mapred.map.tasks.speculative.execution</name>
           <value>false</value>
      </property>
      <property>
           <name>mapred.reduce.tasks.speculative.execution</name>
           <value>false</value>
      </property>
      <property>
           <!-- mapred.roce.setting represents how users uses roce
           0: disable roce (use original hadoop).
           1: use roce with roce-merger. -->
           <name>mapred.roce.setting</name>
           <value>1</value>
      </property>
      <property>
           <!-- merging approach
           0: on-disk merge (not enabled yet)
           1: online merge
           C++ cmd line tag: -a
           -->
           <name>mapred.netmerger.merge.approach</name>
           <value>1</value>
      </property>
      <property>
           <!-- mapred.roce.mofsupplier defines the location of C++ driver
           for launching the roce map task side program -->
           <name>mapred.roce.mofsupplier</name>
	   <value>/data/ywang-hadoop/roce-hadoop/src/c++/roce/MOFSupplier</value>
      </property>
      <property>
           <!-- the port number used by task tracker to launch netmerger and mofsupplier
           C++ cmd line tag: -j
           -->
           <name>mapred.taskTracker.roce.server.port</name>
           <value>9010</value>
      </property>
      <property>
           <!-- mapred.roce.netmerger defines the location of C++ driver
           for launching the roce reduce task side program -->
           <name>mapred.roce.netmerger</name>
           <value>/data/ywang-hadoop/roce-hadoop/src/c++/roce/NetMerger</value>
      </property>
      <property>
            <!-- defines the port number used to do the rdma connection 
            C++ cmd line tag: -r
            -->            
            <name>mapred.roce.cma.port</name>
            <value>9011</value>
      </property>
      <property>
            <!-- 
             the log directory for roce
             C++ cmd line tag: -g
             -->
            <name>mapred.roce.log.dir</name>
            <value>/data/ywang-hadoop/hadoop/rocelogs/</value>
      </property>
      <property>
            <!-- the port number netmerger uses to listen to the launch
            of number hadoop reduce task.
            C++ cmd line tag: -l
            -->
            <name>mapred.netmerger.listener.port</name>
            <value>9012</value>
      </property>
</configuration>
EOF

9. Change the conf/hdfs-site.xml with correct number of input replication
By default, replication "1" is enough for basic testing.
# cat > conf/hdfs-site.xml <<EOF
<configuration>
     <property>
         <name>dfs.replication</name>
         <value>1</value>
     </property>
     <property>
         <!-- DFS block size -->
         <name>dfs.block.size</name>
         <value>268435456</value>
     </property>
</configuration>
EOF

10. Set environmental variables in conf/hadoop-env.sh, for example:
export JAVA_HOME=/your/path/to/JDK/home

11. Copy the whole working directory to all the slave nodes(cs19, cs20) 
#scp -r /data/ywang-hadoop/ ywang@cs19:/data
#scp -r /data/ywang-hadoop/ ywang@cs20:/data


12. Go to hadoop top directory on master node. 
# ssh cs17
# cd <top-directory>/

13. Format Hadoop storage, by default it is on local /tmp directory
# bin/hadoop namenode -format

14. Starting hadoop HDFS
# bin/start-dfs.sh


15. Starting hadoop mapreduce
# bin/start-mapred.sh

16. Prepare the input. Do remember to wait for two minutes because
the map side takes time to get fully started
# bin/hadoop fs -put conf input

17. run a sample program
# bin/hadoop jar examples.jar grep input output 'dfs[a-z.]+'

##########  Appendix ##############
0. For compiling everything from the top directory
-- Be sure to run ``make distclean'' in src/c++/roce
-- Then run the following command at the top directory
   ant -verbose -Dcompile.c++=yes jar examples

1. for re-runing the basic tests, the following steps must be followed
1) stop all the processes that are running among all the nodes.
   "jps" command is useful to check what kind of processes are running on a
   single node.
   #jps
   run the specially designed bash files under current /bin directory,
   whose names are stop-*.sh
2) clean up the logs under trunk/logs/ 
3) clean up the hadoop storage. By default it's on local /tmp directory.

2. Know issues:
-- No progress on reduce tasks even though map can progress to 100%
   This is because of the lack of c++ Merger modules.
   The fetched segments are not channeled to reduce tasks yet.
-- Occasionally, code can throw java exceptions. We believe this
   is also because of the lack of fully funcational ReduceTask.

